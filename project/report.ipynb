{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of oracle-and-real-data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "guBl1hnUsmNf",
        "eEJNbwsLpB1s",
        "ifDu6QJSpOmZ",
        "YSiMq5Ck85MO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "G0qq6bLm4rqh",
        "outputId": "ec32da28-2ba2-42cc-e2c0-7c5d47ed8f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "cell_type": "code",
      "source": [
        "# If running on Google Colab, only cleverhans needs installation. This can be done via:\n",
        "# !pip install cleverhans\n",
        "\n",
        "# If running locally, we've listed our dependencies in requirements.txt, so the following\n",
        "# should get everything up and running:\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "import numpy\n",
        "import keras\n",
        "import pandas\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import cleverhans\n",
        "import tensorflow\n",
        "import seaborn\n",
        "import sklearn\n",
        "\n",
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.attacks import CarliniWagnerL2\n",
        "from cleverhans.attacks import SaliencyMapMethod\n",
        "from cleverhans.attacks_tf import jacobian_augmentation\n",
        "from cleverhans.attacks_tf import jacobian_graph\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "from cleverhans.utils_tf import model_eval\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "numpy.random.seed(0xC0FFEE)\n",
        "tensorflow.set_random_seed(0xC0FFEE)\n",
        "rng = numpy.random.RandomState(0xC0FFEE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cleverhans\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/a0/f0b4386b719f343c4ed3e13cd7792a7a7a4674566ca9b2b34a09b7424220/cleverhans-3.0.1-py3-none-any.whl (198kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.14.6)\n",
            "Collecting pycodestyle (from cleverhans)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/0c/04a353e104d2f324f8ee5f4b32012618c1c86dd79e52a433b64fceed511b/pycodestyle-2.5.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.0MB/s \n",
            "\u001b[?25hCollecting mnist~=0.2 (from cleverhans)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.6.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.0.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->cleverhans) (40.8.0)\n",
            "Installing collected packages: pycodestyle, mnist, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 pycodestyle-2.5.0\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ryjxHLD70y3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code in this notebook goes through the process of:\n",
        "  1. Building a model.\n",
        "  2. Replicating it using Jacobian Augmentation under a black-box attack model.\n",
        "  3. Building adversarial examples.\n",
        "  4. Developing an end-to-end attack which takes in a malicious script and will output an equivelant script which is classified as the target user specified.\n",
        "  \n",
        "**NOTE:** There is some randomness in this notebook which we haven't been able to get rid of (despite all those random we seeds we set at the top). Many of our results (from the surrogate onwards) can differ by a percent or so in either direction. Nonetheless, our analysis does still hold (e.g. the surrogate has consistently replicated the oracle better than it does predict correct labels everytime we've run this notebook)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "guBl1hnUsmNf"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GS0WJpSyuQeU"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OlTdMqCWus2I"
      },
      "cell_type": "markdown",
      "source": [
        "Run the below code to download a copy of the dataset (if you don't already have it):"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fLKVLsJFurZ5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "response = requests.get(\"http://www.schonlau.net/masquerade/masquerade-data.zip\")\n",
        "\n",
        "dataset_file = io.BytesIO(response.content)\n",
        "\n",
        "zipped_dataset = zipfile.ZipFile(dataset_file)\n",
        "zipped_dataset.extractall('data/masquerade-data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gIA5snRclpjq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://www.schonlau.net/intrusion.html\n",
        "# download Masquerade Data (zip File)\n",
        "\n",
        "import pandas as pd\n",
        "directory = './data/masquerade-data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3X6tHalTlpkU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sorts the given iterable in the way that is expected.\n",
        " \n",
        "    Required arguments:\n",
        "    l -- The iterable to be sorted.\n",
        " \n",
        "    \"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
        "    return sorted(l, key = alphanum_key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FP8_s_WClpkp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "users = range(1,51)\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for filename in sorted_nicely(os.listdir(directory)):\n",
        "    user = pd.read_csv(os.path.join(directory, filename), header=None)\n",
        "    df = pd.concat([df, user], axis = 1)\n",
        "    \n",
        "df.columns = sorted_nicely(os.listdir(directory))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xOhVC6HazrjI"
      },
      "cell_type": "markdown",
      "source": [
        "We've loaded in the dataset, but need to do a little coercion to get it into the required format. First, we make sure  the values in the dataframe are categorical variables sharing the same data type:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5vklxycDrRYm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "commands = numpy.unique(df)\n",
        "command_dtype = pandas.api.types.CategoricalDtype(commands)\n",
        "\n",
        "for column in df:\n",
        "    df[column] = df[column].astype(command_dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kO_pHxcslplN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labelled, unlabelled = df.head(5000), df.tail(len(df) - 5000)  # ignore unlabeled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZEH_Lws_op3_"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset contains a list of commands run for each user. Treating this as a timeseries, we perform rolling window sampling in blocks of 100 commands, and summarise the usage over each block."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "co7TNH4XqLYG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rolling_window_command_counts(commands, window_size):\n",
        "    \n",
        "    # Save a copy the name of the series to add again to our output. This will preserve the mapping of\n",
        "    # user identifier to (it's column header in the dataframe it came from), which in\n",
        "    # this case is the user identifier. \n",
        "    user = commands.name\n",
        "\n",
        "    # Convert the single column \"which command was run?\" to a column for each\n",
        "    # command, which says \"was command <x> run?\"\n",
        "    commands = pandas.get_dummies(commands)\n",
        "\n",
        "    # Take a rolling sample of the last 100 commands, then sum each \"was command <x> run?\"\n",
        "    # columns to give a bunch \"command <x> was run <y> times in this window\".\n",
        "    command_counts = commands.rolling(window=window_size).aggregate(numpy.sum)\n",
        "\n",
        "    # Remove the first 100 rows because they contain data from blocks of size < 100.\n",
        "    command_counts = command_counts[window_size-1:]\n",
        "    \n",
        "    # Preserve the user identifier (see top of function) as a new column:\n",
        "    \n",
        "    # First, a nasty hack: https://github.com/pandas-dev/pandas/issues/19136\n",
        "    command_counts = command_counts.rename(columns=str)  \n",
        "    \n",
        "    # Then, add in the user (with an adhoc parser to turn the label into a number)\n",
        "    command_counts['user'] = int(user.replace('User', ''))\n",
        "\n",
        "    return command_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdQxI7iTQNPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apply to the entire dataset:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Oiyth5e0pYSq",
        "outputId": "90fe3472-c732-4c67-ecb2-eda10f627ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "labelled_dataset = pandas.concat([\n",
        "        rolling_window_command_counts(commands, 100)\n",
        "        for user, commands in labelled.iteritems()\n",
        "    ],\n",
        "    ignore_index=True,  # reset index to go from 0 to 4900\n",
        ")\n",
        "\n",
        "labelled_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>%backup%</th>\n",
              "      <th>.java_wr</th>\n",
              "      <th>.maker_w</th>\n",
              "      <th>.wrapper</th>\n",
              "      <th>.xinitrc</th>\n",
              "      <th>.xsessio</th>\n",
              "      <th>1.1</th>\n",
              "      <th>1.2</th>\n",
              "      <th>1.3</th>\n",
              "      <th>4Dwm</th>\n",
              "      <th>...</th>\n",
              "      <th>xxx</th>\n",
              "      <th>yacc</th>\n",
              "      <th>ypcat</th>\n",
              "      <th>yppasswd</th>\n",
              "      <th>z</th>\n",
              "      <th>zip</th>\n",
              "      <th>zsh</th>\n",
              "      <th>zubs</th>\n",
              "      <th>zz2</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245020</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245021</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245022</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245023</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245024</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245025</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245026</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245027</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245028</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245029</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245030</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245031</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245032</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245033</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245034</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245035</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245036</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245037</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245038</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245039</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245040</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245041</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245042</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245043</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245044</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245045</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245046</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245047</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245048</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245049</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>245050 rows × 857 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        %backup%  .java_wr  .maker_w  .wrapper  .xinitrc  .xsessio  1.1  1.2  \\\n",
              "0            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "1            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "2            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "3            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "4            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "5            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "6            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "7            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "8            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "9            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "10           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "11           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "12           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "13           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "14           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "15           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "16           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "17           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "18           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "19           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "20           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "21           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "22           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "23           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "24           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "25           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "26           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "27           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "28           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "29           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "...          ...       ...       ...       ...       ...       ...  ...  ...   \n",
              "245020       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245021       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245022       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245023       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245024       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245025       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245026       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245027       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245028       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245029       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245030       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245031       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245032       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245033       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245034       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245035       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245036       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245037       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245038       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245039       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245040       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245041       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245042       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245043       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245044       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245045       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245046       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245047       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245048       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245049       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "\n",
              "        1.3  4Dwm  ...   xxx  yacc  ypcat  yppasswd    z  zip  zsh  zubs  zz2  \\\n",
              "0       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "1       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "2       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "3       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "4       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "5       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "6       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "7       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "8       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "9       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "10      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "11      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "12      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "13      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "14      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "15      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "16      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "17      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "18      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "19      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "20      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "21      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "22      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "23      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "24      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "25      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "26      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "27      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "28      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "29      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "...     ...   ...  ...   ...   ...    ...       ...  ...  ...  ...   ...  ...   \n",
              "245020  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245021  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245022  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245023  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245024  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245025  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245026  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245027  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245028  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245029  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245030  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245031  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245032  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245033  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245034  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245035  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245036  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245037  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245038  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245039  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245040  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245041  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245042  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245043  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245044  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245045  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245046  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245047  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245048  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245049  0.0   2.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "\n",
              "        user  \n",
              "0          1  \n",
              "1          1  \n",
              "2          1  \n",
              "3          1  \n",
              "4          1  \n",
              "5          1  \n",
              "6          1  \n",
              "7          1  \n",
              "8          1  \n",
              "9          1  \n",
              "10         1  \n",
              "11         1  \n",
              "12         1  \n",
              "13         1  \n",
              "14         1  \n",
              "15         1  \n",
              "16         1  \n",
              "17         1  \n",
              "18         1  \n",
              "19         1  \n",
              "20         1  \n",
              "21         1  \n",
              "22         1  \n",
              "23         1  \n",
              "24         1  \n",
              "25         1  \n",
              "26         1  \n",
              "27         1  \n",
              "28         1  \n",
              "29         1  \n",
              "...      ...  \n",
              "245020    50  \n",
              "245021    50  \n",
              "245022    50  \n",
              "245023    50  \n",
              "245024    50  \n",
              "245025    50  \n",
              "245026    50  \n",
              "245027    50  \n",
              "245028    50  \n",
              "245029    50  \n",
              "245030    50  \n",
              "245031    50  \n",
              "245032    50  \n",
              "245033    50  \n",
              "245034    50  \n",
              "245035    50  \n",
              "245036    50  \n",
              "245037    50  \n",
              "245038    50  \n",
              "245039    50  \n",
              "245040    50  \n",
              "245041    50  \n",
              "245042    50  \n",
              "245043    50  \n",
              "245044    50  \n",
              "245045    50  \n",
              "245046    50  \n",
              "245047    50  \n",
              "245048    50  \n",
              "245049    50  \n",
              "\n",
              "[245050 rows x 857 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4teyPJwGobpx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = labelled_dataset['user'] - 1\n",
        "dataset = labelled_dataset.drop(columns=['user'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HplyOBjLKoPB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels =  keras.utils.to_categorical(labels, num_classes=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T64W_xRg6Gsx"
      },
      "cell_type": "markdown",
      "source": [
        "Creating the training and testing datasets:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l9nuQQCDNOVW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "training_data, testing_data, training_labels, testing_labels = train_test_split(\n",
        "    dataset,\n",
        "    labels, \n",
        "    test_size=0.10,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eEJNbwsLpB1s"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the Oracle"
      ]
    },
    {
      "metadata": {
        "id": "jhnem8RNQWSs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Following the architecture described in Ryan et al 1998, we create a three-layer backpropagation neural network using Keras."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f_05tZlK5Gbz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7FnkFt645UHM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_layer = Dense(\n",
        "    units=856,\n",
        "    activation='relu',\n",
        "    input_dim=856,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lBbvmT7PDKJ3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_layer = Dense(\n",
        "    units=30,\n",
        "    activation='relu',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "StOQDCqqDM8r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_layer = Dense(\n",
        "    units=50,\n",
        "    activation='softmax',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oOEnF-QFDkfr",
        "outputId": "7fcf928a-3b19-4e68-f0be-c3817c5cec23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "oracle.add(input_layer)\n",
        "oracle.add(hidden_layer)\n",
        "oracle.add(output_layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FhIt-t3UD0Dw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ifDu6QJSpOmZ"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Oracle on Dataset"
      ]
    },
    {
      "metadata": {
        "id": "imoKqfKzQlai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we train the neural network intrusion detection system:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EQlU0zeXLjzw",
        "outputId": "91e6c128-6a5d-410a-cca1-1d4ad5f1fbfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "history = oracle.fit(training_data,  training_labels, epochs=3, batch_size=50, validation_data = (testing_data, testing_labels), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 220545 samples, validate on 24505 samples\n",
            "Epoch 1/3\n",
            "220545/220545 [==============================] - 90s 409us/step - loss: 0.1850 - acc: 0.9466 - val_loss: 0.0638 - val_acc: 0.9794\n",
            "Epoch 2/3\n",
            "220545/220545 [==============================] - 89s 402us/step - loss: 0.0648 - acc: 0.9781 - val_loss: 0.0607 - val_acc: 0.9774\n",
            "Epoch 3/3\n",
            "220545/220545 [==============================] - 89s 403us/step - loss: 0.0527 - acc: 0.9819 - val_loss: 0.0389 - val_acc: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2XhEnbRaQ62s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The trainingn ends with 98.58% accuracy against the test set. Here, we plot it's accuracy over time:"
      ]
    },
    {
      "metadata": {
        "id": "kBgpjHitQ4pi",
        "colab_type": "code",
        "outputId": "c62cbd91-c0c7-48af-faa4-4c8a6c2a16de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiLi5f4PNcVdNh/e1oEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noC5JUkoY1CVJSgmDuiRJKWFQlyQpJX4AYnqhSx7pbUIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nh6yX-0H2rbe"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Substitute Model\n",
        "First, mirror the architecture of the oracle:\n",
        "    "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NlBfPzFMSslC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "substitute = Sequential()\n",
        "\n",
        "input_layer = Dense(\n",
        "    units=856,\n",
        "    activation='relu',\n",
        "    input_dim=856,\n",
        ")\n",
        "hidden_layer = Dense(\n",
        "    units=30,\n",
        "    activation='relu',\n",
        ")\n",
        "output_layer = Dense(\n",
        "    units=50,\n",
        "    activation='softmax',\n",
        ")\n",
        "\n",
        "substitute.add(input_layer)\n",
        "substitute.add(hidden_layer)\n",
        "substitute.add(output_layer)\n",
        "\n",
        "# We need to convert our substitute model into the cleverhans format.\n",
        "substitute = KerasModelWrapper(substitute)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w1SpMeOnVNAl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tensorflow_session = tensorflow.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qGUtASc8WXht"
      },
      "cell_type": "markdown",
      "source": [
        "We start by giving the adversary a small dataset to bootstrap its search. We give it a random sample of 5% of the original data set. \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1PQRJpIqWkEE",
        "outputId": "3159ce3d-3207-430c-c899-6e62c18d1c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_training_set, adversary_test_set = train_test_split(\n",
        "    labelled_dataset,\n",
        "    train_size=0.05,\n",
        "    stratify=labelled_dataset['user'],\n",
        ")\n",
        "\n",
        "adversary_training_inputs = adversary_training_set.drop('user', axis='columns')\n",
        "adversary_training_labels = adversary_training_set['user'] - 1  # keras requires 0 based index\n",
        "\n",
        "# For some reason cleverhans doesn't detect a GPU when it runs, but our models at the top using\n",
        "# keras _do_. I think this creates a type mis-match: code running on the GPU uses numpy.float64\n",
        "# whilst the cleverhans stuff runs on the CPU and extects numpy.float32 (or vica versa).\n",
        "adversary_training_inputs = adversary_training_inputs.values.astype(numpy.float32)\n",
        "adversary_training_labels = adversary_training_labels.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g2ixBGzrbVCW"
      },
      "cell_type": "markdown",
      "source": [
        "Define symbolic input placeholders for use in Tensor Flow:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dtn01RpmawNm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "number_of_users = 50\n",
        "number_of_commands = 856\n",
        "\n",
        "input_placeholder = tensorflow.placeholder(\n",
        "    tensorflow.float32,\n",
        "    shape=(None, number_of_commands)\n",
        ")\n",
        "\n",
        "output_placeholder = tensorflow.placeholder(\n",
        "    tensorflow.float32,\n",
        "    shape=(None, number_of_users)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cqBt04p5byWx"
      },
      "cell_type": "markdown",
      "source": [
        "Get the oracle's predictions for the bootstrap inputs:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tXSbKk1MTlaZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bootstrap_oracle_predictions = oracle.predict(adversary_training_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AW8OmE7dc7wX"
      },
      "cell_type": "markdown",
      "source": [
        "Train substitute using Jacobian Dataset Augmentation:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J5GRMMLCneXs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the predictions and loss of the model, symbolically in TensorFlow (i.e. these variables \n",
        "# point to the result of calculations that haven't been performed yet)\n",
        "\n",
        "substitute_predictions = substitute.get_logits(input_placeholder)\n",
        "substitute_loss = CrossEntropy(substitute, smoothing=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cRko-bZKnbpg",
        "outputId": "a11b075c-a4fb-4959-8204-c04daa508114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1322
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the Jacobian symbolically using TensorFlow\n",
        "grads = jacobian_graph(substitute_predictions, input_placeholder, number_of_users)\n",
        "\n",
        "number_of_dataset_augmentation_batches = 5\n",
        "dataset_augmentation_batch_size = 512\n",
        "\n",
        "\n",
        "stepsize = 1  # this is the step-size of the Jacobian augmentation (we are working in ints so use 1).\n",
        "\n",
        "\n",
        "# Train the substitute and augment dataset\n",
        "for batch in range(number_of_dataset_augmentation_batches):\n",
        "    print(\"BATCH #\" + str(batch))\n",
        "    \n",
        "    print(\"Substitute training epoch:\")\n",
        "    train(\n",
        "        tensorflow_session, \n",
        "        substitute_loss,\n",
        "        adversary_training_inputs, \n",
        "        keras.utils.to_categorical(adversary_training_labels, num_classes=50),\n",
        "        init_all=False,\n",
        "        args={\n",
        "            'nb_epochs': 10,\n",
        "            'batch_size': 32,\n",
        "            'learning_rate': 0.001,\n",
        "        },\n",
        "        rng=rng,\n",
        "    )\n",
        "    \n",
        "\n",
        "    # If we are not at last substitute training iteration, augment dataset\n",
        "    in_final_batch = batch == number_of_dataset_augmentation_batches - 1\n",
        "    if not in_final_batch:\n",
        "        print(\"Generating new data points:\")\n",
        "        \n",
        "        # Use Jacobian augmentation to generate new data points:\n",
        "        step_coef = 2 * int(int(batch / 3) != 0) - 1 \n",
        "\n",
        "        augmented_dataset_inputs = jacobian_augmentation(\n",
        "            tensorflow_session, \n",
        "            input_placeholder, \n",
        "            adversary_training_inputs, \n",
        "            adversary_training_labels,\n",
        "            grads,\n",
        "            step_coef * stepsize,\n",
        "            dataset_augmentation_batch_size,\n",
        "        )\n",
        "        new_datapoints = augmented_dataset_inputs[len(adversary_training_inputs):]\n",
        "\n",
        "        # Send the newly generated data points to the oracle, and use its output as their labels:\n",
        "        new_labels = oracle.predict(new_datapoints)\n",
        "\n",
        "        # Use argmax to get the most likely label. This follows the blackbox attack model - the\n",
        "        # substitute shouldn't be able to see exact prediction confidence.\n",
        "        new_labels = numpy.argmax(new_labels, axis=1)\n",
        "\n",
        "        augmented_dataset_labels = numpy.hstack([adversary_training_labels, new_labels])\n",
        "\n",
        "        # Replace dataset and labels with augmented dataset and labels\n",
        "        adversary_training_inputs = augmented_dataset_inputs\n",
        "        adversary_training_labels = augmented_dataset_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH #0\n",
            "Substitute training epoch:\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/cleverhans/compat.py:124: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/cleverhans/utils_tf.py:511: UserWarning: No GPUS, running on CPU\n",
            "  warnings.warn(\"No GPUS, running on CPU\")\n",
            "[INFO 2019-03-21 15:53:58,357 cleverhans] Epoch 0 took 2.5933899879455566 seconds\n",
            "[INFO 2019-03-21 15:54:01,097 cleverhans] Epoch 1 took 2.520472288131714 seconds\n",
            "[INFO 2019-03-21 15:54:03,866 cleverhans] Epoch 2 took 2.5427777767181396 seconds\n",
            "[INFO 2019-03-21 15:54:06,649 cleverhans] Epoch 3 took 2.5649020671844482 seconds\n",
            "[INFO 2019-03-21 15:54:09,399 cleverhans] Epoch 4 took 2.532803535461426 seconds\n",
            "[INFO 2019-03-21 15:54:12,165 cleverhans] Epoch 5 took 2.556119441986084 seconds\n",
            "[INFO 2019-03-21 15:54:14,976 cleverhans] Epoch 6 took 2.5995938777923584 seconds\n",
            "[INFO 2019-03-21 15:54:17,641 cleverhans] Epoch 7 took 2.4462060928344727 seconds\n",
            "[INFO 2019-03-21 15:54:20,376 cleverhans] Epoch 8 took 2.518057107925415 seconds\n",
            "[INFO 2019-03-21 15:54:23,051 cleverhans] Epoch 9 took 2.4629108905792236 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #1\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 15:54:58,616 cleverhans] Epoch 0 took 5.094738245010376 seconds\n",
            "[INFO 2019-03-21 15:55:04,153 cleverhans] Epoch 1 took 5.0530595779418945 seconds\n",
            "[INFO 2019-03-21 15:55:09,672 cleverhans] Epoch 2 took 5.033262252807617 seconds\n",
            "[INFO 2019-03-21 15:55:15,148 cleverhans] Epoch 3 took 5.001842260360718 seconds\n",
            "[INFO 2019-03-21 15:55:20,495 cleverhans] Epoch 4 took 4.860376358032227 seconds\n",
            "[INFO 2019-03-21 15:55:25,871 cleverhans] Epoch 5 took 4.9023191928863525 seconds\n",
            "[INFO 2019-03-21 15:55:31,288 cleverhans] Epoch 6 took 4.932583332061768 seconds\n",
            "[INFO 2019-03-21 15:55:36,819 cleverhans] Epoch 7 took 5.054207801818848 seconds\n",
            "[INFO 2019-03-21 15:55:42,619 cleverhans] Epoch 8 took 5.290948152542114 seconds\n",
            "[INFO 2019-03-21 15:55:48,106 cleverhans] Epoch 9 took 5.000711917877197 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #2\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 15:56:57,495 cleverhans] Epoch 0 took 10.06018352508545 seconds\n",
            "[INFO 2019-03-21 15:57:08,118 cleverhans] Epoch 1 took 9.12224268913269 seconds\n",
            "[INFO 2019-03-21 15:57:19,077 cleverhans] Epoch 2 took 9.83568787574768 seconds\n",
            "[INFO 2019-03-21 15:57:29,484 cleverhans] Epoch 3 took 9.275397777557373 seconds\n",
            "[INFO 2019-03-21 15:57:40,520 cleverhans] Epoch 4 took 9.917881727218628 seconds\n",
            "[INFO 2019-03-21 15:57:50,930 cleverhans] Epoch 5 took 9.288022994995117 seconds\n",
            "[INFO 2019-03-21 15:58:01,263 cleverhans] Epoch 6 took 9.2102792263031 seconds\n",
            "[INFO 2019-03-21 15:58:11,438 cleverhans] Epoch 7 took 9.058958530426025 seconds\n",
            "[INFO 2019-03-21 15:58:21,637 cleverhans] Epoch 8 took 9.079288482666016 seconds\n",
            "[INFO 2019-03-21 15:58:32,123 cleverhans] Epoch 9 took 9.356579542160034 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #3\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:00:58,947 cleverhans] Epoch 0 took 23.859922170639038 seconds\n",
            "[INFO 2019-03-21 16:01:20,678 cleverhans] Epoch 1 took 18.592378854751587 seconds\n",
            "[INFO 2019-03-21 16:01:42,271 cleverhans] Epoch 2 took 18.59326148033142 seconds\n",
            "[INFO 2019-03-21 16:02:03,350 cleverhans] Epoch 3 took 18.201757431030273 seconds\n",
            "[INFO 2019-03-21 16:02:24,467 cleverhans] Epoch 4 took 18.28147578239441 seconds\n",
            "[INFO 2019-03-21 16:02:46,364 cleverhans] Epoch 5 took 18.084150552749634 seconds\n",
            "[INFO 2019-03-21 16:03:07,903 cleverhans] Epoch 6 took 18.667803049087524 seconds\n",
            "[INFO 2019-03-21 16:03:28,953 cleverhans] Epoch 7 took 18.160480260849 seconds\n",
            "[INFO 2019-03-21 16:03:49,961 cleverhans] Epoch 8 took 18.14861226081848 seconds\n",
            "[INFO 2019-03-21 16:04:11,136 cleverhans] Epoch 9 took 18.30265235900879 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #4\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:09:13,198 cleverhans] Epoch 0 took 36.16341257095337 seconds\n",
            "[INFO 2019-03-21 16:09:55,461 cleverhans] Epoch 1 took 35.284483909606934 seconds\n",
            "[INFO 2019-03-21 16:10:37,583 cleverhans] Epoch 2 took 35.19862914085388 seconds\n",
            "[INFO 2019-03-21 16:11:19,736 cleverhans] Epoch 3 took 35.27438044548035 seconds\n",
            "[INFO 2019-03-21 16:12:02,225 cleverhans] Epoch 4 took 35.563578367233276 seconds\n",
            "[INFO 2019-03-21 16:12:43,431 cleverhans] Epoch 5 took 34.22105932235718 seconds\n",
            "[INFO 2019-03-21 16:13:25,408 cleverhans] Epoch 6 took 35.21517586708069 seconds\n",
            "[INFO 2019-03-21 16:14:08,040 cleverhans] Epoch 7 took 35.65660047531128 seconds\n",
            "[INFO 2019-03-21 16:14:51,129 cleverhans] Epoch 8 took 36.129072427749634 seconds\n",
            "[INFO 2019-03-21 16:15:36,107 cleverhans] Epoch 9 took 38.04812955856323 seconds\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5uGnk1fOTqEX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluating Substitute Model\n",
        "\n",
        "Here we evaluate the substitute against the 95% of the dataset it hasn't seen."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2ACXDjruQtfg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "adversary_test_inputs = adversary_test_set.drop('user', axis='columns')\n",
        "adversary_test_labels = adversary_test_set['user'] - 1  # keras requires 0 based index\n",
        "\n",
        "# For some reason cleverhans doesn't detect a GPU when it runs, but our models at the top using\n",
        "# keras _do_. I think this creates a type mis-match: code running on the GPU uses numpy.float64\n",
        "# whilst the cleverhans stuff runs on the CPU and expects numpy.float32 (or vica versa).\n",
        "adversary_test_inputs = adversary_test_inputs.values.astype(numpy.float32)\n",
        "adversary_test_labels = adversary_test_labels.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPGuaA-mVBPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, check its accuracy against the true labels:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xOOplTcKcpNZ",
        "outputId": "314ede96-2a63-4370-84e2-fa035e193a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_test_labels_one_hot = keras.utils.to_categorical(adversary_test_labels, num_classes=50)\n",
        "\n",
        "# Evaluate the substitute model on clean test examples against true labels\n",
        "acc = model_eval(\n",
        "    tensorflow_session, \n",
        "    input_placeholder,\n",
        "    output_placeholder,\n",
        "    substitute_predictions,\n",
        "    adversary_test_inputs,\n",
        "    adversary_test_labels_one_hot,\n",
        "    args={'batch_size': 32}\n",
        ")\n",
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9497074717136745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "8x0v7i-TT8BS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, send this test dataset into the oracle to get its predictions. Then, compare the substitute model's  predictions against those of the oracle. This is important as it allows us to measure how good of an imitation of the oracle our substitute is."
      ]
    },
    {
      "metadata": {
        "id": "LcsqLdLlT5G_",
        "colab_type": "code",
        "outputId": "31db0b6f-d7aa-40a0-a2a4-cf6995ea7bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "oracle_predicted_labels = oracle.predict(adversary_test_inputs)\n",
        "\n",
        "# Evaluate the substitute model on clean test examples against oracle's labels\n",
        "acc = model_eval(\n",
        "    tensorflow_session, \n",
        "    input_placeholder,\n",
        "    output_placeholder,\n",
        "    substitute_predictions,\n",
        "    adversary_test_inputs,\n",
        "    oracle_predicted_labels,\n",
        "    args={'batch_size': 32}\n",
        ")\n",
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9500468217080903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "Q8F_S8nNUgKT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As one would hope, the substitute model is better as a replica than it is a predictor."
      ]
    },
    {
      "metadata": {
        "id": "AyW24uVuUplz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Synthetic Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fpkj-jxkWvQn"
      },
      "cell_type": "markdown",
      "source": [
        "Just inspecting the generated dataset. Notes:\n",
        "  1. Some of the values are negative!\n",
        "  2. The real dataset has an input range of 0-100. This search technique has found all of them, plus a few on each side.\n",
        "  3. The augmented dataset has just less than 200,000 data points. That's almost as many as were used to train the oracle."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t8HgM1JXTxzj",
        "outputId": "d4ccde35-ee5d-44c8-92e7-8347d0f014f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "numpy.unique(adversary_training_inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,\n",
              "         7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n",
              "        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,\n",
              "        29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,\n",
              "        40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,\n",
              "        51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,\n",
              "        62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,  72.,\n",
              "        73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,\n",
              "        84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
              "        95.,  96.,  97.,  98.,  99., 100., 101., 102.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qS3LcfujWlvM"
      },
      "cell_type": "markdown",
      "source": [
        "# Crafting Adversarial Examples\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m5_76KnirHTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_command_vector(command_vector, danger=False):\n",
        "    # reshape into a rectangle, and pad slightly beforehand\n",
        "    rectangle_array= numpy.concatenate([command_vector, numpy.array([0,0])]).reshape((26,33))\n",
        "    normalized_array = sklearn.preprocessing.normalize(rectangle_array)\n",
        "    \n",
        "    color = 'Reds' if danger else 'Greens'\n",
        "    \n",
        "    return seaborn.heatmap(\n",
        "        normalized_array,\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        vmin=0, \n",
        "        vmax=1,\n",
        "        cmap=color,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzOmrmaTbgut",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Fast Gradient Sign Method\n",
        "\n",
        "Build an attack using the Fast Gradient Sign method. Then generate untargeted adversarial examples for each value in our test set."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EziI1AYUcqk_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the Fast Gradient Sign Method (FGSM) attack object.\n",
        "fgsm_par = {'eps': 1., 'ord': numpy.inf, 'clip_min': 0., 'clip_max': 100.}\n",
        "fgsm = FastGradientMethod(substitute, sess=tensorflow_session)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uuP-pUO4cr4b",
        "outputId": "bf75ccf0-7787-44b8-b5ce-c90e2c0780e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Craft adversarial examples using the substitute\n",
        "eval_params = {'batch_size': dataset_augmentation_batch_size}\n",
        "x_adv_sub = fgsm.generate(input_placeholder, **fgsm_par)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/cleverhans/attacks/__init__.py:283: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xwDuDMCDC9AY",
        "colab_type": "code",
        "outputId": "48159386-69dc-49da-f92b-7a9a25a53757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_adv_sub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Identity:0' shape=(?, 856) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "uxwrvuwCC9Ae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle_keras = KerasModelWrapper(oracle)\n",
        "oracle_fgsm_pred = oracle_keras.get_logits(x_adv_sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ViBivzAMC9Ap",
        "colab_type": "code",
        "outputId": "37f4dbfc-88f6-4c12-8850-7ed9ccbb34d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "oracle_fgsm_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'model_2/dense_3/BiasAdd:0' shape=(?, 50) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "3WionbZaYHcd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the cleverhans model_eval function, we can see how good our attack is at fooling the oracle, in general. Because the score for accuray is low, the attack has successfully made the oracle misclassify inputs - in 98% of cases."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZKJui3cVctC5",
        "outputId": "1270568c-2f81-494c-c786-d464bd9edffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the \"black-box\" model on adversarial examples\n",
        "accuracy = model_eval(\n",
        "        tensorflow_session,\n",
        "        input_placeholder,\n",
        "        output_placeholder,\n",
        "        oracle_fgsm_pred,\n",
        "        adversary_test_inputs,\n",
        "        adversary_test_labels_one_hot,\n",
        "        args=eval_params\n",
        ")\n",
        "print('Test accuracy of oracle on adversarial examples generated '\n",
        "    'using the substitute: ' + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy of oracle on adversarial examples generated using the substitute: 0.021834380020446912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CC9zxfxrC9A9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is an example of an untargeted attack: we are trying to get the oracle to misclassify activity from a genuine user. "
      ]
    },
    {
      "metadata": {
        "id": "9RTbGaCxC9A_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Targetting Particular Users\n"
      ]
    },
    {
      "metadata": {
        "id": "ihlyeUfXNqNn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define a function below to take in one particular command vector, and perform a targeted attack against each of the users."
      ]
    },
    {
      "metadata": {
        "id": "wBmYQjCowOFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_targeted_attack_against_all_users(command_vector, label, attack, attack_params):\n",
        "    \"\"\"\n",
        "    Runs a targeted attack for the given sample against. For each user, we attempt to generate a\n",
        "    similar command vector to the original, which is classified as that user.\n",
        "    \n",
        "    The command prints a summary of the results to stdout, then returns a dataframe containing, \n",
        "    for each attack:\n",
        "      - the original user\n",
        "      - the targeted user\n",
        "      - the oracle models prediction against the adversarial example\n",
        "      - the oracles certainty of that prediction\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Since we run this once against all users, make 50 replicas of the command vector, and\n",
        "    # original label:\n",
        "    original_label_one_hot = keras.utils.to_categorical(label, num_classes=50)\n",
        "    original_labels = numpy.tile(original_label_one_hot, reps=(50,1))\n",
        "    \n",
        "    original_command_vectors = numpy.tile(command_vector, reps=(50, 1))\n",
        "    \n",
        "    # Our target labels are the one-hot-encoded values 0, 1, 2, ..., 49:\n",
        "    target_labels = keras.utils.to_categorical(range(50), num_classes=50)\n",
        "\n",
        "    attack_params['y_target'] = target_labels\n",
        "    \n",
        "    # Apply the attack, generating the adversarial examples:\n",
        "    adversarial_examples = attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        **attack_params,\n",
        "    )\n",
        "\n",
        "    # Stick these examples into the oracle, and find out what classification it gives:\n",
        "    predictions = oracle.predict(adversarial_examples)\n",
        "\n",
        "    # Format the results into a summary dataframe:\n",
        "    original_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=original_labels), # undo one hot encode\n",
        "        name='Original User',\n",
        "    )\n",
        "    target_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=target_labels), # undo one hot encode\n",
        "        name='Target User',\n",
        "    )\n",
        "    predicted_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=predictions),  # undo one hot encode\n",
        "        name='Oracle Prediction',\n",
        "    )\n",
        "    prediction_certainty = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.max, axis=1, arr=predictions),\n",
        "        name='Oracle Certainty',\n",
        "    )\n",
        "\n",
        "    summary = pandas.concat(\n",
        "        [\n",
        "            original_label,\n",
        "            target_label,\n",
        "            predicted_label,\n",
        "            prediction_certainty,\n",
        "        ],\n",
        "        axis='columns',\n",
        "    )\n",
        "    \n",
        "    # Count the number of targeted attacks which were succcessful:\n",
        "    successful_attacks = summary.apply(lambda row: row[1] == row[2], axis='columns').sum()\n",
        "    \n",
        "    # Don't count  the original_user -> original_user attack:\n",
        "    successful_attacks -= 1 \n",
        "    total_attacks = 49\n",
        "    \n",
        "    # Print out a little message to say how we did :)\n",
        "    print(\n",
        "        \"A targeted attack was successful against {}/{} users (with the given input):\"\n",
        "        .format(successful_attacks, total_attacks)\n",
        "    )\n",
        "\n",
        "    return summary, adversarial_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B0QEFE1qMqVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, below we take a command vector not yet seen by the substitute model and try to craft adversarial examples targeted at each user. As you can see, the attack is relatively unsucessful."
      ]
    },
    {
      "metadata": {
        "id": "bOi6MhmAMm0c",
        "colab_type": "code",
        "outputId": "27f2340a-b488-42f4-8cbe-eab99285f5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        }
      },
      "cell_type": "code",
      "source": [
        "summary, adversarial_examples = run_targeted_attack_against_all_users(\n",
        "    adversary_test_inputs[0],\n",
        "    adversary_test_labels[0],\n",
        "    fgsm,\n",
        "    fgsm_par,\n",
        ")\n",
        "summary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:20:04,079 cleverhans] Constructing new graph for attack FastGradientMethod\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A targeted attack was successful against 8/49 users (with the given input):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original User</th>\n",
              "      <th>Target User</th>\n",
              "      <th>Oracle Prediction</th>\n",
              "      <th>Oracle Certainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0.999919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>0.930165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>0.793800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>0.999979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.998683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>0.906611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>45</td>\n",
              "      <td>0.566652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0.904598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>43</td>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>0.981476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>0.972488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>27</td>\n",
              "      <td>0.558632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>43</td>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>43</td>\n",
              "      <td>16</td>\n",
              "      <td>26</td>\n",
              "      <td>0.933712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>43</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>0.962532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>0.849187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>0.971559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43</td>\n",
              "      <td>22</td>\n",
              "      <td>45</td>\n",
              "      <td>0.999857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>43</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>45</td>\n",
              "      <td>0.947013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>0.999166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>43</td>\n",
              "      <td>28</td>\n",
              "      <td>11</td>\n",
              "      <td>0.592794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43</td>\n",
              "      <td>29</td>\n",
              "      <td>9</td>\n",
              "      <td>0.849746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>27</td>\n",
              "      <td>0.959036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>26</td>\n",
              "      <td>0.996926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>9</td>\n",
              "      <td>0.296662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>33</td>\n",
              "      <td>26</td>\n",
              "      <td>0.955576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>43</td>\n",
              "      <td>34</td>\n",
              "      <td>11</td>\n",
              "      <td>0.889263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>0.999981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>43</td>\n",
              "      <td>37</td>\n",
              "      <td>43</td>\n",
              "      <td>0.958634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "      <td>49</td>\n",
              "      <td>0.814296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>43</td>\n",
              "      <td>39</td>\n",
              "      <td>26</td>\n",
              "      <td>0.529316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>40</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>12</td>\n",
              "      <td>0.940027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>42</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>0.999699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0.999800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>45</td>\n",
              "      <td>0.631968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>9</td>\n",
              "      <td>0.903847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>43</td>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>0.999981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Original User  Target User  Oracle Prediction  Oracle Certainty\n",
              "0              43            0                 18          0.999919\n",
              "1              43            1                 18          0.930165\n",
              "2              43            2                 26          0.793800\n",
              "3              43            3                 27          0.999979\n",
              "4              43            4                  2          0.998683\n",
              "5              43            5                 11          0.906611\n",
              "6              43            6                  2          1.000000\n",
              "7              43            7                 45          0.566652\n",
              "8              43            8                  7          0.904598\n",
              "9              43            9                  9          1.000000\n",
              "10             43           10                 26          0.981476\n",
              "11             43           11                 11          0.972488\n",
              "12             43           12                 45          1.000000\n",
              "13             43           13                 45          1.000000\n",
              "14             43           14                 27          0.558632\n",
              "15             43           15                 26          0.999389\n",
              "16             43           16                 26          0.933712\n",
              "17             43           17                 11          0.962532\n",
              "18             43           18                  2          0.849187\n",
              "19             43           19                 26          0.971559\n",
              "20             43           20                  9          1.000000\n",
              "21             43           21                 45          1.000000\n",
              "22             43           22                 45          0.999857\n",
              "23             43           23                 23          1.000000\n",
              "24             43           24                 45          0.947013\n",
              "25             43           25                 26          0.999979\n",
              "26             43           26                 26          0.999915\n",
              "27             43           27                 27          0.999166\n",
              "28             43           28                 11          0.592794\n",
              "29             43           29                  9          0.849746\n",
              "30             43           30                 27          0.959036\n",
              "31             43           31                 26          0.996926\n",
              "32             43           32                  9          0.296662\n",
              "33             43           33                 26          0.955576\n",
              "34             43           34                 11          0.889263\n",
              "35             43           35                 35          0.999999\n",
              "36             43           36                 36          0.999981\n",
              "37             43           37                 43          0.958634\n",
              "38             43           38                 49          0.814296\n",
              "39             43           39                 26          0.529316\n",
              "40             43           40                 26          0.999995\n",
              "41             43           41                 12          0.940027\n",
              "42             43           42                 26          0.999425\n",
              "43             43           43                  9          0.999699\n",
              "44             43           44                  2          0.999800\n",
              "45             43           45                 45          1.000000\n",
              "46             43           46                 45          0.631968\n",
              "47             43           47                  9          0.903847\n",
              "48             43           48                  2          0.999981\n",
              "49             43           49                 49          0.999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "mspvKRbob1iV",
        "colab_type": "code",
        "outputId": "3ecd7ff5-7a0c-4905-abda-7d253e2b6a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversary_test_inputs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f8733eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCC0AiiG0ACiG0AKgGP8PrKZu5ERULWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wURvWArRb2Ws",
        "colab_type": "code",
        "outputId": "85e46a84-d8f8-4fee-a7f8-047ed58b5639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversarial_examples[0], danger=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f86a82e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n47tcVVBHIgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "EkFQOGwDcHvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As can be seen in the two plots, the key components i.e. most used commands are left unchanged, whilst FGSM creates background noise to cause the misclassification attempt.. "
      ]
    },
    {
      "metadata": {
        "id": "ADD0NPmjVd5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Developing a Specialised Attack Method\n",
        "\n",
        "Most adversarial attacks are mounted against image classification networks. They aim to minimise visual difference in their generated examples. Our attack has a different set of requirements:\n",
        "\n",
        "  - The set of commands should perform an equivelant task on the target computer.\n",
        "  - We can append the script with as many commands as we like.\n",
        "  \n",
        "In the section below, we modify the Momentum Iterative Method introduced by Dong et al  (2017) to produce additive perturbations only:"
      ]
    },
    {
      "metadata": {
        "id": "4RG1Zq-uklS4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from cleverhans.attacks import optimize_linear\n",
        "from cleverhans.compat import reduce_sum, reduce_mean, softmax_cross_entropy_with_logits\n",
        "from cleverhans import utils_tf\n",
        "\n",
        "\n",
        "class AdditiveMomentumIterativeMethod(cleverhans.attacks.MomentumIterativeMethod):\n",
        "  \"\"\"\n",
        "  Modifies the The Momentum Iterative Method (Dong et al. 2017) to produce additive\n",
        "  perturbations only.\n",
        "  \n",
        "  If it finds the optimal perturbation to be negative, a random addition is performed instead.\n",
        "  \n",
        "  Original paper link: https://arxiv.org/pdf/1710.06081.pdf\n",
        "  \n",
        "  The majority of the code from this cell comes directly from the cleverhans\n",
        "  module (https://github.com/tensorflow/cleverhans/) and is licensed similarly under the MIT\n",
        "  license (https://github.com/tensorflow/cleverhans/blob/master/LICENSE).\n",
        "  \n",
        "  :param model: cleverhans.model.Model\n",
        "  :param sess: optional tf.Session\n",
        "  :param dtypestr: dtype of the data\n",
        "  :param kwargs: passed through to super constructor\n",
        "  \"\"\"\n",
        "\n",
        "  def generate(self, x, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate symbolic graph for adversarial examples and return.\n",
        "    :param x: The model's symbolic inputs.\n",
        "    :param kwargs: Keyword arguments. See `parse_params` for documentation.\n",
        "    \"\"\"\n",
        "    # Parse and save attack-specific parameters\n",
        "    assert self.parse_params(**kwargs)\n",
        "\n",
        "    asserts = []\n",
        "\n",
        "    # If a data range was specified, check that the input was in that range\n",
        "    if self.clip_min is not None:\n",
        "      asserts.append(utils_tf.assert_greater_equal(x,\n",
        "                                                   tf.cast(self.clip_min,\n",
        "                                                           x.dtype)))\n",
        "\n",
        "    if self.clip_max is not None:\n",
        "      asserts.append(utils_tf.assert_less_equal(x,\n",
        "                                                tf.cast(self.clip_max,\n",
        "                                                        x.dtype)))\n",
        "\n",
        "    # Initialize loop variables\n",
        "    momentum = tf.zeros_like(x)\n",
        "    adv_x = x\n",
        "\n",
        "    # Fix labels to the first model predictions for loss computation\n",
        "    y, _nb_classes = self.get_or_guess_labels(x, kwargs)\n",
        "    y = y / reduce_sum(y, 1, keepdims=True)\n",
        "    targeted = (self.y_target is not None)\n",
        "\n",
        "    def cond(i, _, __):\n",
        "      \"\"\"Iterate until number of iterations completed\"\"\"\n",
        "      return tf.less(i, self.nb_iter)\n",
        "\n",
        "    def body(i, ax, m):\n",
        "      \"\"\"Do a momentum step\"\"\"\n",
        "      logits = self.model.get_logits(ax)\n",
        "      loss = softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "      if targeted:\n",
        "        loss = -loss\n",
        "\n",
        "      # Define gradient of loss wrt input\n",
        "      grad, = tf.gradients(loss, ax)\n",
        "\n",
        "      # Normalize current gradient and add it to the accumulated gradient\n",
        "      red_ind = list(range(1, len(grad.get_shape())))\n",
        "      avoid_zero_div = tf.cast(1e-12, grad.dtype)\n",
        "      grad = grad / tf.maximum(\n",
        "          avoid_zero_div,\n",
        "          reduce_mean(tf.abs(grad), red_ind, keepdims=True))\n",
        "      m = self.decay_factor * m + grad\n",
        "\n",
        "      optimal_perturbation = optimize_linear(m, self.eps_iter, self.ord)\n",
        "      optimal_perturbation = tf.maximum(optimal_perturbation, tf.zeros_like(optimal_perturbation))\n",
        "        \n",
        "      if self.ord == 1:\n",
        "        raise NotImplementedError(\"This attack hasn't been tested for ord=1.\"\n",
        "                                  \"It's not clear that FGM makes a good inner \"\n",
        "                                  \"loop step for iterative optimization since \"\n",
        "                                  \"it updates just one coordinate at a time.\")\n",
        "\n",
        "      # Update and clip adversarial example in current iteration\n",
        "      ax = ax + optimal_perturbation\n",
        "      ax = x + utils_tf.clip_eta(ax - x, self.ord, self.eps)\n",
        "\n",
        "      if self.clip_min is not None and self.clip_max is not None:\n",
        "        ax = utils_tf.clip_by_value(ax, self.clip_min, self.clip_max)\n",
        "\n",
        "      ax = tf.stop_gradient(ax)\n",
        "\n",
        "      return i + 1, ax, m\n",
        "\n",
        "    _, adv_x, _ = tf.while_loop(\n",
        "        cond, body, (tf.zeros([]), adv_x, momentum), back_prop=True,\n",
        "        maximum_iterations=self.nb_iter)\n",
        "\n",
        "    if self.sanity_checks:\n",
        "      with tf.control_dependencies(asserts):\n",
        "        adv_x = tf.identity(adv_x)\n",
        "\n",
        "    return adv_x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctn1LalQnU4P",
        "colab_type": "code",
        "outputId": "56349683-34b0-4990-e6b4-866175f8b738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        }
      },
      "cell_type": "code",
      "source": [
        "ami_attack = AdditiveMomentumIterativeMethod(substitute, sess=tensorflow_session)\n",
        "ami_params = {\n",
        "    'eps': 100.0,\n",
        "    'eps_iter': 1.0,\n",
        "    'nb_iter': 100,\n",
        "    'ord': numpy.inf,\n",
        "    'clip_min': 0.0,\n",
        "    'clip_max': 100.0,\n",
        "}\n",
        "\n",
        "summary, adversarial_examples = run_targeted_attack_against_all_users(\n",
        "    adversary_test_inputs[0],\n",
        "    adversary_test_labels[0],\n",
        "    ami_attack,\n",
        "    ami_params,\n",
        ")\n",
        "summary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:20:31,164 cleverhans] Constructing new graph for attack AdditiveMomentumIterativeMethod\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A targeted attack was successful against 22/49 users (with the given input):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original User</th>\n",
              "      <th>Target User</th>\n",
              "      <th>Oracle Prediction</th>\n",
              "      <th>Oracle Certainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>43</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>43</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>43</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>43</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>43</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>35</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>43</td>\n",
              "      <td>28</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43</td>\n",
              "      <td>29</td>\n",
              "      <td>38</td>\n",
              "      <td>0.997517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>33</td>\n",
              "      <td>14</td>\n",
              "      <td>0.560974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>43</td>\n",
              "      <td>34</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>43</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "      <td>49</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>43</td>\n",
              "      <td>39</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>40</td>\n",
              "      <td>8</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>42</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>19</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>43</td>\n",
              "      <td>48</td>\n",
              "      <td>48</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "      <td>39</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Original User  Target User  Oracle Prediction  Oracle Certainty\n",
              "0              43            0                  0          1.000000\n",
              "1              43            1                  1          1.000000\n",
              "2              43            2                  2          1.000000\n",
              "3              43            3                  2          1.000000\n",
              "4              43            4                  2          1.000000\n",
              "5              43            5                  5          1.000000\n",
              "6              43            6                  6          1.000000\n",
              "7              43            7                 12          1.000000\n",
              "8              43            8                  8          1.000000\n",
              "9              43            9                  9          1.000000\n",
              "10             43           10                 20          1.000000\n",
              "11             43           11                 11          1.000000\n",
              "12             43           12                 12          1.000000\n",
              "13             43           13                 44          1.000000\n",
              "14             43           14                 14          1.000000\n",
              "15             43           15                  2          1.000000\n",
              "16             43           16                  2          1.000000\n",
              "17             43           17                 11          1.000000\n",
              "18             43           18                 18          1.000000\n",
              "19             43           19                 19          1.000000\n",
              "20             43           20                 20          1.000000\n",
              "21             43           21                 11          1.000000\n",
              "22             43           22                 22          1.000000\n",
              "23             43           23                 23          1.000000\n",
              "24             43           24                 35          1.000000\n",
              "25             43           25                  0          1.000000\n",
              "26             43           26                 26          1.000000\n",
              "27             43           27                 27          1.000000\n",
              "28             43           28                  9          1.000000\n",
              "29             43           29                 38          0.997517\n",
              "30             43           30                  2          1.000000\n",
              "31             43           31                 12          1.000000\n",
              "32             43           32                 32          1.000000\n",
              "33             43           33                 14          0.560974\n",
              "34             43           34                 11          1.000000\n",
              "35             43           35                 35          1.000000\n",
              "36             43           36                  2          1.000000\n",
              "37             43           37                 37          1.000000\n",
              "38             43           38                 49          1.000000\n",
              "39             43           39                  2          1.000000\n",
              "40             43           40                  8          1.000000\n",
              "41             43           41                 12          1.000000\n",
              "42             43           42                 12          1.000000\n",
              "43             43           43                  1          1.000000\n",
              "44             43           44                 44          1.000000\n",
              "45             43           45                 45          1.000000\n",
              "46             43           46                 12          1.000000\n",
              "47             43           47                 19          1.000000\n",
              "48             43           48                 48          1.000000\n",
              "49             43           49                 39          1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "9SUkSJc2u67k",
        "colab_type": "code",
        "outputId": "fc274207-3d71-4470-c2f2-f5dbfa76b268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversary_test_inputs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f86232e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCC0AiiG0ACiG0AKgGP8PrKZu5ERULWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LlZG6VThu-CL",
        "colab_type": "code",
        "outputId": "d3a615a6-bf9b-41c2-f856-cde6044765e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversarial_examples[0], danger=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f83e7e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n4LMIjjaMbmb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So our modified method gives much better results for targeted attacks, lets see how it compares for untargeted attacks. We have tried to do this below, but the commented part breaks."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IegvI9jAMaJz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Craft adversarial examples using the substitute\n",
        "x_adv_sub_ami = ami_attack.generate(input_placeholder, **ami_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "ba072e72-9e56-4873-9c0d-944ec9bc5c99",
        "id": "CvgUg5w6MaKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_adv_sub_ami"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Identity_3:0' shape=(?, 856) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6LGNE2HLMaK1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle_ami_test = KerasModelWrapper(oracle)\n",
        "oracle_ami_pred = oracle_ami_test.get_logits(x_adv_sub_ami)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b968b217-e6ef-47af-9a94-ae8e773ea7f2",
        "id": "Lvqn-37vMaLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_test_labels_one_hot.shape, adversary_test_inputs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((232798, 50), (232798, 856))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B9LNGbH5MaLQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Evaluate the accuracy of the \"black-box\" model on adversarial examples [BROKEN]\n",
        "# untargetted_accuracy_ami = model_eval(\n",
        "#         tensorflow_session,\n",
        "#         input_placeholder,\n",
        "#         output_placeholder,\n",
        "#         oracle_ami_pred,\n",
        "#         adversary_test_inputs,\n",
        "#         adversary_test_labels_one_hot,\n",
        "#         args=eval_params\n",
        "# )\n",
        "# print('Test accuracy of oracle on adversarial examples generated '\n",
        "#     'using the substitute: ' + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lg3WsDFbVs0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# An End-to-End Attack\n",
        "\n",
        "In this section, we use the models and attacks developed above to perform a complete attack on the intrusion detection system.\n",
        "\n",
        "First we define two functions:\n",
        "  1. `script_to_command_vector` :: converts a list of bash commands into a command vector (as if it were generated by `acct`).\n",
        "  2. `pad_script` :: takes an input script and generats an output script with the same behaviour, but with the command counts specified by command_vector."
      ]
    },
    {
      "metadata": {
        "id": "EY8jUsNCKSxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def script_to_command_vector(script):\n",
        "    lines = script.split(\"\\n\")  # ['netscape', 'sh ./my-script.sh', ...]\n",
        "    commands = [\n",
        "        line.split(\" \")[0] for line in lines\n",
        "    ]  # ['netscape', 'sh', ...]\n",
        "    \n",
        "    commands = pandas.Series(commands).astype(command_dtype)\n",
        "    commands_one_hot = pandas.get_dummies(commands)\n",
        "    command_counts = commands_one_hot.sum()\n",
        "    \n",
        "    return command_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clLauLU2Wu7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For our proof-of-concept, just append --help to turn our commands into no-ops. This won't\n",
        "# actually work for all of these commands, but proves the point.\n",
        "COMMAND_TO_NOOP = {command: command + \" --help\" for command in commands}\n",
        "\n",
        "def pad_script(original_script, target_command_counts):\n",
        "    # First, calculate the command counts of the input script:\n",
        "    original_command_counts = script_to_command_vector(original_script)\n",
        "    \n",
        "    # Find the number of each command we need to pad by:\n",
        "    additional_command_counts = target_command_counts - original_command_counts\n",
        "    \n",
        "    # Loop over additional_command_counts and append no-op commands for each additional\n",
        "    # command needed:\n",
        "    padded_script = original_script\n",
        "    \n",
        "    pandas.Series(additional_command_counts).astype(command_dtype)\n",
        "    \n",
        "    for index, count in enumerate(additional_command_counts):\n",
        "        command = labelled_dataset.columns[index]\n",
        "        \n",
        "        for _ in range(int(count)):\n",
        "            padded_script += \"\\n \" + COMMAND_TO_NOOP[command]\n",
        "         \n",
        "    \n",
        "    return padded_script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbefZdBhEESX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we define the `masq()` function, the entry point to our attack, and run it on the example from the main report. The output is very long, but note theres one more code cell at the very end, which you can modify with your own examples!"
      ]
    },
    {
      "metadata": {
        "id": "FqrpRkx4Wq1p",
        "colab_type": "code",
        "outputId": "76f79dcb-7565-46fd-bc72-8691c1876249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12695
        }
      },
      "cell_type": "code",
      "source": [
        "def masq(script, target_user, aggressiveness=1.0):\n",
        "    command_vector = script_to_command_vector(script)\n",
        "    original_command_vectors = numpy.array([command_vector])\n",
        "\n",
        "    target_labels = keras.utils.to_categorical(numpy.array([target_user]), num_classes=50)\n",
        "    \n",
        "    ami_params['y_target'] = target_labels\n",
        "    ami_params['eps'] = int(aggressiveness*100)\n",
        "\n",
        "    adversarial_examples = ami_attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        **ami_params\n",
        "    )\n",
        "\n",
        "    predicted_labels = oracle.predict(adversarial_examples)\n",
        "    \n",
        "    adversarial_example = adversarial_examples[0]\n",
        "    predicted_label = numpy.argmax(predicted_labels[0])\n",
        "    \n",
        "    if predicted_label == target_user:\n",
        "        fool = 'We have fooled the model to predict user ' + str(target_user) + '. Here is your attack script: \\n'\n",
        "        fool += pad_script(script, adversarial_example)\n",
        "    else:\n",
        "        fool = 'The model has predicted user ' + str(predicted_label) +' We have failed to fool the model... oh dear.'\n",
        "        \n",
        "    print(fool)\n",
        "    \n",
        "\n",
        "# Run masq( ) on the example from our report:\n",
        "masq(\n",
        "    script=\"\"\"\n",
        "ls\n",
        "scp -r top-secret-research dans-server:/my-stash-of-stolen-files\n",
        "sudo rm top-secret-research/data-source.csv\n",
        "head -n 100 < /dev/urandom | tr -dc A-Za-z0-9 > top-secret-research/data-source.csv\n",
        "    \"\"\",\n",
        "    target_user=0,\n",
        "    aggressiveness=0.01,  # aggressiveness of 0.01 works fo 16/49 users\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have fooled the model to predict user 0. Here is your attack script: \n",
            "\n",
            "ls\n",
            "scp -r top-secret-research dans-server:/my-stash-of-stolen-files\n",
            "sudo rm top-secret-research/data-source.csv\n",
            "head -n 100 < /dev/urandom | tr -dc A-Za-z0-9 > top-secret-research/data-source.csv\n",
            "    \n",
            " %backup% --help\n",
            " .java_wr --help\n",
            " .maker_w --help\n",
            " .wrapper --help\n",
            " .xinitrc --help\n",
            " .xsessio --help\n",
            " 1.1 --help\n",
            " 1.3 --help\n",
            " 4Dwm --help\n",
            " 5650.exe --help\n",
            " 5836.exe --help\n",
            " 7105.exe --help\n",
            " 8117.exe --help\n",
            " 8708.exe --help\n",
            " 9term --help\n",
            " = --help\n",
            " == --help\n",
            " =p --help\n",
            " Archie --help\n",
            " BATCH --help\n",
            " CC --help\n",
            " Configur --help\n",
            " DC-prn --help\n",
            " FIFO --help\n",
            " FvwmPage --help\n",
            " LOCK --help\n",
            " Mail --help\n",
            " Main --help\n",
            " MakeTeXP --help\n",
            " MediaMai --help\n",
            " Mosaic --help\n",
            " OLI.sh --help\n",
            " PLATFORM --help\n",
            " R --help\n",
            " Reducyr --help\n",
            " Sizup --help\n",
            " Slmclien --help\n",
            " Slmhelpe --help\n",
            " Sqpe --help\n",
            " Tracy --help\n",
            " UNLOCK --help\n",
            " X --help\n",
            " Xremote --help\n",
            " [ --help\n",
            " a.out --help\n",
            " aa.new.n --help\n",
            " aa.new.s --help\n",
            " aacdec --help\n",
            " acc.prof --help\n",
            " accesspo --help\n",
            " acroread --help\n",
            " admin --help\n",
            " agrep --help\n",
            " aiffplay --help\n",
            " ama.chec --help\n",
            " apanel --help\n",
            " appdefpa --help\n",
            " ar --help\n",
            " arp --help\n",
            " array_te --help\n",
            " as --help\n",
            " ascii --help\n",
            " at --help\n",
            " augment_ --help\n",
            " aupanel --help\n",
            " auplay --help\n",
            " aus --help\n",
            " autoconf --help\n",
            " awk --help\n",
            " awk.html --help\n",
            " basename --help\n",
            " bash --help\n",
            " bb_rep --help\n",
            " bb_rep_f --help\n",
            " bb_rep_n --help\n",
            " bb_rep_t --help\n",
            " bc --help\n",
            " bdftopcf --help\n",
            " bdiff --help\n",
            " be --help\n",
            " bind_so_ --help\n",
            " bindkey --help\n",
            " binhex --help\n",
            " bison --help\n",
            " blossom4 --help\n",
            " bo_rep --help\n",
            " bo_rep_c --help\n",
            " bo_rep_f --help\n",
            " bo_rep_t --help\n",
            " bo_table --help\n",
            " bo_top --help\n",
            " bo_type --help\n",
            " btbuild --help\n",
            " btcreat --help\n",
            " byte_rev --help\n",
            " c++filt --help\n",
            " c++patch --help\n",
            " cal --help\n",
            " calendar --help\n",
            " call_fil --help\n",
            " calldd --help\n",
            " calprog --help\n",
            " cancel --help\n",
            " cat --help\n",
            " catalog --help\n",
            " catdoc --help\n",
            " cc1 --help\n",
            " cdc --help\n",
            " cdec --help\n",
            " cfe --help\n",
            " cgiparse --help\n",
            " chat.awk --help\n",
            " chkconfi --help\n",
            " chmod --help\n",
            " chown --help\n",
            " ci --help\n",
            " cled --help\n",
            " cled_jct --help\n",
            " cmex --help\n",
            " co --help\n",
            " col --help\n",
            " colthloo --help\n",
            " colthrea --help\n",
            " comm --help\n",
            " comma.te --help\n",
            " comp_uni --help\n",
            " compress --help\n",
            " concorde --help\n",
            " config.g --help\n",
            " config.s --help\n",
            " configur --help\n",
            " conftest --help\n",
            " convert --help\n",
            " cpeek --help\n",
            " cpio --help\n",
            " cplex --help\n",
            " cpp --help\n",
            " crnl --help\n",
            " crontab --help\n",
            " csh --help\n",
            " ctags --help\n",
            " cut --help\n",
            " cxwsh --help\n",
            " data_cl. --help\n",
            " date --help\n",
            " dbl --help\n",
            " dbx --help\n",
            " dbxpcs --help\n",
            " dc --help\n",
            " dd --help\n",
            " ddd --help\n",
            " ddtest --help\n",
            " dec --help\n",
            " delatex --help\n",
            " delta --help\n",
            " demo --help\n",
            " deroff --help\n",
            " desktopM --help\n",
            " detail_o --help\n",
            " detex --help\n",
            " dev.X11 --help\n",
            " dev.moti --help\n",
            " dev.post --help\n",
            " df --help\n",
            " dialog.s --help\n",
            " dict --help\n",
            " diff --help\n",
            " dig --help\n",
            " dirname --help\n",
            " do.hourl --help\n",
            " do.priso --help\n",
            " do.trit --help\n",
            " doc2ps --help\n",
            " doctype --help\n",
            " domainna --help\n",
            " dot --help\n",
            " download --help\n",
            " dpost --help\n",
            " dprog --help\n",
            " drag --help\n",
            " drag2 --help\n",
            " drawgrap --help\n",
            " drf --help\n",
            " drill_do --help\n",
            " driver --help\n",
            " ds_ar --help\n",
            " du --help\n",
            " dummy --help\n",
            " dvipost --help\n",
            " dviselec --help\n",
            " e --help\n",
            " echo --help\n",
            " ed --help\n",
            " edg_prel --help\n",
            " edgcpfe --help\n",
            " edgegen --help\n",
            " efm --help\n",
            " egrep --help\n",
            " elm --help\n",
            " emacs-20 --help\n",
            " enc --help\n",
            " endsessi --help\n",
            " engine --help\n",
            " enscript --help\n",
            " env --help\n",
            " eptofax --help\n",
            " eqn --help\n",
            " euphony --help\n",
            " euphony3 --help\n",
            " ex --help\n",
            " expr --help\n",
            " exrecove --help\n",
            " extract_ --help\n",
            " f --help\n",
            " f2ps --help\n",
            " fa.booku --help\n",
            " fa.click --help\n",
            " faces --help\n",
            " false --help\n",
            " fcom --help\n",
            " fec --help\n",
            " fecc --help\n",
            " fgrep --help\n",
            " field --help\n",
            " fig2dev --help\n",
            " file --help\n",
            " find --help\n",
            " find_RT --help\n",
            " findobj --help\n",
            " finger --help\n",
            " fish2 --help\n",
            " fish4 --help\n",
            " flex --help\n",
            " flog --help\n",
            " flow --help\n",
            " fls_star --help\n",
            " fm --help\n",
            " fm_flb --help\n",
            " fm_misd --help\n",
            " fmarch --help\n",
            " fmprintd --help\n",
            " fmt --help\n",
            " fold --help\n",
            " foo --help\n",
            " force_up --help\n",
            " format.d --help\n",
            " frm --help\n",
            " ftp --help\n",
            " ftp.orig --help\n",
            " fvwm --help\n",
            " fx --help\n",
            " fxfilter --help\n",
            " fxprint --help\n",
            " fxsend --help\n",
            " fxshut --help\n",
            " fxstat --help\n",
            " fxstatus --help\n",
            " fxvision --help\n",
            " gawk --help\n",
            " gcc --help\n",
            " gdb --help\n",
            " gdiff --help\n",
            " generic --help\n",
            " gengraph --help\n",
            " get --help\n",
            " get.line --help\n",
            " get_acc --help\n",
            " get_acc_ --help\n",
            " get_line --help\n",
            " getans --help\n",
            " getconf --help\n",
            " gethost --help\n",
            " getopt --help\n",
            " getpgrp --help\n",
            " getsampl --help\n",
            " gettxt --help\n",
            " gftopk --help\n",
            " ghostvie --help\n",
            " giftrans --help\n",
            " gimp --help\n",
            " gnuplot --help\n",
            " gnuplot_ --help\n",
            " gordon --help\n",
            " gp --help\n",
            " gr_top --help\n",
            " gramlx --help\n",
            " graph_te --help\n",
            " gre --help\n",
            " grep --help\n",
            " groups --help\n",
            " gs --help\n",
            " gs3.33 --help\n",
            " gsftopk --help\n",
            " gv --help\n",
            " head --help\n",
            " heartche --help\n",
            " help --help\n",
            " help.fin --help\n",
            " help.key --help\n",
            " help.sor --help\n",
            " help.top --help\n",
            " hightoll --help\n",
            " hilow --help\n",
            " hippo --help\n",
            " history --help\n",
            " hoc --help\n",
            " host --help\n",
            " hostname --help\n",
            " hpost --help\n",
            " htn_date --help\n",
            " htn_edit --help\n",
            " htn_repo --help\n",
            " id --help\n",
            " identify --help\n",
            " imake --help\n",
            " imgview --help\n",
            " infocmp --help\n",
            " inline --help\n",
            " install- --help\n",
            " interest --help\n",
            " ispell --help\n",
            " java --help\n",
            " javac --help\n",
            " join --help\n",
            " jot --help\n",
            " jre --help\n",
            " justlex --help\n",
            " justspec --help\n",
            " keep_up --help\n",
            " kill --help\n",
            " killall --help\n",
            " kludgepl --help\n",
            " kmist --help\n",
            " ksh --help\n",
            " last --help\n",
            " lattice_ --help\n",
            " launchef --help\n",
            " lc --help\n",
            " lcc --help\n",
            " ld --help\n",
            " ld64_ --help\n",
            " ld_ --help\n",
            " less --help\n",
            " lex --help\n",
            " lex.spec --help\n",
            " line.pro --help\n",
            " lint --help\n",
            " lint1 --help\n",
            " lint2 --help\n",
            " list.pl --help\n",
            " list2.pl --help\n",
            " lks --help\n",
            " ln --help\n",
            " local.Sq --help\n",
            " logname --help\n",
            " long --help\n",
            " lp --help\n",
            " lp.orig --help\n",
            " lpdsend --help\n",
            " lpe3 --help\n",
            " lpq --help\n",
            " lps --help\n",
            " ls --help\n",
            " m3_binin --help\n",
            " m3_compt --help\n",
            " m3_flsd --help\n",
            " m3_manfl --help\n",
            " m4 --help\n",
            " magma.ex --help\n",
            " mail --help\n",
            " mailbox --help\n",
            " mailp --help\n",
            " mailx --help\n",
            " make --help\n",
            " make_del --help\n",
            " make_tod --help\n",
            " makeinde --help\n",
            " maker5X. --help\n",
            " makexgvi --help\n",
            " man --help\n",
            " maple.sy --help\n",
            " mapleTTY --help\n",
            " mars.sh --help\n",
            " matlab --help\n",
            " matlab_l --help\n",
            " mc --help\n",
            " mesg --help\n",
            " metamail --help\n",
            " mhl --help\n",
            " mi --help\n",
            " mimencod --help\n",
            " mkdir --help\n",
            " mklink.s --help\n",
            " more --help\n",
            " movemail --help\n",
            " moviepla --help\n",
            " mp --help\n",
            " mpeg_pla --help\n",
            " mplotcha --help\n",
            " mplotps --help\n",
            " mplottek --help\n",
            " msort --help\n",
            " munpack --help\n",
            " my.ls --help\n",
            " my.ls.re --help\n",
            " mycut --help\n",
            " mysql --help\n",
            " mysql_in --help\n",
            " mysqladm --help\n",
            " named --help\n",
            " nawk --help\n",
            " neato --help\n",
            " nedit --help\n",
            " neqn --help\n",
            " netscape --help\n",
            " netstat --help\n",
            " newalias --help\n",
            " news --help\n",
            " nice --help\n",
            " nlcrack --help\n",
            " nlgen --help\n",
            " nlx --help\n",
            " nly --help\n",
            " nlz --help\n",
            " nlz2 --help\n",
            " nm --help\n",
            " nm_elf --help\n",
            " nospool --help\n",
            " npasplit --help\n",
            " nr --help\n",
            " nroff --help\n",
            " ns-insta --help\n",
            " nscal --help\n",
            " nslookup --help\n",
            " ntrim.in --help\n",
            " nw_8s_un --help\n",
            " on --help\n",
            " op_cvmod --help\n",
            " op_mko --help\n",
            " op_mksim --help\n",
            " op_runsi --help\n",
            " orig --help\n",
            " orig_sca --help\n",
            " overlap --help\n",
            " overlap2 --help\n",
            " p --help\n",
            " pacdec --help\n",
            " pagemail --help\n",
            " panel_te --help\n",
            " passwd --help\n",
            " paste --help\n",
            " patch --help\n",
            " payphone --help\n",
            " pcst --help\n",
            " pcst.pur --help\n",
            " pcst1 --help\n",
            " pdf2ps --help\n",
            " pftp --help\n",
            " pg --help\n",
            " pine --help\n",
            " ping --help\n",
            " plaid --help\n",
            " point.sh --help\n",
            " polar --help\n",
            " popper --help\n",
            " post --help\n",
            " postprin --help\n",
            " postreve --help\n",
            " pow --help\n",
            " ppost --help\n",
            " ppq --help\n",
            " pq --help\n",
            " print_ca --help\n",
            " print_de --help\n",
            " print_do --help\n",
            " print_us --help\n",
            " printf --help\n",
            " printreq --help\n",
            " prisoncs --help\n",
            " ps --help\n",
            " ps2epsi --help\n",
            " ps2pdf --help\n",
            " psnr --help\n",
            " psu --help\n",
            " ptelnet --help\n",
            " punlx --help\n",
            " purify.s --help\n",
            " pwd --help\n",
            " q_eg --help\n",
            " q_egtest --help\n",
            " q_test --help\n",
            " qk --help\n",
            " qpage --help\n",
            " quota --help\n",
            " r --help\n",
            " random_t --help\n",
            " randseq --help\n",
            " rbnull --help\n",
            " rcc --help\n",
            " rcsdiff --help\n",
            " rdistd --help\n",
            " readacct --help\n",
            " reaper --help\n",
            " red --help\n",
            " register --help\n",
            " renice --help\n",
            " req.new --help\n",
            " resize --help\n",
            " reverse --help\n",
            " rexecd --help\n",
            " richtext --help\n",
            " rlogin --help\n",
            " rm --help\n",
            " rmail --help\n",
            " rmm --help\n",
            " rootless --help\n",
            " rpcinfo --help\n",
            " rsh --help\n",
            " rshd --help\n",
            " rtslave --help\n",
            " run_swin --help\n",
            " runnit --help\n",
            " rup --help\n",
            " rusers --help\n",
            " rvplayer --help\n",
            " rwho --help\n",
            " rz --help\n",
            " sam --help\n",
            " sample --help\n",
            " samterm --help\n",
            " sar --help\n",
            " scamp_fi --help\n",
            " scamp_pr --help\n",
            " scamp_to --help\n",
            " scampdet --help\n",
            " scan --help\n",
            " scatter_ --help\n",
            " sccs --help\n",
            " scheme --help\n",
            " sdec --help\n",
            " sed --help\n",
            " see_scam --help\n",
            " seecalls --help\n",
            " seediff --help\n",
            " sendmail --help\n",
            " seq --help\n",
            " setup --help\n",
            " sfplay --help\n",
            " sfstdgen --help\n",
            " sgihelp --help\n",
            " sh --help\n",
            " shelpMot --help\n",
            " show --help\n",
            " show_fil --help\n",
            " showcal --help\n",
            " showdoc --help\n",
            " showfile --help\n",
            " shownona --help\n",
            " showprod --help\n",
            " showps --help\n",
            " sim301bK --help\n",
            " sim301bS --help\n",
            " sleep --help\n",
            " slide --help\n",
            " sort --help\n",
            " spec --help\n",
            " spell --help\n",
            " split --help\n",
            " sprog --help\n",
            " sqp_fill --help\n",
            " ssh-add --help\n",
            " ssh-askp --help\n",
            " ssh-keyg --help\n",
            " ssplay --help\n",
            " states --help\n",
            " stream_b --help\n",
            " stream_t --help\n",
            " strings --help\n",
            " stripper --help\n",
            " stty --help\n",
            " style2 --help\n",
            " style3 --help\n",
            " su --help\n",
            " suepope4 --help\n",
            " sum --help\n",
            " summary. --help\n",
            " swap --help\n",
            " sysinfo --help\n",
            " t --help\n",
            " tail --help\n",
            " talk --help\n",
            " tar --help\n",
            " tbl --help\n",
            " tcm --help\n",
            " tcm5na --help\n",
            " tcm8 --help\n",
            " tcm8a --help\n",
            " tcm8na --help\n",
            " tcpostio --help\n",
            " tcppost --help\n",
            " tee --help\n",
            " tektroni --help\n",
            " tel --help\n",
            " telnet --help\n",
            " telno --help\n",
            " tes --help\n",
            " test --help\n",
            " test.m2. --help\n",
            " test.pl --help\n",
            " test2.pl --help\n",
            " testFont --help\n",
            " testHist --help\n",
            " tester --help\n",
            " text_are --help\n",
            " tftp --help\n",
            " tifftofa --help\n",
            " toolches --help\n",
            " top --help\n",
            " touch --help\n",
            " tput --help\n",
            " tr --help\n",
            " tracerou --help\n",
            " trn --help\n",
            " true --help\n",
            " tset --help\n",
            " ttcm --help\n",
            " ttcm8 --help\n",
            " tty --help\n",
            " twm --help\n",
            " twoprint --help\n",
            " ul --help\n",
            " uname --help\n",
            " uniq --help\n",
            " unpack --help\n",
            " unzip --help\n",
            " uopt --help\n",
            " update --help\n",
            " use_abus --help\n",
            " userenv --help\n",
            " uudecode --help\n",
            " uuencode --help\n",
            " uuname --help\n",
            " v10sort --help\n",
            " vacation --help\n",
            " vc --help\n",
            " vim --help\n",
            " vinay --help\n",
            " vipw --help\n",
            " virmf --help\n",
            " virtex --help\n",
            " volumes. --help\n",
            " vsimsg --help\n",
            " vsiupdst --help\n",
            " vt100 --help\n",
            " wait4wm --help\n",
            " wc --help\n",
            " wdefine --help\n",
            " webify --help\n",
            " webmagic --help\n",
            " where --help\n",
            " whereis --help\n",
            " which --help\n",
            " whoami --help\n",
            " whodo --help\n",
            " whois --help\n",
            " window_t --help\n",
            " windows --help\n",
            " x11perf --help\n",
            " x3270 --help\n",
            " xargs --help\n",
            " xbiff --help\n",
            " xcal --help\n",
            " xcalc --help\n",
            " xclock --help\n",
            " xconfirm --help\n",
            " xdemineu --help\n",
            " xdm --help\n",
            " xdpyinfo --help\n",
            " xdvi --help\n",
            " xemacs-1 --help\n",
            " xemacs-2 --help\n",
            " xev --help\n",
            " xfig --help\n",
            " xfontsel --help\n",
            " xfs --help\n",
            " xgas --help\n",
            " xgobi --help\n",
            " xhost --help\n",
            " xlbiff --help\n",
            " xlistscr --help\n",
            " xlsclien --help\n",
            " xlsfonts --help\n",
            " xmag --help\n",
            " xman --help\n",
            " xmaplev4 --help\n",
            " xmaplev5 --help\n",
            " xmessage --help\n",
            " xmh --help\n",
            " xmkmf --help\n",
            " xpaint --help\n",
            " xpdf --help\n",
            " xpr --help\n",
            " xprop --help\n",
            " xrdb --help\n",
            " xrn --help\n",
            " xrt_auth --help\n",
            " xrtld --help\n",
            " xset --help\n",
            " xsetroot --help\n",
            " xt --help\n",
            " xterm --help\n",
            " xupdate --help\n",
            " xv --help\n",
            " xwininfo --help\n",
            " xwsh --help\n",
            " xxx --help\n",
            " ypcat --help\n",
            " yppasswd --help\n",
            " z --help\n",
            " zip --help\n",
            " zsh --help\n",
            " zubs --help\n",
            " zz2 --help\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "brLLve2g9vX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run `masq()` on your own examples below:"
      ]
    },
    {
      "metadata": {
        "id": "2ulst1zuf78W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run masq( ) on your own examples:\n",
        "masq(\n",
        "    # Put your malicious commands here:\n",
        "    script=\"\"\"\n",
        "\n",
        "    \"\"\",\n",
        "    # Put your target user here (indexed from 0 to 49):\n",
        "    target_user=0, \n",
        "    # Aggressiveness: how large a deviation do we allow before we give up? (scaled from 0.0-1.0)\n",
        "    # Smaller numbers will result in a shorter script, but also give less flexibility. If the attack fails,\n",
        "    # it might work if you up the aggressiveness.\n",
        "    aggressiveness=0.5, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}