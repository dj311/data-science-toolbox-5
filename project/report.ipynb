{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of oracle-and-real-data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "guBl1hnUsmNf",
        "eEJNbwsLpB1s",
        "ifDu6QJSpOmZ",
        "YSiMq5Ck85MO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "G0qq6bLm4rqh",
        "outputId": "81219b48-e145-45f1-ce7a-56cfa97803bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "cell_type": "code",
      "source": [
        "# If running on Google Colab, only cleverhans needs installation. This can be done via:\n",
        "!pip install cleverhans\n",
        "\n",
        "# If running locally, we've listed (TODO) our dependencies in requirements.txt, so the following\n",
        "# should get everything up and running:\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "import numpy\n",
        "import keras\n",
        "import pandas\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import cleverhans\n",
        "import tensorflow\n",
        "import seaborn\n",
        "import sklearn\n",
        "\n",
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.attacks import CarliniWagnerL2\n",
        "from cleverhans.attacks import SaliencyMapMethod\n",
        "from cleverhans.attacks_tf import jacobian_augmentation\n",
        "from cleverhans.attacks_tf import jacobian_graph\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "from cleverhans.utils_tf import model_eval\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "numpy.random.seed(0xC0FFEE)\n",
        "tensorflow.set_random_seed(0xC0FFEE)\n",
        "rng = numpy.random.RandomState(0xC0FFEE)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cleverhans\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/a0/f0b4386b719f343c4ed3e13cd7792a7a7a4674566ca9b2b34a09b7424220/cleverhans-3.0.1-py3-none-any.whl (198kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.6.0)\n",
            "Collecting mnist~=0.2 (from cleverhans)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.14.6)\n",
            "Collecting pycodestyle (from cleverhans)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/0c/04a353e104d2f324f8ee5f4b32012618c1c86dd79e52a433b64fceed511b/pycodestyle-2.5.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.3.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->cleverhans) (40.8.0)\n",
            "Installing collected packages: mnist, pycodestyle, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 pycodestyle-2.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "guBl1hnUsmNf"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GS0WJpSyuQeU"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OlTdMqCWus2I"
      },
      "cell_type": "markdown",
      "source": [
        "Run the below code to download a copy of the dataset (if you don't already have it):"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fLKVLsJFurZ5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "response = requests.get(\"http://www.schonlau.net/masquerade/masquerade-data.zip\")\n",
        "\n",
        "dataset_file = io.BytesIO(response.content)\n",
        "\n",
        "zipped_dataset = zipfile.ZipFile(dataset_file)\n",
        "zipped_dataset.extractall('data/masquerade-data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gIA5snRclpjq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://www.schonlau.net/intrusion.html\n",
        "# download Masquerade Data (zip File)\n",
        "\n",
        "import pandas as pd\n",
        "directory = './data/masquerade-data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3X6tHalTlpkU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sorts the given iterable in the way that is expected.\n",
        " \n",
        "    Required arguments:\n",
        "    l -- The iterable to be sorted.\n",
        " \n",
        "    \"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
        "    return sorted(l, key = alphanum_key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FP8_s_WClpkp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "users = range(1,51)\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for filename in sorted_nicely(os.listdir(directory)):\n",
        "    user = pd.read_csv(os.path.join(directory, filename), header=None)\n",
        "    df = pd.concat([df, user], axis = 1)\n",
        "    \n",
        "df.columns = sorted_nicely(os.listdir(directory))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xOhVC6HazrjI"
      },
      "cell_type": "markdown",
      "source": [
        "We've loaded in the dataset, but need to do a little coercion to get it into the required format. First, we make sure  the values in the dataframe are categorical variables sharing the same data type:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5vklxycDrRYm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "commands = numpy.unique(df)\n",
        "command_dtype = pandas.api.types.CategoricalDtype(commands)\n",
        "\n",
        "for column in df:\n",
        "    df[column] = df[column].astype(command_dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kO_pHxcslplN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labelled, unlabelled = df.head(5000), df.tail(len(df) - 5000)  # ignore unlabeled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZEH_Lws_op3_"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset contains a list of commands run for each user. Treating this as a timeseries, we perform rolling window sampling in blocks of 100 commands, and summarise the usage over each block."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "co7TNH4XqLYG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rolling_window_command_counts(commands, window_size):\n",
        "    \n",
        "    # Save a copy the name of the series to add again to our output. This will preserve the mapping of\n",
        "    # user identifier to (it's column header in the dataframe it came from), which in\n",
        "    # this case is the user identifier. \n",
        "    user = commands.name\n",
        "\n",
        "    # Convert the single column \"which command was run?\" to a column for each\n",
        "    # command, which says \"was command <x> run?\"\n",
        "    commands = pandas.get_dummies(commands)\n",
        "\n",
        "    # Take a rolling sample of the last 100 commands, then sum each \"was command <x> run?\"\n",
        "    # columns to give a bunch \"command <x> was run <y> times in this window\".\n",
        "    command_counts = commands.rolling(window=window_size).aggregate(numpy.sum)\n",
        "\n",
        "    # Remove the first 100 rows because they contain data from blocks of size < 100.\n",
        "    command_counts = command_counts[window_size-1:]\n",
        "    \n",
        "    # Preserve the user identifier (see top of function) as a new column:\n",
        "    \n",
        "    # First, a nasty hack: https://github.com/pandas-dev/pandas/issues/19136\n",
        "    command_counts = command_counts.rename(columns=str)  \n",
        "    \n",
        "    # Then, add in the user (with an adhoc parser to turn the label into a number)\n",
        "    command_counts['user'] = int(user.replace('User', ''))\n",
        "\n",
        "    return command_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdQxI7iTQNPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apply to the entire dataset:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Oiyth5e0pYSq",
        "outputId": "b9d1e6c5-b4dd-4e95-c490-d6ba01febb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2012
        }
      },
      "cell_type": "code",
      "source": [
        "labelled_dataset = pandas.concat([\n",
        "        rolling_window_command_counts(commands, 100)\n",
        "        for user, commands in labelled.iteritems()\n",
        "    ],\n",
        "    ignore_index=True,  # reset index to go from 0 to 4900\n",
        ")\n",
        "\n",
        "labelled_dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>%backup%</th>\n",
              "      <th>.java_wr</th>\n",
              "      <th>.maker_w</th>\n",
              "      <th>.wrapper</th>\n",
              "      <th>.xinitrc</th>\n",
              "      <th>.xsessio</th>\n",
              "      <th>1.1</th>\n",
              "      <th>1.2</th>\n",
              "      <th>1.3</th>\n",
              "      <th>4Dwm</th>\n",
              "      <th>...</th>\n",
              "      <th>xxx</th>\n",
              "      <th>yacc</th>\n",
              "      <th>ypcat</th>\n",
              "      <th>yppasswd</th>\n",
              "      <th>z</th>\n",
              "      <th>zip</th>\n",
              "      <th>zsh</th>\n",
              "      <th>zubs</th>\n",
              "      <th>zz2</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245020</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245021</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245022</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245023</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245024</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245025</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245026</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245027</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245028</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245029</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245030</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245031</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245032</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245033</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245034</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245035</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245036</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245037</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245038</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245039</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245040</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245041</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245042</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245043</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245044</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245045</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245046</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245047</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245048</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245049</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>245050 rows × 857 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        %backup%  .java_wr  .maker_w  .wrapper  .xinitrc  .xsessio  1.1  1.2  \\\n",
              "0            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "1            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "2            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "3            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "4            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "5            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "6            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "7            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "8            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "9            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "10           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "11           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "12           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "13           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "14           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "15           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "16           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "17           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "18           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "19           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "20           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "21           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "22           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "23           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "24           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "25           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "26           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "27           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "28           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "29           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "...          ...       ...       ...       ...       ...       ...  ...  ...   \n",
              "245020       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245021       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245022       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245023       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245024       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245025       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245026       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245027       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245028       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245029       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245030       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245031       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245032       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245033       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245034       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245035       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245036       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245037       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245038       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245039       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245040       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245041       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245042       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245043       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245044       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245045       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245046       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245047       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245048       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245049       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "\n",
              "        1.3  4Dwm  ...   xxx  yacc  ypcat  yppasswd    z  zip  zsh  zubs  zz2  \\\n",
              "0       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "1       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "2       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "3       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "4       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "5       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "6       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "7       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "8       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "9       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "10      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "11      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "12      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "13      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "14      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "15      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "16      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "17      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "18      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "19      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "20      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "21      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "22      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "23      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "24      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "25      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "26      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "27      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "28      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "29      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "...     ...   ...  ...   ...   ...    ...       ...  ...  ...  ...   ...  ...   \n",
              "245020  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245021  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245022  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245023  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245024  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245025  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245026  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245027  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245028  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245029  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245030  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245031  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245032  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245033  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245034  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245035  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245036  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245037  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245038  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245039  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245040  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245041  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245042  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245043  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245044  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245045  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245046  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245047  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245048  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245049  0.0   2.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "\n",
              "        user  \n",
              "0          1  \n",
              "1          1  \n",
              "2          1  \n",
              "3          1  \n",
              "4          1  \n",
              "5          1  \n",
              "6          1  \n",
              "7          1  \n",
              "8          1  \n",
              "9          1  \n",
              "10         1  \n",
              "11         1  \n",
              "12         1  \n",
              "13         1  \n",
              "14         1  \n",
              "15         1  \n",
              "16         1  \n",
              "17         1  \n",
              "18         1  \n",
              "19         1  \n",
              "20         1  \n",
              "21         1  \n",
              "22         1  \n",
              "23         1  \n",
              "24         1  \n",
              "25         1  \n",
              "26         1  \n",
              "27         1  \n",
              "28         1  \n",
              "29         1  \n",
              "...      ...  \n",
              "245020    50  \n",
              "245021    50  \n",
              "245022    50  \n",
              "245023    50  \n",
              "245024    50  \n",
              "245025    50  \n",
              "245026    50  \n",
              "245027    50  \n",
              "245028    50  \n",
              "245029    50  \n",
              "245030    50  \n",
              "245031    50  \n",
              "245032    50  \n",
              "245033    50  \n",
              "245034    50  \n",
              "245035    50  \n",
              "245036    50  \n",
              "245037    50  \n",
              "245038    50  \n",
              "245039    50  \n",
              "245040    50  \n",
              "245041    50  \n",
              "245042    50  \n",
              "245043    50  \n",
              "245044    50  \n",
              "245045    50  \n",
              "245046    50  \n",
              "245047    50  \n",
              "245048    50  \n",
              "245049    50  \n",
              "\n",
              "[245050 rows x 857 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4teyPJwGobpx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = labelled_dataset['user'] - 1\n",
        "dataset = labelled_dataset.drop(columns=['user'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HplyOBjLKoPB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels =  keras.utils.to_categorical(labels, num_classes=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T64W_xRg6Gsx"
      },
      "cell_type": "markdown",
      "source": [
        "Creating the training and testing datasets:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l9nuQQCDNOVW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "training_data, testing_data, training_labels, testing_labels = train_test_split(\n",
        "    dataset,\n",
        "    labels, \n",
        "    test_size=0.10,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eEJNbwsLpB1s"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the Oracle"
      ]
    },
    {
      "metadata": {
        "id": "jhnem8RNQWSs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Following the architecture described in Ryan et al 1998, we create a three-layer backpropagation neural network using Keras."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f_05tZlK5Gbz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7FnkFt645UHM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_layer = Dense(\n",
        "    units=856,\n",
        "    activation='relu',\n",
        "    input_dim=856,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lBbvmT7PDKJ3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_layer = Dense(\n",
        "    units=30,\n",
        "    activation='relu',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "StOQDCqqDM8r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_layer = Dense(\n",
        "    units=50,\n",
        "    activation='softmax',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oOEnF-QFDkfr",
        "outputId": "ad11c5ff-9322-445f-8136-bf1a462f0bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "oracle.add(input_layer)\n",
        "oracle.add(hidden_layer)\n",
        "oracle.add(output_layer)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FhIt-t3UD0Dw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ifDu6QJSpOmZ"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Oracle on Dataset"
      ]
    },
    {
      "metadata": {
        "id": "imoKqfKzQlai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we train the neural network intrusion detection system:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EQlU0zeXLjzw",
        "outputId": "703a984b-c628-425d-8905-739d949f18c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "source": [
        "history = oracle.fit(training_data,  training_labels, epochs=3, batch_size=50, validation_data = (testing_data, testing_labels), shuffle=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 220545 samples, validate on 24505 samples\n",
            "Epoch 1/3\n",
            "220545/220545 [==============================] - 80s 365us/step - loss: 0.1845 - acc: 0.9469 - val_loss: 0.0677 - val_acc: 0.9771\n",
            "Epoch 2/3\n",
            "220545/220545 [==============================] - 77s 349us/step - loss: 0.0643 - acc: 0.9780 - val_loss: 0.0468 - val_acc: 0.9822\n",
            "Epoch 3/3\n",
            "220545/220545 [==============================] - 75s 342us/step - loss: 0.0610 - acc: 0.9819 - val_loss: 0.0378 - val_acc: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2XhEnbRaQ62s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The trainingn ends with 98.58% accuracy against the test set. Here, we plot it's accuracy over time:"
      ]
    },
    {
      "metadata": {
        "id": "kBgpjHitQ4pi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "ca21dc49-f1f0-471c-f8a5-584fa0e87b93"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlTsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl4U3W+P/D3ydY2TZombdKVpZat\nFEot1wWLgkhlUVwQ2+oFdZwRFxAF5v6Yy+jAXIV75SqIiOO4zZ2Loxaw4jaIG3gVqwjShSIIiKV7\nk+7pnuX3RyG00CVA0pOcvl/Pw0Nztnw+1Pg+6zeC0+l0goiIiPyeTOwCiIiIyDMY6kRERBLBUCci\nIpIIhjoREZFEMNSJiIgkgqFOREQkEQx1IokYPXo0lixZct70P/7xjxg9evQFb++Pf/wjNm3a1Ocy\n2dnZuO+++9yeTkTexVAnkpCjR4/CarW6Xre3t6OgoEDEiohoIDHUiSTkqquuwmeffeZ6/c0332D8\n+PHdltm5cyduvvlmzJw5E/fccw9OnToFAKitrcX999+PadOmYeHChWhsbHStc/z4ccyfPx8zZszA\nnDlzLmhHoa6uDo899hhmzJiB2bNn45VXXnHN27BhA2bMmIEZM2bgnnvuQWVlZZ/TiahvDHUiCZk1\naxY++ugj1+uPP/4YM2fOdL0uKyvDk08+ic2bN+OTTz7B1KlT8ac//QkA8Oqrr0Kv1+PLL7/En/70\nJ3zzzTcAAIfDgUWLFuHWW2/Frl27sHr1ajzyyCOw2Wxu1bR+/XrodDrs2rULb731Ft5++23s378f\nx44dwyeffIKPPvoIu3btQlpaGnJycnqdTkT9Y6gTSciVV16JY8eOobq6Gi0tLTh48CAmTZrkmr93\n715cddVVGDZsGADgzjvvxPfffw+bzYb9+/dj1qxZAIDY2FhceeWVAIBffvkF1dXVmDdvHgBg4sSJ\nMBgMOHjwoFs1ffXVV7j77rsBAKGhoUhLS8PevXsREhKCmpoafPjhh6ivr8eCBQtw22239TqdiPrH\nUCeSELlcjhtvvBE7d+7E7t27MXnyZCgUCtf82tpahISEuF5rtVo4nU7U1taivr4eWq3WNe/Mcg0N\nDWhtbcWsWbMwc+ZMzJw5E9XV1airq3Orppqamm7vGRISgurqakRERGDTpk2uMwYLFy5EeXl5r9OJ\nqH8MdSKJmT17Nnbt2oVPPvkEs2fP7jYvLCysWxjX19dDJpNBr9cjJCSk23X0mpoaAIDJZEJwcDA+\n+eQT159vvvkGaWlpbtUTHh7e7T3r6uoQHh4OALj66qvxyiuvYO/evYiKisKzzz7b53Qi6htDnUhi\nLr/8clRVVeHYsWOuU+hnpKamYv/+/SguLgYAvPPOO0hNTYVCoUBycjI+//xzAMCpU6dw4MABAEBM\nTAwiIyPxySefAOgM+2XLlqG5udmteqZOnYqsrCzXup999hmmTp2Kb775Bn/+85/hcDigVqsxZswY\nCILQ63Qi6p+i/0WIyJ8IgoC0tDS0tLRAJuu+3x4ZGYmnn34ajzzyCDo6OhAbG4unnnoKAPDggw9i\n6dKlmDZtGuLj43HjjTe6trd+/XqsXr0azz//PGQyGX7zm99ArVa7Vc/jjz+O1atXY+bMmZDJZFi4\ncCGSkpLQ1taGjz/+GDNmzIBKpYLBYMDatWthMpl6nE5E/RP4fepERETSwNPvREREEsFQJyIikgiG\nOhERkUQw1ImIiCSCoU5ERCQRfv9Im9nc2P9CF0CvV6O21r3nb30de/E9UukDYC++SCp9AOylL0aj\nttd5PFI/h0IhF7sEj2EvvkcqfQDsxRdJpQ+AvVwshjoREZFEMNSJiIgkgqFOREQkEQx1IiIiiWCo\nExERSQRDnYiISCIY6kRERBLh94PP+KJNmzbg6NGfUFNTjdbWVkRHxyAkRIe1a/+7z/X++c8PERys\nwZQp1w9QpUREJCUMdS949NGlADpD+pdfTmDx4sfdWm/27DneLIuIiCSOoT5AfvxxP9555000Nzdj\n8eKlOHjwAPbs+QIOhwOTJqXi/vsX4vXX/4rQ0FDExcUjO3srBEGGoqKTmDr1Btx//0KxWyAiIh8n\n+VDf+uVx/HCkyq1l7Q4HnAAUsr5vNbhijAnp00ZccC0nThzH229nQ6VS4eDBA3jppdcgk8mQnn4r\nMjLu7rbs4cOFeOutd+FwOHDnnXMY6kRE1C/Jh/qFaG6zob3DgeAgBQKVnv+nGTFiJFQqFQAgMDAQ\nixcvhFwuR11dHRoaGrotO3r0GAQGBnq8BiIiki7Jh3r6tBFuH1WXVzfhv/5xENaWdtw/KwGXjzJ6\ntBalUgkAqKgoR1bWP/DGG/+AWq3GggXp5y0rl0vnywyIiGhg8JG2LqLCgrHqd1dBqZDh5Q8K8XNx\nnVfep66uDnq9Hmq1GkePHkFFRQU6Ojq88l5ERDR4MNTPMXqYAYtvHw+Hw4mN2/NRXGX1+HuMHDkK\nQUFqPPzw/fjii09x661z8dxzz3j8fYiIaHARnE6nU+wiLoXZ3OjR7RmNWpjNjcgprMCrHx6GTqPC\nyvkTYQwN8uj7DIQzvUiBVHqRSh8Ae/FFUukDYC/9ba83Xj1SX7t2LTIyMpCZmYn8/Pxu89ra2rBi\nxQrMnTvXNa2pqQmLFy/GggULkJmZia+//tqb5fVpUmIk7rphJOqt7XguKxcNTe2i1UJEROQOr4X6\nvn37UFRUhKysLKxZswZr1qzpNn/dunVISEjoNu29995DXFwctmzZgo0bN563zkBLu2IIbpo0DFW1\nLdiwLQ8tbTZR6yEiIuqL10I9JycH06dPBwDEx8ejvr4eVuvZ69NLly51zT9Dr9ejrq7z5rSGhgbo\n9Xpvlee2udddhmuTolBU0YgXswvQYXOIXRIREVGPvBbqFoulWygbDAaYzWbXa41Gc946N910E8rK\nypCWlob58+djxYoV3irPbYIg4J6Zo3H5yHD8VFSL1z46DIfDr29DICIiiRqw59TduR/v/fffR3R0\nNF5//XUcOXIEK1euRHZ2dp/r6PVqKBSefaa7p5sQ/vjbq7HqlRz8cKQKprBgPHj7eAiC4NH39Ya+\nbqjwN1LpRSp9AOzFF0mlD4C9XAyvhbrJZILFYnG9rqqqgtHY92AuP/74IyZPngwAGDNmDKqqqmC3\n2/sciKW2ttkzBZ/W112KD98yFv/1j4P4eO9JKAXglslxHn1vT+Pdo75HKn0A7MUXSaUPgL30t73e\neO30e2pqKnbt2gUAKCwshMlk6vGUe1fDhg1DXl4eAKC0tBTBwcE+NbKaOlCJZRkTEK4LxI5vTmL3\nwdIel9u0aQMWL16Iu+++A3Pn3oTFixdi5cp/c/t9ysvLcOTIYU+VTUREg4TXjtRTUlKQmJiIzMxM\nCIKAVatWITs7G1qtFmlpaViyZAkqKipw8uRJLFiwAOnp6cjIyMDKlSsxf/582Gw2rF692lvlXbRQ\nTQCWZyRj7ZsH8Oauo9AGKfEvY0zdlrnYr149Y//+fbDbbRgzZqzH6iYiIunz6jX13//+991ejxkz\nxvXzCy+80OM6Gzdu9GZJHhFhUGNp+gQ889ZBvPJhIYKDlEgY1v+d+i+99AIKCwvgcNgxb95duOGG\nNOTk7MUbb/wVKlUAwsPDsWjR4/if/3kNSqUKJlMkrrlm8gB0REREUiD5L3TJPv4RDlYVuL28XCbA\n3s/d7ZebxmPuiJvx6NzxeH5bHja9m48Vd6dgWGTv1zl+/HE/amtrsHnzq2hra8Vvf3sPrr12Ct59\nNwuPPfZ7jBuXhN27P4dSqcSMGbNhMpkY6EREdEE49vslGDvcgAfmJKKt3Y4NW3NR2cdNewUFeSgo\nyMPixQuxfPkSOBx21NRU4/rrp+OZZ57Gli3/g4SEROj1hgHsgIiIpETyR+pzR9yMuSNudnv5C71L\n8YoxJjTeOApvfvoz1mflYuX8idBpAs5bTqlU4pZbbsfdd9/TbfpNN92CSZNS8X//twf/9m+PYe3a\nZ91+byIioq54pO4B01JicUvqcJjrWrF+ax6aW88fTnbs2HHYu/drOBwOtLa24vnnO8P7b397FSpV\nAG677Q5MnXoDiopOQiaTwW63D3QbRETk5yR/pD5Qbp0ch4bmDuw5WIpN7+ZjWcaEbvOTk1MwblwS\nHnzwNwCcuOOODACA0WjCkiUPQasNgU6nw/z590KhUOI///M/oNOFYvr0GSJ0Q0RE/ohfvXqOSxkk\nwOFw4uX3D2H/UTNSRhnxyG3jIJOJN+ocB2/wPVLpA2AvvkgqfQDspb/t9Yan3z1IJhPwwJxEjBka\nih9/NuN/dx11a3hcIiIiT2Coe5hSIcOjdyRhaIQG/5dXhve+Pil2SURENEgw1L0gKECBpenJMIUG\n4aNvf8Xn+4vFLomIiAYBhrqX6IJVWJaZDF2wCm9/fgzfH64UuyQiIpI4hroXmUKDsDR9AgID5Hjt\no8MoPFkjdklERCRhDHUvGxqhxZI7kiAIAl7MLsDJ8gaxSyIiIoliqA+A0UP1ePCWRLTb7NiwNQ/l\n1U1il0RERBLEUB8gE0cbce/MMbC2dGB9Vi5qG9vELomIiCSGoT6ArpsQjduvuwzVDW1YvzUXTa0d\nYpdEREQSwlAfYDdPGoYbJsai1NyEjdvz0dbBMd6JiMgzGOoDTBAE3DV9JK5MMOF4ST1e3nEINrtD\n7LKIiEgCGOoikAkCfnfzWCTGGZB3ohp//+QIh5MlIqJLxlAXiUIuw6LbxyEuSou9BRXYvueE2CUR\nEZGfY6iLKFClwGN3TkCEQY2d35/Crn2nxC6JiIj8GENdZCFqFZZnTECoRoWsL4/j20PlYpdERER+\niqHuA8J1QViWkQx1gAJ/++cR5J+wiF0SERH5IYa6j4g1avDYnUmQyQS89N4hHC+tF7skIiLyMwx1\nHzIyNhQP3zYONrsTG7flodTC4WSJiMh9DHUfkzwiHPfNGoOmVhvWZ+WipqFV7JKIiMhPMNR90OSk\nKNx5fTxqG9vwXFYurC0cTpaIiPrHUPdRs64ahhlXDkF5dTOe35aHtnYOJ0tERH1jqPuwO68fgUmJ\nkfilrAGbdxRwOFkiIuqTV0N97dq1yMjIQGZmJvLz87vNa2trw4oVKzB37txu0z/44APccsstmDt3\nLvbs2ePN8nyeTBDwm9ljkBQfhkO/1OCNf/4EB4eTJSKiXngt1Pft24eioiJkZWVhzZo1WLNmTbf5\n69atQ0JCQrdptbW12Lx5M9566y28/PLL+OKLL7xVnt9QyGV4+NZxiI8OwXeFlcj64jjHiScioh55\nLdRzcnIwffp0AEB8fDzq6+thtVpd85cuXeqa33WdSZMmQaPRwGQy4amnnvJWeX4lQCXHY3dOQHR4\nMD7bX4x/flckdklEROSDvBbqFosFer3e9dpgMMBsNrteazSa89YpKSlBa2srHnroIdx9993Iycnx\nVnl+RxOkxLL0CTCEBODdr37B13llYpdEREQ+RjFQb+TuKeO6ujq8+OKLKCsrwz333IPdu3dDEIRe\nl9fr1VAo5J4qEwBgNGo9uj1PMRq1ePqhVKx48Wv8/ZMjiIkMwVXjovpdRyqk0otU+gDYiy+SSh8A\ne7kYXgt1k8kEi+XsGOZVVVUwGo19rhMWFobLL78cCoUCQ4cORXBwMGpqahAWFtbrOrW1zR6rGej8\nhzebGz26TU8KlAFL5iXhv98+iGe27MfyjGSMGhLa47K+3suFkEovUukDYC++SCp9AOylv+31xmun\n31NTU7Fr1y4AQGFhIUwmU4+n3LuaPHkyvvvuOzgcDtTW1qK5ubnbKXzqFB+tw6Lbx8PhcGLj9nwU\nV1n7X4mIiCTPa0fqKSkpSExMRGZmJgRBwKpVq5CdnQ2tVou0tDQsWbIEFRUVOHnyJBYsWID09HTM\nmTMHM2bMQHp6OgDgiSeegEzGR+l7Mv6yMNx/UwJe/fAw1m/Nxcr5E2EMDRK7LCIiEpHg9PPnozx9\nesbfTvl8+kMx3vniGCL0Qfj3+RMREqxyzfO3XvoilV6k0gfAXnyRVPoA2Et/2+sND4P93I1XDMHs\nq4ehsrYFG7bloaXNJnZJREQkEoa6BNwx5TJMTopCUUUjXswuQIeNw8kSEQ1GDHUJEAQB984cjctH\nhuOnolq89tFhOBx+fVWFiIguAkNdIuQyGR68JRGjYnX44UgV3vr8Zw4nS0Q0yDDUJUSllGPJvCTE\nGoPx5Y+lyPr8Z7FLIiKiAcRQlxh1oBJL05MRrgvEPz45gj0HS8UuiYiIBghDXYL02gAsy0iGTqPC\nll1Hsf9IldglERHRAGCoS1SkQY3Vv5sElUqOVz4sxE9FtWKXREREXsZQl7ARQ0KxeO54OJ3Apnfz\nUVQhjYEciIioZwx1iUscbsADc8aird2ODdvyUOXhL8AhIiLfwVAfBK5MiMC/3jgKDU3teC4rF/XW\nNrFLIiIiL2CoDxLTUmJxS+pwmOtasX5rHppbOZwsEZHUMNQHkVsnx2FqcjSKq6zY9G4+Omx2sUsi\nIiIPYqgPIoIgYP6NozFxtBFHi+vwygccTpaISEoY6oOMTCZg4ZyxGDM0FAd+NmPLp0c5nCwRkUQw\n1AchpUKOR+9IwtAIDb7KLcN7X58UuyQiIvIAhvogFRSgwNL0ZJhCg/DRt7/i8/3FYpdERESXiKE+\niOmCVViWmYyQYBXe/vwY9v1UKXZJRER0CRjqg5wpNAjL0icgMECOVz88jMKTNWKXREREF4mhThga\nocWjc5MgCAJezC7AyfIGsUsiIqKLwFAnAMCYYXo8eEsi2m12bNiah/LqJrFLIiKiC8RQJ5eJo424\nZ8ZoWFs6sD4rD7WNHE6WiMifMNSpmynJMbj92jhUN7Ri/dZcNLV2iF0SERG5iaFO57n5muG4YWIs\nSs1N2Lg9H20dHE6WiMgfMNTpPIIg4K7pI3FlggnHS+rx8o5DsDscYpdFRET9YKhTj2SCgN/dPBaJ\nw/XIO1GNv+/kcLJERL6OoU69UshleOT28RgeqcU3BeXY/tUJsUsiIqI+MNSpT0EBCjyePgERBjV2\nfncKu/adErskIiLqhVdDfe3atcjIyEBmZiby8/O7zWtra8OKFSswd+7c89ZrbW3F9OnTkZ2d7c3y\nyE0hahWWZ0xAqEaFrC+P49tD5WKXREREPfBaqO/btw9FRUXIysrCmjVrsGbNmm7z161bh4SEhB7X\n/ctf/gKdTuet0ugihOuCsCwjGeoABf72zyPIP1EtdklERHQOr4V6Tk4Opk+fDgCIj49HfX09rFar\na/7SpUtd87s6ceIEjh8/jqlTp3qrNLpIsUYNlsxLgkwm4KUdBThRWi92SURE1IXXQt1isUCv17te\nGwwGmM1m12uNRtPjes888wz+8Ic/eKssukSjhoTi4VvHwWZz4vlteSi1cDhZIiJfoRioN3Lncagd\nO3YgOTkZQ4YMcXu7er0aCoX8Uko7j9Go9ej2xOSNXtKMWggKOTZmHcTGbXlY9+h1MOqDPP4+55LK\n70UqfQDsxRdJpQ+AvVwMr4W6yWSCxWJxva6qqoLRaOxznT179qC4uBh79uxBRUUFVCoVIiMjcc01\n1/S6Tm1ts8dqBjr/4c3mRo9uUyze7GVCnB53To3Htj0n8Me/fIN/nz8RmiClV94LkM7vRSp9AOzF\nF0mlD4C99Le93ngt1FNTU7Fp0yZkZmaisLAQJpOp11PuZzz//POunzdt2oSYmJg+A53ENfOqoahv\nasenPxTj+W15+LfMyxGg8uxZEyIicp/XQj0lJQWJiYnIzMyEIAhYtWoVsrOzodVqkZaWhiVLlqCi\nogInT57EggULkJ6ejjlz5nirHPICQRCQPm0EGpvbkVNYiZd2HMKjd4yHQs7hD4iIxCA4/XzsT0+f\nnuEpnwtnszuw6d0CFPxSjUmJEfjtzWMhEwSPvodUfi9S6QNgL75IKn0A7KW/7fWGh1R0yRRyGR65\nbRzio0OQU1iJrV8e5zjxREQiYKiTRwSo5HjszgmIClPj0x+KsfN7DidLRDTQGOrkMZogJZZnJEOv\nDcD2PSfwdX6Z2CUREQ0qDHXyKENIIJZnJCM4UIG/7zyKg8fM/a9EREQewVAnj4sOD8bj6ROgUAh4\n+f1C/FxcJ3ZJRESDAkOdvCI+WodFt4+Hw+HExu35KKmy9r8SERFdEoY6ec34y8Jw/00JaGmz4bmt\nubDUtYhdEhGRpDHUyasmJUYic9oI1Fvb8VxWLhqa2sUuiYhIshjq5HU3XjkUs68ehsraFmzYloeW\nNpvYJRERSRJDnQbEHVMuw+SkKBRVNGLzewXosDnELomISHIY6jQgBEHAvTNHI3lEOA7/WovXPz4M\nB0edIyLyKIY6DRi5TIaHbk3EyFgd9v1Uhbc/O8bhZImIPIihTgNKpZRjybwkxBqD8cWPJfjo21/F\nLomISDIY6jTgggOVWJqejHBdIN77+iT2HCwVuyQiIklgqJMo9NoALMtIhiZIiS2fHsWBo1Vil0RE\n5PcY6iSaSIMaS9MnQKWU468fFOJIUa3YJRER+TWGOokqLioEi+eOh9MJvPBuPooqGsUuiYjIbzHU\nSXSJww14YM5YtLXbsWFbHqpqm8UuiYjILzHUySdcmRCBu9NGoaGpczjZemub2CUREfkdhjr5jBsm\nxmLONcNhrmvFhq15aG7lcLJERBeCoU4+5bZr4zAlORqnqqx4MTsfHTa72CUREfkNhjr5FEEQsODG\n0Zg4yogjp+rwygeH4XBw1DkiIncw1MnnyGQCFt4yFmOGhuLAz2Zs+fQoh5MlInIDQ518klIhx6N3\nJGGoSYOvcsvwj11HxC6JiMjnMdTJZwUFKLA0fQKMoYHI+uxnfHGgROySiIh8GkOdfJpOE4DlGckI\n1Qbgrc9+xr6fKsUuiYjIZzHUyeeZ9Gr8+YFJCAyQ49UPD6PwZI3YJRER+SSGOvmFy2J0eHRuEgRB\nwIvZBThZ3iB2SUREPserob527VpkZGQgMzMT+fn53ea1tbVhxYoVmDt3brfp69atQ0ZGBu644w58\n+umn3iyP/MyYYXo8eMtYtNvs2LA1DxU1HE6WiKgrr4X6vn37UFRUhKysLKxZswZr1qzpNn/dunVI\nSEjoNu27777DsWPHkJWVhddeew1r1671VnnkpyaONmHBjNGwtnTguXdyUdvI4WSJiM7wWqjn5ORg\n+vTpAID4+HjU19fDarW65i9dutQ1/4wrrrgCGzduBACEhISgpaUFdjtHFKPupibH4PZr41Dd0Ir1\nW3PR1NohdklERD7Ba6FusVig1+tdrw0GA8xms+u1RqM5bx25XA61Wg0A2L59O6677jrI5XJvlUh+\n7OZrhuOGlFiUmpvwwvZ8tHdw54+ISDFQb3QhI4J9/vnn2L59O954441+l9Xr1VAoPBv8RqPWo9sT\nk5R7WXJXCtodTnydW4o3dh7FyvuugFzu+/d+Svl34s+k0otU+gDYy8XwWqibTCZYLBbX66qqKhiN\nxn7X+/rrr/Hyyy/jtddeg1bb/z9CrYe/e9to1MJsbvToNsUyGHqZP30kquuase9wBZ7dsh+/mT0G\ngiCIUKF7BsPvxB9JpRep9AGwl/621xuvHdakpqZi165dAIDCwkKYTKYeT7l31djYiHXr1uGvf/0r\nQkNDvVUaSYhSIcOi28djeKQW3xSUY/tXJ8QuiYhINF47Uk9JSUFiYiIyMzMhCAJWrVqF7OxsaLVa\npKWlYcmSJaioqMDJkyexYMECpKeno7m5GbW1tXj88cdd23nmmWcQHR3trTJJAoICFHg8fQL+880f\nsfO7U9CpVbjxyqFil0VENOAEp59//ZWnT8/wlI9vcqcXS10L1r55AHXWdjxw81hMGhc5QNW5b7D9\nTvyFVHqRSh8Ae+lve73x/buKiNwUHhqEZenJUAco8MY/f0L+iWqxSyIiGlAMdZKUWJMGS+YlQSYT\n8NKOApworRe7JCKiAeNWqB86dAi7d+8GAGzYsAH33nsv9u/f79XCiC7WqCGhePjWcbDZnHh+Wx7K\nLE1il0RENCDcCvWnn34acXFx2L9/PwoKCvDkk0/ihRde8HZtRBcteWQ47p01Gk2tNjyXlYuahlax\nSyIi8jq3Qj0gIADDhw/HF198gfT0dIwYMQIyGc/ck2+7Nika86bGo7axDc9l5cLawuFkiUja3Erm\nlpYW7Ny5E59//jkmT56Muro6NDTwqy/J9826aihuvGIIyqubsXFbHtraOZwsEUmXW6G+bNkyfPjh\nh1i6dCk0Gg22bNmC++67z8ulEV06QRCQPm0EJiVG4ERZA17acQg2u0PssoiIvMKtwWeuvvpqjBs3\nDhqNBhaLBZMmTUJKSoq3ayPyCJkg4DezE2BtsaHgl2r87Z8/4bc3j4XMh4eTJSK6GG4dqT/11FPY\nuXMn6urqkJmZiTfffBOrV6/2cmlEnqOQy/DIbeMQHx2CnMJKbP3y+AV9yRARkT9wK9QPHz6MO++8\nEzt37sTtt9+O559/HkVFRd6ujcijAlRyPHbnBESFqfHpD8X45PtTYpdERORRboX6mSOaPXv2YNq0\naQCA9vZ271VF5CWaICWWZyRDrw3Atj0n8HV+mdglERF5jFuhHhcXh9mzZ6OpqQkJCQnYsWMHdDqd\nt2sj8gpDSCCWZyQjOFCBv+88itxjlv5XIiLyA24PPvPcc8/hjTfeAACMGDEC69at82phRN4UHR6M\nx++cAIVCwF/eP4Sfi+vELomI6JK5Feqtra348ssvsWTJEjz88MPYu3cvVCqVt2sj8qr4GB0euW08\nHA4nXtiej5Iqq9glERFdErdC/cknn4TVakVmZibS09NhsVjwxBNPeLs2Iq9Lig/D/bMT0Nxmw/qt\nubDUtYhdEhHRRXPrOXWLxYL169e7Xl9//fVYsGCB14oiGkiTxkWisbkd73x5HM9tzcO/z09BiJpn\noojI/7g9TGxLy9kjmObmZrS1tXmtKKKBduOVQzHr6qGorGnG81vz0NJmE7skIqIL5taRekZGBmbN\nmoVx48YBAAoLC/HYY495tTCigTZvSjwamzrwTUE5Nr9X0HkjnZxfXERE/sOtUJ83bx5SU1NRWFgI\nQRDw5JNPYsuWLd6ujWhACYKHhby3AAAgAElEQVSAe2eNhrWlA7nHLXjto8NYeEsih5MlIr/hVqgD\nQFRUFKKiolyv8/PzvVIQkZjkMhkeujURz2XlYt9PVdAGqXB32kgIDHYi8gMXfW6R42aTVKmUciyZ\nl4QYYzC++LEEH337q9glERG55aJDnUcuJGXBgUosS09GWEgg3vv6JPbklopdEhFRv/o8/T5lypQe\nw9vpdKK2ttZrRRH5Ar02AMszk7F2ywFs2XUU2iAlJo42iV0WEVGv+gz1t956a6DqIPJJkQY1lqZP\nwLq3D+KvHxRiWboSY4bpxS6LiKhHfYZ6TEzMQNVB5LPiokKweO54PL81D5uy87Hi7hQMjdCKXRYR\n0Xn4EC6RGxKHG/DAnLFobbNj/dY8VNU2i10SEdF5GOpEbroyIQJ3p41CQ1M71mflob6pXeySiIi6\nYagTXYAbJsZizjXDUVXXgg1ZuWhu5XCyROQ7vBrqa9euRUZGBjIzM88brKatrQ0rVqzA3Llz3V6H\nyBfcdm0cpiRH41SVFS9m56PDZhe7JCIiAF4M9X379qGoqAhZWVlYs2YN1qxZ023+unXrkJCQcEHr\nEPkCQRCw4MbRmDjKiCOn6vDKB4fhcHAwJiISn9dCPScnB9OnTwcAxMfHo76+Hlar1TV/6dKlrvnu\nrkPkK2QyAQtvGYsxQ0Nx4Gcz3vz0KEdZJCLReS3ULRYL9Pqzz/MaDAaYzWbXa41Gc8HrEPkSpUKO\nxXOTMNSkwZ7cMrz/zUmxSyKiQc7tL3S5VBdzFOPOOnq9GgqF/GJK6pXRKJ1nkNmL9z39cCr+34tf\n44O9vyLKpMXNky/rc3lf7eNisBffI5U+APZyMbwW6iaTCRaLxfW6qqoKRqPR4+vUevh5YaNRC7O5\n0aPbFAt7GTiPz0vC2jd/xCvvFUBwOHBlQkSPy/l6HxeCvfgeqfQBsJf+ttcbr51+T01Nxa5duwAA\nhYWFMJlMPZ5yv9R1iHyBSa/G0jsnIEAlx6sfHkbhrzVil0REg5DXjtRTUlKQmJiIzMxMCIKAVatW\nITs7G1qtFmlpaViyZAkqKipw8uRJLFiwAOnp6ZgzZ8556xD5i2GRWiy5Iwnrt+bixewC/L+7Lkdc\nVIjYZRHRICI4/fyWXU+fnuEpH9/kT70cOFqFl3YcgiZIiX+fPxGRBrVrnj/10R/24nuk0gfAXvrb\nXm84ohyRh00cbcKCGaPR2NyB597JRW1jm9glEdEgwVAn8oKpyTG47do4VDe0YsPWXDS3dohdEhEN\nAgx1Ii+Zc81wTEuJQYm5CRu356O9g8PJEpF3MdSJvEQQBNydNgpXJphwrKQeL79fCLvdIXZZRCRh\nDHUiL5IJAn5701iMHa5H7nELNm/P43CyROQ1DHUiL1MqZFh0+3gMj9Tis32n8O5Xv4hdEhFJFEOd\naAAEBSjwePoERIcH45/fFeHTH4rFLomIJIihTjRAQtQq/MeD10CnUeGdL44hp7BC7JKISGIY6kQD\nKMKgxvL0ZKgDFHjj45+Qf6Ja7JKISEIY6kQDLNakwZJ5SZDJBLy0owAnyurFLomIJIKhTiSCUUNC\n8dCtibDZnHh+ax7KLE1il0REEsBQJxLJ5SONuHfWaDS12rB+ay5qGlrFLomI/BxDnUhE1yZFY97U\neNQ0tOG5rFxYWzicLBFdPIY6kchmXTUUN14xBOXVzdi4LQ9t7RxOloguDkOdSGSCICB92ghMSozA\nibIGvLTjEGwcTpaILgJDncgHyAQBv5mdgHGXGVDwSzX+9s8jcHA4WSK6QAx1Ih+hkMuw6LbxuCw6\nBDmFFdj65XGOE09EF4ShTuRDAlRyPH7nBESFqfHpD8X45PtTYpdERH6EoU7kYzRBSizPSIZeG4Bt\ne07gm/xysUsiIj/BUCfyQYaQQCzLSEZwoAL/s/MIco9ZxC6JiPwAQ53IR8WEB+OxOydAoRDwl/cP\n4efiOrFLIiIfx1An8mEjYnR45LbxcDiceGF7PkqqrGKXREQ+jKFO5OOS4sNw/+wENLd1DidrqW8R\nuyQi8lEMdSI/MGlcJDKmjUCdtR3PZeWhobld7JKIyAcx1In8xIwrh2LW1UNRWdM5nGxru03skojI\nxzDUifzIvCnxmDw+CifLG7E5u4DDyRJRNwx1Ij8iCALunTUaySPCUfhrLV776DCHkyUiF4Y6kZ+R\ny2R48NZEjIjVYd9PVXj782McTpaIAHg51NeuXYuMjAxkZmYiPz+/27xvv/0W8+bNQ0ZGBjZv3gwA\naGpqwuLFi7FgwQJkZmbi66+/9mZ5RH4rQCnHY/OSEGMMxhcHSvBRTpHYJRGRD/BaqO/btw9FRUXI\nysrCmjVrsGbNmm7zn376aWzatAlvv/029u7di+PHj+O9995DXFwctmzZgo0bN563DhGdFRyoxLL0\nZISFBOK9//sFX+WWil0SEYnMa6Gek5OD6dOnAwDi4+NRX18Pq7Vz4Izi4mLodDpERUVBJpNhypQp\nyMnJgV6vR11d56hZDQ0N0Ov13iqPSBL02gAsz0yGJkiJ/911FAeOVoldEhGJyGuhbrFYuoWywWCA\n2WwGAJjNZhgMhvPm3XTTTSgrK0NaWhrmz5+PFStWeKs8IsmINKixNH0CVAo5/vrBYRw9VSt2SUQk\nEsVAvZE7N/K8//77iI6Oxuuvv44jR45g5cqVyM7O7nMdvV4NhULuqTIBAEaj1qPbExN78T3e6MNo\n1OKPAUr8x+vfYVN2Af7zkcm4LEbn8ffp6X2lQiq9SKUPgL1cDK+FuslkgsVy9pulqqqqYDQae5xX\nWVkJk8mEH3/8EZMnTwYAjBkzBlVVVbDb7ZDLew/t2tpmj9ZtNGphNjd6dJtiYS++x5t9xBqC8Lub\nx+Kv7xfiyb9+i5ULJsIUGuSV9wKk8zsBpNOLVPoA2Et/2+uN106/p6amYteuXQCAwsJCmEwmaDQa\nAEBsbCysVitKSkpgs9mwe/dupKamYtiwYcjLywMAlJaWIjg4uM9AJ6LurkyIwN1po9DQ1I717+Si\nvonDyRINJl47Uk9JSUFiYiIyMzMhCAJWrVqF7OxsaLVapKWlYfXq1Vi+fDkAYPbs2YiLi4PJZMLK\nlSsxf/582Gw2rF692lvlEUnWDRNjUd/Ujo++/RUbtuZixd0pCAoYsCttRCQiwenno1Z4+vQMT/n4\nJqn0MlB9OJ1O/O+uo/gqtwxjhoZiafoEKL1w74kUfieAdHqRSh8Ae+lve73hiHJEEiQIAhbcOBoT\nRxlx5FQdXvnwMBwOv95/JyI3MNSJJEomE7DwlrEYMzQUB46a8eanRzmcLJHEMdSJJEypkGPx3CQM\nMWmwJ7cM739zUuySiMiLGOpEEqcOVGBZ+gQYQwPxwd5f8eWPJWKXRERewlAnGgR0mgAsz0hGiFqJ\nf3z6M/b9VCl2SUTkBQx1okHCpFdjaXoyAlRyvPrhYRT+WiN2SUTkYQx1okFkWKQWj96RBEEAXswu\nwMnyBrFLIiIPYqgTDTIJw/RYOCcR7e12PL8tD5U1nh1qmYjEw1AnGoT+ZYwJC2aMRmNzB57LykVt\nY5vYJRGRBzDUiQapqZfH4LZr42Cpb8WGrblobu0QuyQiukQMdaJBbM41wzEtJQYl5iZs3J6P9g67\n2CUR0SVgqBMNYoIg4O7po3DFGBOOldTj5fcLYXc4xC6LiC4SQ51okJPJBPzu5rFIGKZH7nEL/v4J\nh5Ml8lcMdSKCUiHD4rnjMSxSi2/yy5H9f7+IXRIRXQSGOhEBAIICFFiaPgER+iB8nFOET38oFrsk\nIrpADHUicglRq7A8Ixk6jQrvfHEMOYUVYpdERBeAoX4OXkukwS48NAjL0pMRFKDAGx//hIJfqsUu\niYjcpBC7AF+y7ef3sXfP94gKjkCsJgax2mgM0UYjRhONALlK7PKIBswQkwaPzUvCc1m52PxeAf7t\nrssRH60Tuywi6gdDvYv40DicairGqbpSnGosBco7pwsQYFKHI1YTjSHazrCP1URDq9KIWzCRF40a\nEoqHbk3E5uxD2LgtH3/41xREhweLXRYR9YGh3kWKKQkzElNRUVmHiuYqlDSWodhaipLGMpRYy3Cg\nKg8HqvJcy4cG6E4HfTRitTEYoomGIVAPQRBE7ILIcy4facS9M0fjbzuPYP3WXKycPxGGkECxyyKi\nXjDUeyCXyRGjiUKMJgpXYSKAzmvt1a01KD4d8CWNpShuLMOh6p9wqPon17pBiiDEaqI6j+hPH9lH\nqI2Qy+RitUN0Sa6dEI2G5na8+9UvWL81D3/41xRogpRil0VEPWCou0kQBIQHhSE8KAyXm8a7pje2\nW1HcWNrtqP5Y3S84Vnf2OV+lTIHo4CjXNfpYTTRiNFFQ8To9+YnZVw9DQ1MHPttfjI3b8/D7zMvF\nLomIesBQv0RalQZjw0ZjbNho17RWWytKrRVnT903lqLEWoaixrPP/QoQEKE2ng76zqP6WG00NEpe\nsyTfIwgCMm4YgcaWdnxXWIm/7DiEPz94jdhlEdE5GOpeEKgIRHzocMSHDndNszlsKG+qcgV8cWMZ\nSq1lqGiuwv7KXNdy+oDQzqDXnL5Or42GPiCU1+lJdDJBwP2zE2Bt6UD+iWose/4raIOUUCpkUMpl\nUChkrp+VXX4+b3qX+YpepisVMijkMv53T3SBGOoDRCFTYMjp0+9nOJwOWFpqTof82VP4BZbDKLAc\ndi0XrFC77rg/c2QfoTZCJnCYARpYCrkMi24bj03Z+Tj8a+0AvJ/QLfAVCjmU50xTKuRnl1PIoJTL\nTy8rnF5Gfv5Oxjk7ENYOB6yNrd13OOQyKOQCdyzIrzDURSQTZDCpw2FShyPFlOSaXt/WiBJr5414\nJY2lKLaW4WjtcRytPe5aRilTIkYT1eWoPhrRwVFQyXkDE3lXgEqO5RnJMIRpUF5Rjw6bo/OPvfNv\n2+m/u/5xTbP3MM2N6WemtbbZ0Nhl+kDotlNxzs5AX9N72nlQ9HFWordtymXcsSD3MdR9kC5AC13A\nGCSGjXFNa7G1dF6n73JEf6qxBL82nHItIxNkndfpNZ2n7cc5RkBrD4VaqRajDZIwQRCgkMsQqFIg\nUKT7PZ1OJ+wOZ/cdCLsDtnN2EnrbqXDtMNgcUCjlaLC2np7u7NyGzd5lOefpbdvR3Nrhmmaze3/H\nQhDQ545C1+na4ADY7fbuZy+6nNFQyoUul0NOn8GQC66zGb1dKpHJuFPhLxjqfiJIEYQRoXEYERrn\nmtbhsKG8qaIz5M88amctQ3lTJX6o/BHZpw/sDYF619H8mZvyQgN03Psnv9a5Y9G5cxEUcGnbMhq1\nMJsbL3g9h9MJ+7lnJuzn7GD0tjPRw3o97ZD0NL21ucO1vt3h/aGt5TKhx/sfztvJ8OAZDXmAEk2t\nHa5lZPz/lVsY6n5MKVNgqDYWQ7WxrmkOpwPmlmqUNJai2m7Bz1W/orixFHmWQuRZCl3LaZTBZ6/R\nn74pz6QO53V6ogsgEwTIFHIoFeKNQ+FwOF2BrwtVo6Ky4aJ3IHo8o2Gzw2Z3dpluR7vNjuY2Gzps\nDrTb7BiIr8w4swPX545CbzsbvV76OPeMxukzFq57NORd3sc/LoN4NdTXrl2LvLw8CIKAlStXIinp\n7HXjb7/9FuvXr4dcLsd1112HRYsWAQA++OADvPbaa1AoFFiyZAmmTp3qzRIl58wp+Ai10XX04XQ6\nUd/e0OWIvvN6/ZHaYzhSe8y1rkqmRIxrhLzOG/OigyOh5HV6Ip8lkwkIkMkRoJTDEBIIe1vHgNdg\nd5zZKXC6dgTO7jw4O1+7eUnkzNkNmUIOa1Nb9zMeXZZv7bDD2tLhmj4QX8XV705FL2cfbrhyGCJ1\nl3g6yd0avbXhffv2oaioCFlZWThx4gRWrlyJrKws1/ynn34ar7/+OiIiIjB//nzMmDEDYWFh2Lx5\nM9599100Nzdj06ZNDHUPEAQBoQE6hAboMC48wTW9uaPFNTpeibUcxY2lKGosxsmGItcyMkGGSLXJ\nNeb9mdP4QYogMVohIh8kl8kgV3n2LN+FXBLpdn9FLzsBXe+h8NQlkTNnK86s2xtrqw0PzhnrqX+a\nPnkt1HNycjB9+nQAQHx8POrr62G1WqHRaFBcXAydToeoqCgAwJQpU5CTk4OwsDBMmjQJGo0GGo0G\nTz31lLfKIwBqZRBG6eMxSh/vmtZh70DZmev01jKUnH6evqypAt9XHHAtFx5oOH00H+M6stepQvzi\n9BQRSUu3+ytEqsHpdJ53meLM2YvEUSY01DUPSB1eC3WLxYLExETXa4PBALPZDI1GA7PZDIPB0G1e\ncXExWlpa0NraioceeggNDQ149NFHMWnSpD7fR69XQ+Hh61lGo9aj2xPTxfQSDQP+BWf3Kh0OB8qt\nVThZW4xf64o7/64tRq75EHLNh1zL6QK0GK4fgjj9EAwPjcVw/RBEajz3PL1Ufi9S6QNgL75IKn0A\n7OViDNiNck4376Soq6vDiy++iLKyMtxzzz3YvXt3n0d/tbWe3fu52LtgfZEne1EhGKPVYzBaPQaI\n7vx91rXVnzNwThnyKg4jr+LswDkBctXZ6/Snj+ojgyOglF3Yf3pS+b1IpQ+AvfgiqfQBsJf+ttcb\nr4W6yWSCxWJxva6qqoLRaOxxXmVlJUwmE4KCgnD55ZdDoVBg6NChCA4ORk1NDcLCwrxVJl0kQRCg\nDwyFPjAU48PPHtU3dTSf95W1J+uL8Ev9r65l5IIckcEmDNHEuB6zi9FEIUjBr/QkIroUXgv11NRU\nbNq0CZmZmSgsLITJZIJGowEAxMbGwmq1oqSkBJGRkdi9ezeeffZZqNVq/OEPf8ADDzyA+vp6NDc3\nQ6/Xe6tE8oJgpRqjDSMw2jDCNa3d3oGypvJuI+SVWctRai0HKs6uawwKO/2YXYzryF4XIJ3Tb0RE\n3ua1UE9JSUFiYiIyMzMhCAJWrVqF7OxsaLVapKWlYfXq1Vi+fDkAYPbs2YiL6xxUZcaMGUhPTwcA\nPPHEE5DJ+Ny0v1PJlRgeMhTDQ4a6ptkddlQ2m0/ffX/mprxSHDQX4KC5wLVciEqLWG00RpviYJCH\nI1YTjfAgA5+nJyLqgeB092K3j/L0NRdexxGP0+lETWsdSqyl3e6+r22r67ZcoDygy/P0MRiiiUZU\ncATkMvEGAHGXv/1O+sJefI9U+gDYS3/b6w1HlCOfIQgCwoL0CAvSY4JxnGu6tb0JjfJaHCo57vra\n2l/qf8WJ+pOuZRSCHFHBEYh1PU/feZ0+UDEwAz4QEfkChjr5PI0qGHHGSETJzw6H22ZvR5m1vNsI\neWVNFSi2lgHlncsIEGBUd16n73pTnlalEakTIiLvYqiTXwqQqxCnG4Y43TDXNLvDjormKtdd98WN\npSixluHHqnz8WJXvWk6nCul26j5WG42wQAMHziEiv8dQJ8mQy+SI0UQhRhOFqzARQOd1+urWWtdd\n92eGxD1UfQSHqo+41g1SBHb5gpvOo/pItckvrtMTEZ3BUCdJEwQB4UEGhAcZkGwa75re2G7t9jx9\nsbUUx+tO4ljdL65lFDIFooMjugyF23mdPkAu0heIExH1g6FOg5JWpUFC2CgkhI1yTWu1tfXwPH0F\nTjWWdrtOb1IbTz9Hf/bIXqMKFqkTIqKzGOpEpwUqAnCZbjgu0w13TbM5bKhoqkKxtQylriP7clQ2\nV2F/Za5rudAAXbehcGM1MTAEhvI6PRENKIY6UR8UMkXnt9Fpo4HOLxWEw+lAdUttt1P3JY1lKLD8\nhALLT6511Yqgs0fz2hjEaqJhCFOL1AkRDQYMdaILJBNkMKrDYFSHIcWU5Jpe39bo+n76Mzfl/Vx3\nAj/XnXAtozygRLQ68nTQdx7Rx2gioeJ1eiLyAIY6kYfoArTQBYxGYtho17QWWytKreWux+sqWipQ\nXF+GosZi1zICBEQEm1yP18VqOo/sg5U8qieiC8NQJ/KiIEUgRoTGYURo53cbGI1alFXWoqKpstvA\nOaXWMlQ0VeKHyoOudfUBoZ2n7bXRGHI66EMDdLxOT0S9YqgTDTClTIEh2hgM0cYAuAJA53V6S0v1\n6aA/+x31+ZZC5FsKXesGK9Xd7rofoo2GSW3kF9wQEQCGOpFPkAkymNRGmNRGTIyYAKBz4Jz69oYu\nI+R1Xqc/WnscR2uPu9ZVyZSdg+50OaKPDo6EUq4Uqx0iEglDnchHCYKA0AAdQgN0GBee4JreYmvp\n9i12JdYyFDWW4GTDKdcyMkGGSLXJder+zLV6Na/TE0kaQ53IzwQpgjBSH4+R+njXtA57B8qbKs8+\nZnf6On1ZUwX24UfXcmGB+m5j3g/RxkCnCuF1eiKJYKgTSYBSrsTQkFgMDTn7TXYOpwPmZovriP7M\nHfh55kPIMx9yLadRBrvuuD9zZG9Uh/M6PZEfYqgTSZRMkCEi2ISIYBP+JSIZQOd1+rq2+i7fYleO\nksZSHKk9hiO1x1zrquQqxGqiut2UF6WJhFLG/2UQ+TJ+QokGEUEQoA8MhT4wFOPDx7qmN3c0u27G\nO3Pq/teGYvxSX+RaRibIEBUc4TqqH2kbAkezHBpVMDRKDVS8MY9IdAx1IoJaqcYo/QiM0o9wTWu3\nd6C8qQLFrhHyylBqLUeptRzfVxwAjnXfhkquglbZGfCdQR8MjSoYWqXG9bNGqYH29LwAeQCv5RN5\nGEOdiHqkkisxLGQIhoUMcU1zOB2obDajuLEUbfJmVNbVwNrehMZ2K6wdTbB2NKHUWgab097v9hUy\nBTTK4M4dAdU5wd/l584dg2AEKYK4E0DUD4Y6EbntzCn4qOAIGI1amM2N5y3jdDrRam+Dtb0J1o7O\nsG8883N7Z/A3dvm5stmMYmuZW++tUZ45A9A1+LuH/5kdhGClmjf70aDDUCcijxIEAUGKQAQpAmFE\nmFvrtNvbzwZ/R1Pn0X+X4D+zQ9DY0YSa1jqUNVX0XwcEBCvV3c4AuIK/yyUCrUoDhcYBu8MJuUx+\nqe0TiYqhTkSiU8lVCAtSISxI79byHQ4bmno4A2Btt6Kx4+zPnWcJrKhornJru0GKoO6n/l33BXS/\nRHDmPgGO2ke+hqFORH5HKVO4Rttzh91hR5Ot2XVJoLG9e/C3y9pQ3VjnOktgbqmGE85+txsgV/V6\nBqD7JQLN6ZsDVbwvgLyKoU5EkieXyRGi0iJEpe1x/rn3BzicDjTbWs4/A9DLJYKSxjLY3bg5UClT\ndA/+Lk8DnPt0gEapQZAikDsBdEEY6kRE5+h6U547Om8ObD3vDIAr+LvsEDS2N6GiqQodjo5+tysX\n5NAo1WdP/fdyBkB7+ucwp3v1knQx1ImILlHnzYFBCFIEwYRwt9Zps7d3u+5vdd0LcM5Ngu1WVLfU\noNRa7lYdwQp1l+A//XeXewK0XW4aDFaoeXOgxDDUiYhEECBXISDIgLAgg1vLd9g7zg/+rpcE2pvQ\nilbUNtejoa0BFU2Vbm23cyfgnODvaeyA08soOFSwT/Pqb2ft2rXIy8uDIAhYuXIlkpKSXPO+/fZb\nrF+/HnK5HNdddx0WLVrkmtfa2oqbb74ZjzzyCObOnevNEomI/IJSroRe3jnEb2+63htgd9hh7Wju\n8nRA1/sCujwdcPrnqmaLWzcHBsoDuzwRcO5TAuffL6CSqzz2b0D981qo79u3D0VFRcjKysKJEyew\ncuVKZGVlueY//fTTeP311xEREYH58+djxowZGDGic4jKv/zlL9Dp3LurlYiIzieXyaEL0EIX0PPN\ngedyOB1o7mjp/nRAl/EBrN0uEVhR1FoLh9PR73ZVMmWPjwP2dGOgRhWMQA4ffEm8Fuo5OTmYPn06\nACA+Ph719fWwWq3QaDQoLi6GTqdDVFQUAGDKlCnIycnBiBEjcOLECRw/fhxTp071VmlERHQOmSDr\nDFpVMCLduN/O6XSixdba+6iB5zwpUN5UgY5GW7/bVQhyaFQahAZpESgEnX8G4JxLBEGKQI4c2IXX\nQt1isSAxMdH12mAwwGw2Q6PRwGw2w2AwdJtXXFwMAHjmmWfw5JNPYseOHW69j16vhkLh2Rs9jEb3\n9mz9AXvxPVLpA2Avvmhg+wgBYHJrSafTiTZbGxrarKf/NHb/u7X7tAqrGa22tn63KxNk0AZoEOL6\no+3+c2D3aVqVBjLZwO8EDNTvZcDueHA6+79Ws2PHDiQnJ2PIkCH9LntGbW3zpZR1nt7Gs/ZH7MX3\nSKUPgL34In/oQ0AAdAiAThkGKAFoel7OaNSitKKmc+TAnkYNPOcSgaWpFsX1/X+HgAABamXQOY8D\n9nWT4KXfHOjp30tfOwheC3WTyQSLxeJ6XVVVBaPR2OO8yspKmEwm7NmzB8XFxdizZw8qKiqgUqkQ\nGRmJa665xltlEhGRj1LJlVD1c3NgV503BzZ1f0zwvJsEz+4gVDWb3bo5MEgR2OsXB/U0doBKxOGD\nvRbqqamp2LRpEzIzM1FYWAiTyQSNpnOXLDY2FlarFSUlJYiMjMTu3bvx7LPPYv78+a71N23ahJiY\nGAY6ERG5pfPmwBDoAkLcWt7hdKCpo7mfUQPP3iRY3Vrs3s2BclW3YYNnjL4W8YEjL7U9t3gt1FNS\nUpCYmIjMzEwIgoBVq1YhOzsbWq0WaWlpWL16NZYvXw4AmD17NuLi4rxVChER0XlkggxaVed1dgRH\n9Lt8582BLecd9Td2dD8DcGYHobSpHLZGG/QlWsSPGJhQF5zuXOz2YZ6+fuQP16TcxV58j1T6ANiL\nL5JKH4A0enE6nWiztyM2MgwWi9Vj2+3rmjqfAyAiIvICQRAQqBjY5+4Z6kRERBLBUCciIpIIhjoR\nEZFEMNSJiIgkgqFOREQkEQx1IiIiiWCoExERSQRDnYiISCIY6kRERBLBUCciIpIIhjoREZFE+P0X\nuhAREVEnHqkTERFJBEOdiIhIIhjqREREEsFQJyIikgiGOhERkUQw1ImIiCRCIXYBA23t2rXIy8uD\nIAhYuXIlkpKSXPO+/U1GRDIAAAgSSURBVPZbrF+/HnK5HNdddx0WLVrU7zpi6aum7777DuvXr4dM\nJkNcXBzWrFmDH374AY899hhGjhwJABg1ahSefPJJscrvpq9epk2bhsjISMjlcgDAs88+i4iICJ/8\nnQC991JZWYnf//73ruWKi4uxfPlydHR0YOPGjRg6dCgA4JprrsHDDz8sSu3n+vnnn/HII4/gvvvu\nw/z587vN86fPCtB3L/70eemrD3/7rPTWiz9+VtatW4cDBw7AZrPhwQcfxI033uiaN+CfFecg8v33\n3zsXLlzodDqdzuPHjzvT09O7zZ81a5azrKzMabfbnXfddZfz2LFj/a4jhv5qSktLc5aXlzudTqfz\n0Ucfde7Zs8f53XffOR999NEBr7U//fVy/fXXO61W6wWtIxZ36+ro6HBmZmY6rVar891333X+13/9\n10CW6Zampibn/PnznU888YRzy5Yt5833l8+K09l/L/7yeemvD3/6rPTXyxn+8FnJyclx/u53v3M6\nnU5nTU2Nc8qUKd3mD/RnZVCdfs/JycH06dMBAPHx8aivr4fVagXQuTeo0+kQFRUFmUyGKVOmICcn\np891xNJfTdnZ2YiMjAQAGAwG1NbWilKnOy7m39cXfyeA+3W99957mDFjBoKDgwe6RLepVCq8+uqr\nMJlM583zp88K0HcvgP98Xvrroyf++js5wx8+K1dccQU2btwIAAgJCUFLSwvsdjsAcT4rgyrULRYL\n9Hq967XBYIDZbAYAmM1mGAyG8+b1tY5Y+qtJo9EAAKqqqrB3715MmTIFAHD8+HE89NBDuOuuu7B3\n796BLboX7vz7rlq1CnfddReeffZZOJ1On/ydAO71AgDbtm3DvHnzXK/37duH3/72t7j33ntx+PDh\nAam1PwqFAoGBgT3O86fPCtB3L4D/fF766wPwn8+KO70A/vFZkcvlUKvVAIDt27fjuuuuc10CEeOz\nMuiuqXflvIgRci9mHW/rqabq6mo89NBDWLVqFfR6PYYPH47Fixdj1qxZKC4uxj333INPP/0UKpVK\nhIp7d24vS5YswbXXXgudTodFixZh165d/a7jK3qq6+DBg7jssstcQTJhwgQYDAZMnToVBw8exIoV\nK/Dhhx8OdKle4au/l5746+elK3/+rPTE3z4rn3/+ObZv34433njjgtf15O9lUIW6yWSCxWJxva6q\nqoLRaOxxXmVlJUwmE5RKZa/riKWvPgDAarXigf/f3h2ENPnHcRx/P/WQp4HIah1WUDoXO6mQEROD\noBA1CDp5iIjVJRQiVPCwrEttIhF1SUpChoqdIqSOCRNKrIOVCoIgYpAHBaddaq4Og4cM3YT+f7fn\neT6v0/Zsz8Pvy2/ffZ/f79me3/Xr3Lx5k7q6OgB8Ph+NjY0AHD16FK/Xy/LyMkeOHNnbxv8lXywX\nL160HtfX1zM3N5d3n0LZTbvGxsY4ffq09by8vJzy8nIAqqurWV1dZXNz0zrTL0Z2ypXdsFO+5GKn\nXNkNO+VKMpnkyZMnPHv2DI/HY20vRK64avo9HA5bZ6/T09McOnTIOgv0+/1sbGywtLREOp3m7du3\nhMPhnPsUSr42xWIxrly5Qn19vbXt1atX9Pf3A9kpoZWVFXw+3942fBu5YllfXycSifDjxw8AJicn\nCQQCRdknkL9fAD5//syJEyes50+fPmV0dBTI/hq4rKysKL6kcrFTruyGnfJlJ3bLld2wS66sr6/T\n09NDX18fpaWlW14rRK64bpW23t5ePnz4gGEYdHd3MzMzg8fj4dy5c0xOTtLb2wvA+fPniUQi2+7z\n5wetUHaKo66ujpMnT1JdXW29t7m5maamJtrb20mlUvz8+ZPW1lbr2mGh5eqTgYEBXr58SUlJCaFQ\niGg0imEYRdknkDsWgAsXLvD8+XO8Xi8A3759o6Ojg1+/fpFOp4vmL0dfvnwhHo/z9etXTNPE5/Nx\n9uxZ/H6/7XIlVyx2ypd8fWKnXMkXC9gnV0ZGRnj8+DHHjh2ztp06dYpgMFiQXHFdURcREXEqV02/\ni4iIOJmKuoiIiEOoqIuIiDiEirqIiIhDqKiLiIg4hKtuPiMiWy0tLdHQ0LDlL10AZ86c4dq1a/98\n/ImJCR4+fMjw8PA/H0tE8lNRF3G5srIyEolEoZshIv8BFXUR2VYoFOLGjRtMTEzw/ft3YrEYlZWV\nTE1NEYvFME0TwzC4ffs2FRUVLCwsEI1GyWQylJSUcP/+fQAymQzd3d3Mzs5y4MAB+vr6inrVLRE7\n0zV1EdnW5uYmgUCARCJBS0sLjx49AqCzs5Ouri4SiQRXr17l7t27QHaFsEgkwuDgIJcuXeLNmzcA\nzM/P09bWxosXLzBNk/Hx8YLFJOJ0GqmLuNzq6iqXL1/esq2jowPAWuCkpqaG/v5+UqkUKysr1u05\na2truXXrFgCfPn2itrYWgKamJiB7Tf348ePWrT4PHz5MKpX6/4MScSkVdRGXy3VN/c+7SBuGgWEY\nO74O2an2vxXDohsibqHpdxHZ0fv37wH4+PEjwWAQj8fDwYMHmZqaAuDdu3dUVVUB2dF8MpkE4PXr\n1zx48KAwjRZxMY3URVxuu+l3v98PwMzMDMPDw6ytrRGPxwGIx+PEYjH279/Pvn37uHPnDgDRaJRo\nNMrQ0BCmaXLv3j0WFxf3NBYRt9MqbSKyrWAwyPT0NKapc38Ru9D0u4iIiENopC4iIuIQGqmLiIg4\nhIq6iIiIQ6ioi4iIOISKuoiIiEOoqIuIiDiEirqIiIhD/AZd4FLh3xV72AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nh6yX-0H2rbe"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Substitute Model\n",
        "First, mirror the architecture of the oracle:\n",
        "    "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NlBfPzFMSslC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "substitute = Sequential()\n",
        "\n",
        "input_layer = Dense(\n",
        "    units=856,\n",
        "    activation='relu',\n",
        "    input_dim=856,\n",
        ")\n",
        "hidden_layer = Dense(\n",
        "    units=30,\n",
        "    activation='relu',\n",
        ")\n",
        "output_layer = Dense(\n",
        "    units=50,\n",
        "    activation='softmax',\n",
        ")\n",
        "\n",
        "substitute.add(input_layer)\n",
        "substitute.add(hidden_layer)\n",
        "substitute.add(output_layer)\n",
        "\n",
        "# We need to convert our substitute model into the cleverhans format.\n",
        "substitute = KerasModelWrapper(substitute)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w1SpMeOnVNAl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tensorflow_session = tensorflow.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qGUtASc8WXht"
      },
      "cell_type": "markdown",
      "source": [
        "We start by giving the adversary a small dataset to bootstrap its search. We give it a random sample of 5% of the original data set. \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1PQRJpIqWkEE",
        "outputId": "85443418-805f-447c-8c6f-3d468b11d5dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_training_set, adversary_test_set = train_test_split(\n",
        "    labelled_dataset,\n",
        "    train_size=0.05,\n",
        "    stratify=labelled_dataset['user'],\n",
        ")\n",
        "\n",
        "adversary_training_inputs = adversary_training_set.drop('user', axis='columns')\n",
        "adversary_training_labels = adversary_training_set['user'] - 1  # keras requires 0 based index\n",
        "\n",
        "# For some reason cleverhans doesn't detect a GPU when it runs, but our models at the top using\n",
        "# keras _do_. I think this creates a type mis-match: code running on the GPU uses numpy.float64\n",
        "# whilst the cleverhans stuff runs on the CPU and extects numpy.float32 (or vica versa).\n",
        "adversary_training_inputs = adversary_training_inputs.values.astype(numpy.float32)\n",
        "adversary_training_labels = adversary_training_labels.values"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g2ixBGzrbVCW"
      },
      "cell_type": "markdown",
      "source": [
        "Define symbolic input placeholders for use in Tensor Flow:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dtn01RpmawNm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "number_of_users = 50\n",
        "number_of_commands = 856\n",
        "\n",
        "input_placeholder = tensorflow.placeholder(\n",
        "    tensorflow.float32,\n",
        "    shape=(None, number_of_commands)\n",
        ")\n",
        "\n",
        "output_placeholder = tensorflow.placeholder(\n",
        "    tensorflow.float32,\n",
        "    shape=(None, number_of_users)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cqBt04p5byWx"
      },
      "cell_type": "markdown",
      "source": [
        "Get the oracle's predictions for the bootstrap inputs:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tXSbKk1MTlaZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bootstrap_oracle_predictions = oracle.predict(adversary_training_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AW8OmE7dc7wX"
      },
      "cell_type": "markdown",
      "source": [
        "Train substitute using Jacobian Dataset Augmentation:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J5GRMMLCneXs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the predictions and loss of the model, symbolically in TensorFlow (i.e. these variables \n",
        "# point to the result of calculations that haven't been performed yet)\n",
        "\n",
        "substitute_predictions = substitute.get_logits(input_placeholder)\n",
        "substitute_loss = CrossEntropy(substitute, smoothing=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cRko-bZKnbpg",
        "outputId": "a20c7349-4407-4b2c-81c0-fee2b9e6a614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the Jacobian symbolically using TensorFlow\n",
        "grads = jacobian_graph(substitute_predictions, input_placeholder, number_of_users)\n",
        "\n",
        "number_of_dataset_augmentation_batches = 5\n",
        "dataset_augmentation_batch_size = 512\n",
        "\n",
        "\n",
        "stepsize = 1  # this is the step-size of the Jacobian augmentation (we are working in ints so use 1).\n",
        "\n",
        "\n",
        "# Train the substitute and augment dataset\n",
        "for batch in range(number_of_dataset_augmentation_batches):\n",
        "    print(\"BATCH #\" + str(batch))\n",
        "    \n",
        "    print(\"Substitute training epoch:\")\n",
        "    train(\n",
        "        tensorflow_session, \n",
        "        substitute_loss,\n",
        "        adversary_training_inputs, \n",
        "        keras.utils.to_categorical(adversary_training_labels, num_classes=50),\n",
        "        init_all=False,\n",
        "        args={\n",
        "            'nb_epochs': 10,\n",
        "            'batch_size': 32,\n",
        "            'learning_rate': 0.001,\n",
        "        },\n",
        "        rng=rng,\n",
        "    )\n",
        "    \n",
        "\n",
        "    # If we are not at last substitute training iteration, augment dataset\n",
        "    in_final_batch = batch == number_of_dataset_augmentation_batches - 1\n",
        "    if not in_final_batch:\n",
        "        print(\"Generating new data points:\")\n",
        "        \n",
        "        # Use Jacobian augmentation to generate new data points:\n",
        "        step_coef = 2 * int(int(batch / 3) != 0) - 1 \n",
        "\n",
        "        augmented_dataset_inputs = jacobian_augmentation(\n",
        "            tensorflow_session, \n",
        "            input_placeholder, \n",
        "            adversary_training_inputs, \n",
        "            adversary_training_labels,\n",
        "            grads,\n",
        "            step_coef * stepsize,\n",
        "            dataset_augmentation_batch_size,\n",
        "        )\n",
        "        new_datapoints = augmented_dataset_inputs[len(adversary_training_inputs):]\n",
        "\n",
        "        # Send the newly generated data points to the oracle, and use its output as their labels:\n",
        "        new_labels = oracle.predict(new_datapoints)\n",
        "\n",
        "        # Use argmax to get the most likely label. This follows the blackbox attack model - the\n",
        "        # substitute shouldn't be able to see exact prediction confidence.\n",
        "        new_labels = numpy.argmax(new_labels, axis=1)\n",
        "\n",
        "        augmented_dataset_labels = numpy.hstack([adversary_training_labels, new_labels])\n",
        "\n",
        "        # Replace dataset and labels with augmented dataset and labels\n",
        "        adversary_training_inputs = augmented_dataset_inputs\n",
        "        adversary_training_labels = augmented_dataset_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH #0\n",
            "Substitute training epoch:\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/cleverhans/compat.py:124: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/cleverhans/utils_tf.py:511: UserWarning: No GPUS, running on CPU\n",
            "  warnings.warn(\"No GPUS, running on CPU\")\n",
            "[INFO 2019-03-18 15:01:45,689 cleverhans] Epoch 0 took 2.0771210193634033 seconds\n",
            "[INFO 2019-03-18 15:01:47,819 cleverhans] Epoch 1 took 1.9393043518066406 seconds\n",
            "[INFO 2019-03-18 15:01:49,957 cleverhans] Epoch 2 took 1.9512372016906738 seconds\n",
            "[INFO 2019-03-18 15:01:51,987 cleverhans] Epoch 3 took 1.8366508483886719 seconds\n",
            "[INFO 2019-03-18 15:01:54,105 cleverhans] Epoch 4 took 1.9322068691253662 seconds\n",
            "[INFO 2019-03-18 15:01:56,228 cleverhans] Epoch 5 took 1.9418976306915283 seconds\n",
            "[INFO 2019-03-18 15:01:58,237 cleverhans] Epoch 6 took 1.8289566040039062 seconds\n",
            "[INFO 2019-03-18 15:02:00,328 cleverhans] Epoch 7 took 1.9082238674163818 seconds\n",
            "[INFO 2019-03-18 15:02:02,449 cleverhans] Epoch 8 took 1.9298672676086426 seconds\n",
            "[INFO 2019-03-18 15:02:04,535 cleverhans] Epoch 9 took 1.903015375137329 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #1\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-18 15:02:35,025 cleverhans] Epoch 0 took 4.265499114990234 seconds\n",
            "[INFO 2019-03-18 15:02:39,431 cleverhans] Epoch 1 took 3.966740369796753 seconds\n",
            "[INFO 2019-03-18 15:02:44,056 cleverhans] Epoch 2 took 4.188504457473755 seconds\n",
            "[INFO 2019-03-18 15:02:48,267 cleverhans] Epoch 3 took 3.7840728759765625 seconds\n",
            "[INFO 2019-03-18 15:02:52,730 cleverhans] Epoch 4 took 4.039032220840454 seconds\n",
            "[INFO 2019-03-18 15:02:56,880 cleverhans] Epoch 5 took 3.7284364700317383 seconds\n",
            "[INFO 2019-03-18 15:03:01,341 cleverhans] Epoch 6 took 4.043584823608398 seconds\n",
            "[INFO 2019-03-18 15:03:05,763 cleverhans] Epoch 7 took 3.999586343765259 seconds\n",
            "[INFO 2019-03-18 15:03:10,156 cleverhans] Epoch 8 took 3.9749202728271484 seconds\n",
            "[INFO 2019-03-18 15:03:14,257 cleverhans] Epoch 9 took 3.680140733718872 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #2\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-18 15:04:15,706 cleverhans] Epoch 0 took 8.24474573135376 seconds\n",
            "[INFO 2019-03-18 15:04:24,634 cleverhans] Epoch 1 took 7.891599893569946 seconds\n",
            "[INFO 2019-03-18 15:04:33,035 cleverhans] Epoch 2 took 7.477725505828857 seconds\n",
            "[INFO 2019-03-18 15:04:41,251 cleverhans] Epoch 3 took 7.287150621414185 seconds\n",
            "[INFO 2019-03-18 15:04:49,604 cleverhans] Epoch 4 took 7.430698394775391 seconds\n",
            "[INFO 2019-03-18 15:04:57,749 cleverhans] Epoch 5 took 7.225452661514282 seconds\n",
            "[INFO 2019-03-18 15:05:05,951 cleverhans] Epoch 6 took 7.276463270187378 seconds\n",
            "[INFO 2019-03-18 15:05:14,135 cleverhans] Epoch 7 took 7.269887924194336 seconds\n",
            "[INFO 2019-03-18 15:05:22,527 cleverhans] Epoch 8 took 7.476808071136475 seconds\n",
            "[INFO 2019-03-18 15:05:30,764 cleverhans] Epoch 9 took 7.311864376068115 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5uGnk1fOTqEX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluating Substitute Model\n",
        "\n",
        "Here we evaluate the substitute against the 95% of the dataset it hasn't seen."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2ACXDjruQtfg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "adversary_test_inputs = adversary_test_set.drop('user', axis='columns')\n",
        "adversary_test_labels = adversary_test_set['user'] - 1  # keras requires 0 based index\n",
        "\n",
        "# For some reason cleverhans doesn't detect a GPU when it runs, but our models at the top using\n",
        "# keras _do_. I think this creates a type mis-match: code running on the GPU uses numpy.float64\n",
        "# whilst the cleverhans stuff runs on the CPU and expects numpy.float32 (or vica versa).\n",
        "adversary_test_inputs = adversary_test_inputs.values.astype(numpy.float32)\n",
        "adversary_test_labels = adversary_test_labels.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPGuaA-mVBPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, check its accuracy against the true labels:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xOOplTcKcpNZ",
        "outputId": "0be140e2-a829-4f43-ff49-c803a71b8698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_test_labels_one_hot = keras.utils.to_categorical(adversary_test_labels, num_classes=50)\n",
        "\n",
        "# Evaluate the substitute model on clean test examples against true labels\n",
        "acc = model_eval(\n",
        "    tensorflow_session, \n",
        "    input_placeholder,\n",
        "    output_placeholder,\n",
        "    substitute_predictions,\n",
        "    adversary_test_inputs,\n",
        "    adversary_test_labels_one_hot,\n",
        "    args={'batch_size': 32}\n",
        ")\n",
        "acc"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.948801106538716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "8x0v7i-TT8BS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, send this test dataset into the oracle to get it's predictions. Then, compare the substitute models  predctions against those of the oracle. This is important, as it allows us to measure: how good of an imitatin of the oracle our substitute is."
      ]
    },
    {
      "metadata": {
        "id": "LcsqLdLlT5G_",
        "colab_type": "code",
        "outputId": "a004c8c8-9412-45de-8be5-a128f45f8964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "oracle_predicted_labels = oracle.predict(adversary_test_inputs)\n",
        "\n",
        "# Evaluate the substitute model on clean test examples against oracle's labels\n",
        "acc = model_eval(\n",
        "    tensorflow_session, \n",
        "    input_placeholder,\n",
        "    output_placeholder,\n",
        "    substitute_predictions,\n",
        "    adversary_test_inputs,\n",
        "    oracle_predicted_labels,\n",
        "    args={'batch_size': 32}\n",
        ")\n",
        "acc"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9488182888169142"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "Q8F_S8nNUgKT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: How do these two accuracies compare? What does that mean about our substitute? What does that say about our oracle?)"
      ]
    },
    {
      "metadata": {
        "id": "AyW24uVuUplz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Synthetic Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fpkj-jxkWvQn"
      },
      "cell_type": "markdown",
      "source": [
        "Just inspecting the generated dataset. Notes:\n",
        "  1. Some of the values are negative!\n",
        "  2. The real dataset has an input range of 0-100. This search technique has found all of them, plus a few on each side.\n",
        "  3. The augmented dataset has just less than 200,000 data points. That's almost as many as were used to train the oracle."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t8HgM1JXTxzj",
        "outputId": "84dd8c51-7a72-4d65-af04-cfaa702388d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "numpy.unique(adversary_training_inputs)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,\n",
              "         7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n",
              "        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,\n",
              "        29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,\n",
              "        40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,\n",
              "        51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,\n",
              "        62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,  72.,\n",
              "        73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,\n",
              "        84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
              "        95.,  96.,  97.,  98.,  99., 100., 101.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SnKRNqzmU09-",
        "outputId": "0ebf7f2c-6c1f-47d1-d552-23ed696188d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(adversary_training_inputs)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "POy7p0XmVJt9",
        "outputId": "76e59a05-1f48-44e0-aae1-c216dd907c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "cell_type": "code",
      "source": [
        "pandas.DataFrame(adversary_training_inputs[numpy.random.choice(adversary_training_inputs.shape[0], size=20)])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>846</th>\n",
              "      <th>847</th>\n",
              "      <th>848</th>\n",
              "      <th>849</th>\n",
              "      <th>850</th>\n",
              "      <th>851</th>\n",
              "      <th>852</th>\n",
              "      <th>853</th>\n",
              "      <th>854</th>\n",
              "      <th>855</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 856 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    0    1    2    3    4    5    6    7    8    9   ...   846  847  848  849  \\\n",
              "0  -1.0  1.0  1.0 -1.0  1.0  1.0 -1.0 -1.0 -1.0 -1.0 ...   4.0  1.0  1.0  1.0   \n",
              "1   2.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0 ...   2.0 -2.0  0.0  2.0   \n",
              "2   1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 ...   1.0 -1.0 -1.0 -1.0   \n",
              "3  -3.0 -1.0 -1.0  1.0  3.0  3.0  1.0  1.0 -1.0  1.0 ...  -1.0 -1.0  1.0  1.0   \n",
              "4   0.0  0.0  2.0  0.0 -2.0  0.0 -2.0 -2.0 -2.0  2.0 ...   2.0  2.0  0.0  2.0   \n",
              "5  -1.0  1.0 -1.0  1.0  5.0  1.0 -1.0  1.0 -1.0  2.0 ...   1.0  1.0  1.0  1.0   \n",
              "6  -3.0  3.0 -1.0  1.0  1.0 -1.0 -1.0  3.0 -1.0  5.0 ...   3.0 -1.0  1.0 -1.0   \n",
              "7  -3.0  1.0 -1.0 -1.0  1.0  1.0 -3.0  3.0 -3.0  3.0 ...   2.0 -3.0  1.0 -1.0   \n",
              "8  -1.0  1.0 -1.0 -1.0  1.0  1.0 -1.0  1.0 -1.0  1.0 ...   1.0  1.0  1.0 -1.0   \n",
              "9  -1.0  1.0 -1.0 -1.0  3.0  1.0 -1.0 -3.0 -1.0  1.0 ...  -1.0  3.0 -1.0 -1.0   \n",
              "10  0.0  2.0  0.0  0.0  0.0  2.0  0.0  2.0 -2.0 -2.0 ...   2.0  0.0  0.0  0.0   \n",
              "11 -1.0  1.0  1.0 -1.0  1.0 -1.0  1.0 -1.0 -1.0 -1.0 ...   1.0  1.0  1.0  1.0   \n",
              "12  0.0  0.0  0.0 -2.0 -2.0  0.0  0.0  0.0 -2.0  3.0 ...   2.0  2.0  2.0  2.0   \n",
              "13 -2.0  2.0 -2.0  2.0  0.0  2.0  2.0  0.0  0.0 -2.0 ...   0.0 -2.0  0.0 -2.0   \n",
              "14  1.0  1.0 -1.0  1.0  3.0  1.0  1.0 -1.0 -1.0 -1.0 ...   1.0  1.0 -1.0 -1.0   \n",
              "15 -1.0  1.0  1.0  1.0  1.0  1.0  1.0 -1.0 -1.0  1.0 ...  -1.0  1.0  1.0  1.0   \n",
              "16  0.0  0.0  2.0  2.0  2.0  0.0  2.0  0.0  0.0  0.0 ...  -2.0  0.0  0.0  0.0   \n",
              "17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
              "18 -1.0  1.0 -1.0 -1.0  1.0  1.0 -1.0  1.0 -1.0  0.0 ...   1.0  1.0  1.0  1.0   \n",
              "19 -2.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0 ...   0.0  0.0 -2.0  2.0   \n",
              "\n",
              "    850  851  852  853  854  855  \n",
              "0   1.0  1.0  3.0 -1.0  1.0  1.0  \n",
              "1   0.0  0.0  0.0  2.0  0.0  0.0  \n",
              "2  -1.0 -1.0  1.0  1.0 -1.0  1.0  \n",
              "3  -1.0  1.0 -1.0 -1.0  3.0 -1.0  \n",
              "4  -2.0  2.0 -2.0 -2.0  2.0  2.0  \n",
              "5   1.0  1.0 -1.0 -1.0  1.0  1.0  \n",
              "6   1.0 -1.0 -1.0  1.0  3.0 -1.0  \n",
              "7  -1.0  3.0 -1.0  1.0  3.0  1.0  \n",
              "8  -1.0  1.0  1.0  1.0  1.0 -1.0  \n",
              "9  -3.0  1.0 -1.0 -1.0  1.0 -1.0  \n",
              "10  0.0  0.0  0.0  0.0  2.0  0.0  \n",
              "11 -1.0 -1.0 -1.0 -1.0  1.0 -1.0  \n",
              "12  0.0 -2.0  2.0  2.0  2.0  0.0  \n",
              "13  0.0  0.0 -2.0  0.0  0.0  0.0  \n",
              "14  3.0 -1.0 -1.0  1.0 -1.0 -1.0  \n",
              "15  1.0 -1.0  1.0 -1.0 -1.0  1.0  \n",
              "16 -2.0 -2.0  0.0  0.0  0.0  0.0  \n",
              "17  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "18  1.0 -1.0 -1.0 -1.0  1.0 -1.0  \n",
              "19  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[20 rows x 856 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qS3LcfujWlvM"
      },
      "cell_type": "markdown",
      "source": [
        "# Crafting Adversarial Examples\n",
        "\n",
        "First we build an attack using the Fast Gradient Sign method. This attack is then used to generate untargeted adversarial examples for each value in our test set."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EziI1AYUcqk_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the Fast Gradient Sign Method (FGSM) attack object.\n",
        "fgsm_par = {'eps': 1., 'ord': numpy.inf, 'clip_min': 0., 'clip_max': 100.}\n",
        "fgsm = FastGradientMethod(substitute, sess=tensorflow_session)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uuP-pUO4cr4b",
        "outputId": "a6317d1b-1f91-4fa5-ee6e-256616fa734a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Craft adversarial examples using the substitute\n",
        "eval_params = {'batch_size': dataset_augmentation_batch_size}\n",
        "x_adv_sub = fgsm.generate(input_placeholder, **fgsm_par)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/cleverhans/attacks/__init__.py:283: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xwDuDMCDC9AY",
        "colab_type": "code",
        "outputId": "69776788-f723-4300-9f9a-7bb9d9b0250b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_adv_sub"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Identity:0' shape=(?, 856) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "uxwrvuwCC9Ae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle_keras = KerasModelWrapper(oracle)\n",
        "oracle_fgsm_pred = oracle_keras.get_logits(x_adv_sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ViBivzAMC9Ap",
        "colab_type": "code",
        "outputId": "b83756b1-56c3-4446-a97b-ef1a57ddd611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "oracle_fgsm_pred"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(50)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZKJui3cVctC5",
        "outputId": "26dd3aee-9070-4eee-aa20-1979e6868f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the \"black-box\" model on adversarial examples\n",
        "accuracy = model_eval(\n",
        "        tensorflow_session,\n",
        "        input_placeholder,\n",
        "        output_placeholder,\n",
        "        oracle_fgsm_pred,\n",
        "        adversary_test_inputs,\n",
        "        adversary_test_labels_one_hot,\n",
        "        args=eval_params\n",
        ")\n",
        "print('Test accuracy of oracle on adversarial examples generated '\n",
        "    'using the substitute: ' + str(accuracy))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy of oracle on adversarial examples generated using the substitute: 0.01817025919466662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CC9zxfxrC9A9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the above we have shown that the accuracy of the oracle is reduced to 1.6% when using our minimally changed command vectors! This is an example of an untargeted attack: all we are trying to do is get the oracle to misclassify a datapoint as a user which we believe truly represents a different user."
      ]
    },
    {
      "metadata": {
        "id": "9RTbGaCxC9A_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Targetting Particular Users\n",
        "\n",
        "Pick an example command vector which wasn't user 2, then generate a similar example that is classified as user 2. \n",
        "\n",
        "We then put that example into the oracle model and check it's classification. If it is classified as user 2, our attempt was successfull."
      ]
    },
    {
      "metadata": {
        "id": "DwzaxOtEC9BC",
        "colab_type": "code",
        "outputId": "df095c15-122f-4e42-9646-fd84736cce7a",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "original_command_vectors = numpy.array([adversary_test_inputs[0]])\n",
        "original_labels = numpy.array([adversary_test_labels_one_hot[0]])\n",
        "\n",
        "target_labels = keras.utils.to_categorical(numpy.array([2]), num_classes=50)\n",
        "\n",
        "fgsm_attack = FastGradientMethod(substitute, sess=tensorflow_session)\n",
        "fgsm_params = {\n",
        "    'eps': 1.0,\n",
        "    'ord': numpy.inf,\n",
        "    'clip_min': 0.0,\n",
        "    'clip_max': 100.0,\n",
        "}\n",
        "adversarial_examples = fgsm_attack.generate_np(\n",
        "    original_command_vectors,\n",
        "    y_target=target_labels,\n",
        "    **fgsm_params,\n",
        ")\n",
        "\n",
        "predicted_labels = oracle.predict(adversarial_examples)\n",
        "\n",
        "print(\"original label = {}\".format(adversary_test_labels[0]))\n",
        "print(\"predicted label = {}\".format(numpy.argmax(predicted_labels[0])))\n",
        "print(\"prediction certainty = {}\".format(numpy.max(predicted_labels[0])))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-18 13:32:46,895 cleverhans] Constructing new graph for attack FastGradientMethod\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original label = 43\n",
            "predicted label = 26\n",
            "prediction certainty = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ihlyeUfXNqNn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Whilst  the above demonstrates the attacks effectiveness with one particular example, targetted against one particular user, we should be sure to evaluate the attacks efffectiveness in general.\n",
        "\n",
        "In fact, it is most often the case that the fast gradient sign method is able to generate successful adversarial examples based on a particular starting vector, and a particular target user.\n",
        "\n",
        "We define a function below to take in one particular command vector, and perform a targeted attack against each of the users."
      ]
    },
    {
      "metadata": {
        "id": "wBmYQjCowOFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_targeted_attack_against_all_users(command_vector, label, attack, attack_params):\n",
        "    \"\"\"\n",
        "    Runs a targeted attack for the given sample against. For each user, we attempt to generate a\n",
        "    similar command vector to the original, which is classified as that user.\n",
        "    \n",
        "    The command prints a summary of the results to stdout, then returns a dataframe containing, \n",
        "    for each attack:\n",
        "      - the original user\n",
        "      - the targeted user\n",
        "      - the oracle models prediction against the adversarial example\n",
        "      - the oracles certainty of that prediction\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Since we run this once against all users, make 50 replicas of the command vector, and\n",
        "    # original label:\n",
        "    original_label_one_hot = keras.utils.to_categorical(label, num_classes=50)\n",
        "    original_labels = numpy.tile(original_label_one_hot, reps=(50,1))\n",
        "    \n",
        "    original_command_vectors = numpy.tile(command_vector, reps=(50, 1))\n",
        "    \n",
        "    # Our target labels are the one-hot-encoded values 0, 1, 2, ..., 49:\n",
        "    target_labels = keras.utils.to_categorical(range(50), num_classes=50)\n",
        "\n",
        "    attack_params['y_target'] = target_labels\n",
        "    \n",
        "    # Apply the attack, generating the adversarial examples:\n",
        "    adversarial_examples = attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        **attack_params,\n",
        "    )\n",
        "\n",
        "    # Stick these examples into the oracle, and find out what classification it gives:\n",
        "    predictions = oracle.predict(adversarial_examples)\n",
        "\n",
        "    # Format the results into a summary dataframe:\n",
        "    original_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=original_labels), # undo one hot encode\n",
        "        name='Original User',\n",
        "    )\n",
        "    target_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=target_labels), # undo one hot encode\n",
        "        name='Target User',\n",
        "    )\n",
        "    predicted_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=predictions),  # undo one hot encode\n",
        "        name='Oracle Prediction',\n",
        "    )\n",
        "    prediction_certainty = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.max, axis=1, arr=predictions),\n",
        "        name='Oracle Certainty',\n",
        "    )\n",
        "\n",
        "    summary = pandas.concat(\n",
        "        [\n",
        "            original_label,\n",
        "            target_label,\n",
        "            predicted_label,\n",
        "            prediction_certainty,\n",
        "        ],\n",
        "        axis='columns',\n",
        "    )\n",
        "    \n",
        "    # Count the number of targeted attacks which were succcessful:\n",
        "    successful_attacks = summary.apply(lambda row: row[1] == row[2], axis='columns').sum()\n",
        "    \n",
        "    # Don't count  the original_user -> original_user attack:\n",
        "    successful_attacks -= 1 \n",
        "    total_attacks = 49\n",
        "    \n",
        "    # Print out a little message to say how we did :)\n",
        "    print(\n",
        "        \"A targeted attack was successful against {}/{} users (with the given input):\"\n",
        "        .format(successful_attacks, total_attacks)\n",
        "    )\n",
        "\n",
        "    return summary, adversarial_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B0QEFE1qMqVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, below we take a command vector not yet seen by the substitute model and try to craft adversarial examples targeted at each user. As you can see, the attack is relatively sucessful!"
      ]
    },
    {
      "metadata": {
        "id": "bOi6MhmAMm0c",
        "colab_type": "code",
        "outputId": "a00a1863-8234-4877-f097-088f1976917d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1635
        }
      },
      "cell_type": "code",
      "source": [
        "summary, _ = run_targeted_attack_against_all_users(\n",
        "    adversary_test_inputs[0],\n",
        "    adversary_test_labels[0],\n",
        "    fgsm_attack,\n",
        "    fgsm_params,\n",
        ")\n",
        "summary"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A targeted attack was successful against 2/49 users (with the given input):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original User</th>\n",
              "      <th>Target User</th>\n",
              "      <th>Oracle Prediction</th>\n",
              "      <th>Oracle Certainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.999068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>43</td>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>43</td>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>43</td>\n",
              "      <td>16</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>43</td>\n",
              "      <td>17</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>0.999703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>0.999998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43</td>\n",
              "      <td>22</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>43</td>\n",
              "      <td>23</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>27</td>\n",
              "      <td>0.999060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>43</td>\n",
              "      <td>28</td>\n",
              "      <td>26</td>\n",
              "      <td>0.966046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43</td>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>33</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>43</td>\n",
              "      <td>34</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>48</td>\n",
              "      <td>0.923205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>43</td>\n",
              "      <td>37</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>43</td>\n",
              "      <td>39</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>40</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>42</td>\n",
              "      <td>26</td>\n",
              "      <td>0.843084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>12</td>\n",
              "      <td>0.832954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>48</td>\n",
              "      <td>0.809272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>12</td>\n",
              "      <td>0.999984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>43</td>\n",
              "      <td>48</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Original User  Target User  Oracle Prediction  Oracle Certainty\n",
              "0              43            0                 26          1.000000\n",
              "1              43            1                 26          1.000000\n",
              "2              43            2                 26          1.000000\n",
              "3              43            3                 26          0.999998\n",
              "4              43            4                 26          0.999997\n",
              "5              43            5                 26          0.999997\n",
              "6              43            6                 26          1.000000\n",
              "7              43            7                 26          0.999998\n",
              "8              43            8                 26          1.000000\n",
              "9              43            9                  9          0.999068\n",
              "10             43           10                 26          1.000000\n",
              "11             43           11                 26          1.000000\n",
              "12             43           12                 26          1.000000\n",
              "13             43           13                 26          0.999546\n",
              "14             43           14                 26          1.000000\n",
              "15             43           15                 26          1.000000\n",
              "16             43           16                 26          1.000000\n",
              "17             43           17                 26          1.000000\n",
              "18             43           18                 26          1.000000\n",
              "19             43           19                 26          1.000000\n",
              "20             43           20                  9          0.999703\n",
              "21             43           21                 27          0.999998\n",
              "22             43           22                 26          1.000000\n",
              "23             43           23                 26          1.000000\n",
              "24             43           24                 26          0.999965\n",
              "25             43           25                 27          0.999060\n",
              "26             43           26                 26          1.000000\n",
              "27             43           27                 26          1.000000\n",
              "28             43           28                 26          0.966046\n",
              "29             43           29                 12          1.000000\n",
              "30             43           30                 26          1.000000\n",
              "31             43           31                 26          1.000000\n",
              "32             43           32                 26          1.000000\n",
              "33             43           33                 26          1.000000\n",
              "34             43           34                 26          1.000000\n",
              "35             43           35                 48          0.923205\n",
              "36             43           36                 26          1.000000\n",
              "37             43           37                 26          1.000000\n",
              "38             43           38                 26          1.000000\n",
              "39             43           39                 26          1.000000\n",
              "40             43           40                 26          1.000000\n",
              "41             43           41                 26          1.000000\n",
              "42             43           42                 26          0.843084\n",
              "43             43           43                 43          1.000000\n",
              "44             43           44                 26          1.000000\n",
              "45             43           45                 12          0.832954\n",
              "46             43           46                 48          0.809272\n",
              "47             43           47                 12          0.999984\n",
              "48             43           48                 26          0.999999\n",
              "49             43           49                 26          1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "TWrnWm9ChKU9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "a9355716-36f5-484b-c20d-05630fbfb849"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_targetted_attack(test_inputs, test_labels, attack):\n",
        "    # For each test input, duplicate it 50 times, once for each potential target user 0, 1, ... 49\n",
        "    # Input all of these into the attack and get outputs\n",
        "    # Count rows for which target_user == oracle_prediction\n",
        "    # Divide by total rows\n",
        "    \n",
        "    \n",
        "    # Since we run this once against all users, make 50 replicas of the command vector, and\n",
        "    # original label:\n",
        "    original_label_one_hot = keras.utils.to_categorical(label, num_classes=50)\n",
        "    original_labels = numpy.tile(original_label_one_hot, reps=(50,1))\n",
        "    \n",
        "    original_command_vectors = numpy.tile(command_vector, reps=(50, 1))\n",
        "    \n",
        "    # Our target labels are the one-hot-encoded values 0, 1, 2, ..., 49:\n",
        "    target_labels = keras.utils.to_categorical(range(50), num_classes=50)\n",
        "\n",
        "    attack_params['y_target'] = target_labels\n",
        "    \n",
        "    # Apply the attack, generating the adversarial examples:\n",
        "    adversarial_examples = attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        **attack_params,\n",
        "    )\n",
        "\n",
        "    # Stick these examples into the oracle, and find out what clas"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-7a86d73888a3>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    # Divide by total rows\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ADD0NPmjVd5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom Attack Method\n",
        "\n",
        "The oracle model uses integers and absolute counts as inputs. In contrast to something like percentage inputs. This is quite succesfull in limiting effectiveness of our attacks, the results of which can  be seen above. \n",
        "\n",
        "A common approach to limiting adversarial attacks for image classifiers is to use thresholding. The use of integer inputs serves a simlar purpose in the original model. Simliarly, using absolute values severly limits our search space. When converting a naughty script, we aren't able to remove commands: we need them to perform our evil deeds! If the model took in percentage inputs (as a distribution) we would be able to to lower values by increasing all other values.\n",
        "\n",
        "This combination makes the original model relatively resistant to standard attack methods. In this section we explore methods of overcoming these restrictions. In particular, we extend an attack originally proposed by Carlini and Wagner. It uses the Adam optimiser and prioritises accuracy over speed. We modify it's optimisation function such that it deprioritise negative perturbations.\n",
        "  \n",
        "In contrast to attacks against image classification models, our input does not need to look similar to a human. For this reason, we further modify the optimisation functions not to consider the size of positive perturbations.\n",
        "\n",
        "Our optimisation functions considers the following most important:\n",
        "  - no commands should be removed\n",
        "  - it should classify as strongly as possible to the target class\n"
      ]
    },
    {
      "metadata": {
        "id": "NQHvOoQAVrMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Just use CarliniWagnerL2 and see how it does\n",
        "\n",
        "smm_attack = cleverhans.attacks.MomentumIterativeMethod(substitute, sess=tensorflow_session)\n",
        "smm_params = {\n",
        "    'eps': 100.0,\n",
        "    'eps_iter': 1.0,\n",
        "    'nb_iter': 100,\n",
        "    'ord': 2,\n",
        "    'clip_min': 0.0,\n",
        "    'clip_max': 100.0,\n",
        "}\n",
        "\n",
        "summary, adversarial_examples = run_targeted_attack_against_all_users(adversary_test_inputs[0], adversary_test_labels[0], smm_attack, smm_params)\n",
        "summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5_76KnirHTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_command_vector(command_vector, danger=False):\n",
        "    # reshape into a rectangle, and pad slightly beforehand\n",
        "    rectangle_array= numpy.concatenate([command_vector, numpy.array([0,0])]).reshape((26,33))\n",
        "    normalized_array = sklearn.preprocessing.normalize(rectangle_array)\n",
        "    \n",
        "    color = 'Reds' if danger else 'Greens'\n",
        "    \n",
        "    return seaborn.heatmap(\n",
        "        normalized_array,\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        vmin=0, \n",
        "        vmax=1,\n",
        "        cmap=color,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRzqqNirOD_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversary_test_inputs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtnS6WcruHWZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " plot_command_vector(adversarial_examples[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M4T9zboQuKys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " adversarial_examples[0] >= adversary_test_inputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CTM0eGzkvAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqsWEr5MnLFu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Additive-only Momentum Iterative Method"
      ]
    },
    {
      "metadata": {
        "id": "4RG1Zq-uklS4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from cleverhans.attacks import optimize_linear\n",
        "from cleverhans.compat import reduce_sum, reduce_mean, softmax_cross_entropy_with_logits\n",
        "from cleverhans import utils_tf\n",
        "\n",
        "\n",
        "class AdditiveMomentumIterativeMethod(cleverhans.attacks.MomentumIterativeMethod):\n",
        "  \"\"\"\n",
        "  Modifies the The Momentum Iterative Method (Dong et al. 2017) to produce additive\n",
        "  perturbations only.\n",
        "  \n",
        "  If it finds the optimal perturbation to be negative, a random addition is performed instead.\n",
        "  \n",
        "  Original paper link: https://arxiv.org/pdf/1710.06081.pdf\n",
        "  \n",
        "  :param model: cleverhans.model.Model\n",
        "  :param sess: optional tf.Session\n",
        "  :param dtypestr: dtype of the data\n",
        "  :param kwargs: passed through to super constructor\n",
        "  \"\"\"\n",
        "\n",
        "  def generate(self, x, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate symbolic graph for adversarial examples and return.\n",
        "    :param x: The model's symbolic inputs.\n",
        "    :param kwargs: Keyword arguments. See `parse_params` for documentation.\n",
        "    \"\"\"\n",
        "    # Parse and save attack-specific parameters\n",
        "    assert self.parse_params(**kwargs)\n",
        "\n",
        "    asserts = []\n",
        "\n",
        "    # If a data range was specified, check that the input was in that range\n",
        "    if self.clip_min is not None:\n",
        "      asserts.append(utils_tf.assert_greater_equal(x,\n",
        "                                                   tf.cast(self.clip_min,\n",
        "                                                           x.dtype)))\n",
        "\n",
        "    if self.clip_max is not None:\n",
        "      asserts.append(utils_tf.assert_less_equal(x,\n",
        "                                                tf.cast(self.clip_max,\n",
        "                                                        x.dtype)))\n",
        "\n",
        "    # Initialize loop variables\n",
        "    momentum = tf.zeros_like(x)\n",
        "    adv_x = x\n",
        "\n",
        "    # Fix labels to the first model predictions for loss computation\n",
        "    y, _nb_classes = self.get_or_guess_labels(x, kwargs)\n",
        "    y = y / reduce_sum(y, 1, keepdims=True)\n",
        "    targeted = (self.y_target is not None)\n",
        "\n",
        "    def cond(i, _, __):\n",
        "      \"\"\"Iterate until number of iterations completed\"\"\"\n",
        "      return tf.less(i, self.nb_iter)\n",
        "\n",
        "    def body(i, ax, m):\n",
        "      \"\"\"Do a momentum step\"\"\"\n",
        "      logits = self.model.get_logits(ax)\n",
        "      loss = softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "      if targeted:\n",
        "        loss = -loss\n",
        "\n",
        "      # Define gradient of loss wrt input\n",
        "      grad, = tf.gradients(loss, ax)\n",
        "\n",
        "      # Normalize current gradient and add it to the accumulated gradient\n",
        "      red_ind = list(range(1, len(grad.get_shape())))\n",
        "      avoid_zero_div = tf.cast(1e-12, grad.dtype)\n",
        "      grad = grad / tf.maximum(\n",
        "          avoid_zero_div,\n",
        "          reduce_mean(tf.abs(grad), red_ind, keepdims=True))\n",
        "      m = self.decay_factor * m + grad\n",
        "\n",
        "      optimal_perturbation = optimize_linear(m, self.eps_iter, self.ord)\n",
        "      optimal_perturbation = tf.maximum(optimal_perturbation, tf.zeros_like(optimal_perturbation))\n",
        "        \n",
        "      if self.ord == 1:\n",
        "        raise NotImplementedError(\"This attack hasn't been tested for ord=1.\"\n",
        "                                  \"It's not clear that FGM makes a good inner \"\n",
        "                                  \"loop step for iterative optimization since \"\n",
        "                                  \"it updates just one coordinate at a time.\")\n",
        "\n",
        "      # Update and clip adversarial example in current iteration\n",
        "      ax = ax + optimal_perturbation\n",
        "      ax = x + utils_tf.clip_eta(ax - x, self.ord, self.eps)\n",
        "\n",
        "      if self.clip_min is not None and self.clip_max is not None:\n",
        "        ax = utils_tf.clip_by_value(ax, self.clip_min, self.clip_max)\n",
        "\n",
        "      ax = tf.stop_gradient(ax)\n",
        "\n",
        "      return i + 1, ax, m\n",
        "\n",
        "    _, adv_x, _ = tf.while_loop(\n",
        "        cond, body, (tf.zeros([]), adv_x, momentum), back_prop=True,\n",
        "        maximum_iterations=self.nb_iter)\n",
        "\n",
        "    if self.sanity_checks:\n",
        "      with tf.control_dependencies(asserts):\n",
        "        adv_x = tf.identity(adv_x)\n",
        "\n",
        "    return adv_x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctn1LalQnU4P",
        "colab_type": "code",
        "outputId": "082b0ce1-76bb-4fb6-8a4d-30c557674390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        }
      },
      "cell_type": "code",
      "source": [
        "ami_attack = AdditiveMomentumIterativeMethod(substitute, sess=tensorflow_session)\n",
        "ami_params = {\n",
        "    'eps': 100.0,\n",
        "    'eps_iter': 1.0,\n",
        "    'nb_iter': 100,\n",
        "    'ord': numpy.inf,\n",
        "    'clip_min': 0.0,\n",
        "    'clip_max': 100.0,\n",
        "}\n",
        "\n",
        "summary, adversarial_examples = run_targeted_attack_against_all_users(\n",
        "    adversary_test_inputs[0],\n",
        "    adversary_test_labels[0],\n",
        "    ami_attack,\n",
        "    ami_params,\n",
        ")\n",
        "summary"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-18 14:00:09,219 cleverhans] Constructing new graph for attack AdditiveMomentumIterativeMethod\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A targeted attack was successful against 23/49 users (with the given input):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original User</th>\n",
              "      <th>Target User</th>\n",
              "      <th>Oracle Prediction</th>\n",
              "      <th>Oracle Certainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>43</td>\n",
              "      <td>10</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>48</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>43</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>43</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>43</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>19</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43</td>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>43</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>43</td>\n",
              "      <td>28</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43</td>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>11</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>11</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>33</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>43</td>\n",
              "      <td>34</td>\n",
              "      <td>27</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>48</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>43</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>43</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>40</td>\n",
              "      <td>11</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>43</td>\n",
              "      <td>48</td>\n",
              "      <td>48</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Original User  Target User  Oracle Prediction  Oracle Certainty\n",
              "0              43            0                 14               1.0\n",
              "1              43            1                  1               1.0\n",
              "2              43            2                  2               1.0\n",
              "3              43            3                  3               1.0\n",
              "4              43            4                  0               1.0\n",
              "5              43            5                  5               1.0\n",
              "6              43            6                 14               1.0\n",
              "7              43            7                  7               1.0\n",
              "8              43            8                  8               1.0\n",
              "9              43            9                  9               1.0\n",
              "10             43           10                 12               1.0\n",
              "11             43           11                 11               1.0\n",
              "12             43           12                 12               1.0\n",
              "13             43           13                 48               1.0\n",
              "14             43           14                 14               1.0\n",
              "15             43           15                 19               1.0\n",
              "16             43           16                 17               1.0\n",
              "17             43           17                 17               1.0\n",
              "18             43           18                 19               1.0\n",
              "19             43           19                 19               1.0\n",
              "20             43           20                 20               1.0\n",
              "21             43           21                  8               1.0\n",
              "22             43           22                 12               1.0\n",
              "23             43           23                 23               1.0\n",
              "24             43           24                 12               1.0\n",
              "25             43           25                 25               1.0\n",
              "26             43           26                  2               1.0\n",
              "27             43           27                 27               1.0\n",
              "28             43           28                  7               1.0\n",
              "29             43           29                 12               1.0\n",
              "30             43           30                 11               1.0\n",
              "31             43           31                 11               1.0\n",
              "32             43           32                 32               1.0\n",
              "33             43           33                 12               1.0\n",
              "34             43           34                 27               1.0\n",
              "35             43           35                 48               1.0\n",
              "36             43           36                 36               1.0\n",
              "37             43           37                 37               1.0\n",
              "38             43           38                  8               1.0\n",
              "39             43           39                 39               1.0\n",
              "40             43           40                 11               1.0\n",
              "41             43           41                 12               1.0\n",
              "42             43           42                 42               1.0\n",
              "43             43           43                  8               1.0\n",
              "44             43           44                 44               1.0\n",
              "45             43           45                 12               1.0\n",
              "46             43           46                  8               1.0\n",
              "47             43           47                 47               1.0\n",
              "48             43           48                 48               1.0\n",
              "49             43           49                 39               1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "9SUkSJc2u67k",
        "colab_type": "code",
        "outputId": "ae67817b-328a-4c0a-de07-8e24aa54c1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversary_test_inputs[0])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4b411eceb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEYlJREFUeJzt3W9sndV9B/DvtZOQBGdpLPmqQEDN\nrGYZYdGaUSRqRCRmd1mh2qQhxQiVqqCWTkhdoUxrXQ23gN1Ug0hT1Be06l60oNQSs1i7aUQVo1sV\njJwiNSxhDOKthlAU+zYlnZdkEHr3oq1LSnJzY3yTnOTzia7kh+fc42Px58vvPD+fW6nX6/UAQAHa\nzvQCAKBZQguAYggtAIohtAAohtACoBhCC4BiCC0AWuqFF15Ib29vHn744bfde+qpp3LjjTdm06ZN\n+cpXvnLSuYQWAC1z6NCh3Hfffbn66quPe//+++/P1q1bs23btuzYsSN79+5tOJ/QAqBlFi1alK99\n7WupVqtvu/fyyy9n+fLlueiii9LW1pYNGzZkbGys4XxCC4CWWbBgQRYvXnzce9PT0+ns7Jy97uzs\nzPT0dOP55nV1ABSp0rdyzu+tf3ffPK6kMaEFQFKpnPZvWa1WU6vVZq/3799/3G3Et7I9CMAZsXLl\nyszMzGTfvn05evRonnzyyfT09DR8j0oLgJaVMLt3786Xv/zlvPLKK1mwYEG2b9+e6667LitXrkxf\nX1++8IUv5DOf+UyS5EMf+lBWrVrVcL6KjyYBoPLHl835vfV/fmkeV9KYSguA5PQ/0poToQXAGWnE\nmAuhBUAxbXlCC4Bzo9I68uah07UO4Bxx1UP9TY8dv/1b5+waWmFx+9IzvYQzTqUFgEYMAArSVkZq\nCS0AVFoAFORcaMQA4DxRRmaV0pkPACotABKNGAAUpIzMEloARCMGAAUpZHuw4edpOcaJU7Fk4+qm\nxx5+/IUWrgTOTa08xqly83vn/N76Iy/O40oaU2kBUMz2oJZ3AIqh0gJA9yAABSmkEUNoAaDSAqAg\nhTRiCC0AimnLE1oAFFNpFZKtAKDSAiDRiMH552w4mumqh/qbHjt++7dauBKa4e/XWaSQ7UGhBUAx\nD4uEFgAqLQAKUkZmCS0AUswxToXsYgKASguAxDMtAApSRmYJLQCSikoLgFIILQCKUUhmpVKv1+sn\nunnkzUOncy3AabZk4+qmxp0NR3SRLG5f2rK5F931+3N+7+tbfjiPK2lMyzsAxbA9CIBnWgCUQ2gB\nUAyhBUAxCsksoQWASguAgpQSWlreASiGSguAVAo5MVdowXnMSRf8Sinbg0ILAN2DAJSjrZDUEloA\n2B4EoBytDK3h4eHs2rUrlUolAwMDWbdu3ey9Rx55JN/+9rfT1taWK664Ip///OcbzqXlHYCWGR8f\nz+TkZEZGRjI0NJShoaHZezMzM/n617+eRx55JNu2bcvExER++MPGH3MitABIpTL3VyNjY2Pp7e1N\nknR3d+fgwYOZmZlJkixcuDALFy7MoUOHcvTo0Rw+fDjLly9vOJ/tQQBatj1Yq9Wydu3a2evOzs5M\nT0+no6MjF1xwQe6444709vbmggsuyPXXX59Vq1Y1nE+lBUAqlcqcX6eiXq/Pfj0zM5OHHnoojz/+\neJ544ons2rUrzz//fMP3Cy0AWhZa1Wo1tVpt9npqaipdXV1JkomJiVx66aXp7OzMokWLcuWVV2b3\n7t0N5xNaALQstHp6erJ9+/YkyZ49e1KtVtPR0ZEkueSSSzIxMZEjR44kSXbv3p33vOc9Deebt2da\nSzaubmqcY2MAzj6t6nhfv3591q5dm/7+/lQqlQwODmZ0dDTLli1LX19fbrvtttxyyy1pb2/P+973\nvlx55ZWN11l/6wbjbzjy5qGmFya0AFprcfvSls190X3Xzvm9r/71v83jShrTPQiAEzEAKIfQAqAY\nDswFoBiFZJbQAsD2IAAFqaSM0PLLxQAUQ6UFgO1BAMpx3oVWsyddNHtyxqnMCcA7U0hmqbQAOA8r\nLQDKJbQAKEYpoaXlHYBiqLQA0IgBQDlK2R4UWgAILQDKIbQAKEYhmSW0AFBpndC5ejTTLdvvanrs\nN/5oSwtXAvPvVP75PhX+XeBUqbQAUGkBUA6hBUAxCsksoQWASguAkggtAEpRSqXllHcAiqHSAqCU\n3UGhBUA524NCC4BiQqtSr9frJ7p55M1Dp3MtAKfdko2rmxp3NhxBt7h9acvmfv/f3Tjn9+689dF5\nXEljKi0APNMCoBylbA9qeQegGCotAIqptIQWAEILgHIILQCKUUhmCS0AVFoAFKSU0NLyDkAxzstK\n66qH+pseO377t87YnEDrnQ3HM50NSqm0zsvQAuBYhWSW0AJApQVASYQWAKVQaQFQjLYyMkvLOwDl\nUGkBYHsQgHK0CS0AStHKSmt4eDi7du1KpVLJwMBA1q1bN3vv1VdfzV133ZU33ngjl19+ee69996G\nc52XodWKEymccnF2WLJxddNjnYQAv9aqBofx8fFMTk5mZGQkExMTGRgYyMjIyOz9zZs359Zbb01f\nX1+++MUv5sc//nEuvvji075OAArSVqnM+dXI2NhYent7kyTd3d05ePBgZmZmkiQ///nP88wzz+S6\n665LkgwODjYMrERoAZBfbA/O9dVIrVbLihUrZq87OzszPT2dJDlw4EAuvPDCfOlLX8pNN92UBx98\n8KTrFFoAnDb1ev2Yr/fv359bbrklDz/8cJ577rl873vfa/h+oQVAy7YHq9VqarXa7PXU1FS6urqS\nJCtWrMjFF1+cyy67LO3t7bn66qvz4osvNl7nO/9RAShdq7YHe3p6sn379iTJnj17Uq1W09HRkSRZ\nsGBBLr300vzoRz+avb9q1aqG852X3YMAHKtVFcz69euzdu3a9Pf3p1KpZHBwMKOjo1m2bFn6+voy\nMDCQz372s6nX61m9evVsU8aJVOpv3WD8DUfePDTvPwC0kpZ3zmWL25e2bO4/+8dPzPm9f3/DV+dx\nJY2ptABwjBMA5SjlGCeNGAAUQ6XFOcVzKpibMuosoQVAytkeFFoACC0AyqF7EIBiqLQAKEYZkaXl\nHYCCqLQAsD0IQDmEFgDF0D0IQDFUWpzQc6892/TYy9+1roUrmV+vvX6g6bHvWtTZwpWce/7ppX9o\neuz1l/1JC1dCM5bc/gdNjz380DMtXEnzyogsoQVAyqm0tLwDUAyVFgDFVFpCCwDdgwCUo5RnRUIL\nAJUWAOXwTAuAYpQSWqVsYwKASguA8/CZ1sZHP9HUuMdv/Op8fctilXQ006lwNFPrtOpopgP/N93U\nuM4Lulry/U/Fmr/5cNNjn//L77RwJSd3thzNdCraCjnISaUFwPlXaQFQrlIaMYQWAKnYHgSgFKVs\nD2p5B6AYKi0APNMCoByVQjbehBYAKi0AylFKI4bQAuD8a3l3PBOU52w4nqlZZ/popnNdKduDZTx5\nA4DYHgQgnmkBUJC2QjbehBYAKi0AyiG0ACiGD4EEoBilVFplPHkDgKi0AEg5v1wstAA4/45xAqBc\nbZUynhYJLQCKacQQWgAUsz1YRj0IABFaAOQX3YNzfZ3M8PBwNm3alP7+/jz77LPHHfPggw/mIx/5\nyEnnsj0IQMu2B8fHxzM5OZmRkZFMTExkYGAgIyMjx4zZu3dvdu7cmYULF550PpUWAC2rtMbGxtLb\n25sk6e7uzsGDBzMzM3PMmM2bN+fOO+9sbp1z+/EAOJdUKm1zfjVSq9WyYsWK2evOzs5MT0/PXo+O\njuaqq67KJZdc0tQ6hRYAqbyDP6eiXq/Pfv3aa69ldHQ0H/vYx5p+v2daALTsGKdqtZparTZ7PTU1\nla6uriTJ008/nQMHDuTmm2/O66+/npdeeinDw8MZGBg44XxCi/PWzumnmhr3/q4PtOT7X3pvX9Nj\nX77nuy1Zw1/864n/4/BWf7thuCXfn3NfT09Ptm7dmv7+/uzZsyfVajUdHR1Jko0bN2bjxo1Jkn37\n9uVzn/tcw8BKhBYAad2JGOvXr8/atWvT39+fSqWSwcHBjI6OZtmyZenra/5/3H5FaAHQ0g+BvPvu\nu4+5XrNmzdvGrFy5Mt/85jdPOpfQAsDZgwCU42St62cLoQVAS7cH55PQAqCY7cEy6kEAiEoLgJTz\neVpCC4BitgeFFgDFNGJU6m89vfA3HHnz0OlcCwANLG5f2rK5RyZO/ou9J7Kp++Qf3jhfVFoAeKYF\nQDlKeaal5R2AYqi0ALA9CEA5StkeFFoAFNPyLrQAUGkBUI5KIX15QguAYiqtMqIVAKLSOiOW3PR7\nTY89vO3fmx77xCuPNz32Dy/Z2PRY4Nyn5R2AYrQVsj0otABQaQFQjlIaMYQWAFreAShHKZVWGdEK\nAFFpARBnDwJQkFK2B4UWAFreASiHSosTOpWjmf7rf15oeuyZPpppyZ/+btNjDz/2Hy1cSRl+9sZr\nTY/9rYXvauFKQMs7AAUp5RinMqIVAKLSAiAaMQAoiEYMAIqh0gKgGCotAIrRVkhfntACoJhKq4xo\nBYCotACIRgzmyW8vW32ml9A0RzOdGkczcTYpZXtQaAGg0gKgHEILgHLYHgSgFKVUWlreASiGSgsA\n3YMAlKOU7UGhBYDQAqAcrdweHB4ezq5du1KpVDIwMJB169bN3nv66aezZcuWtLW1ZdWqVRkaGkpb\n24nbLYQWJ/Xn//JXTY279wN3Nj3nFUM3NT12/31PNj0WmJtWVVrj4+OZnJzMyMhIJiYmMjAwkJGR\nkdn799xzT77xjW/k3e9+dz71qU/l+9//fjZs2HDC+YQWAC0LrbGxsfT29iZJuru7c/DgwczMzKSj\noyNJMjo6Ovt1Z2dnfvrTnzacT8s7AC1Tq9WyYsWK2evOzs5MT0/PXv8qsKamprJjx46GVVai0gIg\np6/lvV6vv+2v/eQnP8knP/nJDA4OHhNwxyO0AGjZ9mC1Wk2tVpu9npqaSldX1+z1zMxMPv7xj+fT\nn/50rrnmmpPOZ3sQgFQqlTm/Gunp6cn27duTJHv27Em1Wp3dEkySzZs356Mf/Wiuvfbaptap0gKg\nZZXW+vXrs3bt2vT396dSqWRwcDCjo6NZtmxZrrnmmjz22GOZnJzMo48+miS54YYbsmnTphPOJ7QA\naOkvF999993HXK9Zs2b26927d5/SXEILgGLOHvRMC4BiqLQAKObswUr9eE3zv3TkzUOncy1vs+TD\na04+6JcOf+f5Fq5kfk387D+bHtv9W7/TwpUAJVncvrRlc794cM+c3/ve5WvncSWNqbQAKOaZltAC\nIClke1BoAaDSAqAcpTRiaHkHoBgqLQCKqbSEFgCeaQFQDpUWAMUQWgAUo5TtwbP6GCcAfq2Vxzjt\n+9//nvN7V164ah5X0piWdwCKYXsQgGK2B4UWABoxACiJ0AKgEGVEltACIJ5pAVCUMkJLyzsAxVBp\nAVBInSW0AEhSSmwJLQCKacTwTAuAYqi0AHAiBgDlKCW0bA8CUAyhBUAxbA8CoHsQAOabSguAYhox\nhBYAcSIGAMUoI7I80wKgICotAIrpHhRaAKSUDUKhBUAhkSW0AEhSSmwJLQCKeaalexCAYggtAIph\nexAAxzgBUJJzILQWty89XesA4AwqI7JUWgCknO5BoQVASqm1hBYAhUSWlncACqLSAiCtrLWGh4ez\na9euVCqVDAwMZN26dbP3nnrqqWzZsiXt7e259tprc8cddzScS6UFQCqVypxfjYyPj2dycjIjIyMZ\nGhrK0NDQMffvv//+bN26Ndu2bcuOHTuyd+/ehvMJLQBaZmxsLL29vUmS7u7uHDx4MDMzM0mSl19+\nOcuXL89FF12Utra2bNiwIWNjYw3nE1oApPIO/jRSq9WyYsWK2evOzs5MT08nSaanp9PZ2Xnceyfi\nmRYAp+0wiXq9/o7er9ICoGWq1Wpqtdrs9dTUVLq6uo57b//+/alWqw3nE1oAtExPT0+2b9+eJNmz\nZ0+q1Wo6OjqSJCtXrszMzEz27duXo0eP5sknn0xPT0/D+Sr1d1qrAUADDzzwQH7wgx+kUqlkcHAw\nzz33XJYtW5a+vr7s3LkzDzzwQJLkgx/8YG677baGcwktAIphexCAYggtAIohtAAohtACoBhCC4Bi\nCC0AiiG0ACiG0AKgGP8PrKZu5ERULWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LlZG6VThu-CL",
        "colab_type": "code",
        "outputId": "f154be68-bae5-4890-ef3e-cebb00b1f769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversarial_examples[0], danger=True)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4b41159c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfLF4hgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n4LMIjjaMbmb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So our modified method gives much better results for targeted attacks, lets see how it compares for untargeted attacks. We have tried to do this below, but the commented part breaks."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IegvI9jAMaJz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Craft adversarial examples using the substitute\n",
        "x_adv_sub_ami = ami_attack.generate(input_placeholder, **ami_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "aa82c553-24f2-49e0-e5f9-909bbf357f97",
        "id": "CvgUg5w6MaKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_adv_sub_ami"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Identity_4:0' shape=(?, 856) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6LGNE2HLMaK1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle_ami_test = KerasModelWrapper(oracle)\n",
        "oracle_ami_pred = oracle_ami_test.get_logits(x_adv_sub_ami)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "f4dc7eb2-5861-4e12-d44e-1681d0e4365e",
        "id": "Lvqn-37vMaLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_test_labels_one_hot.shape, adversary_test_inputs.shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((232798, 50), (232798, 856))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B9LNGbH5MaLQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Evaluate the accuracy of the \"black-box\" model on adversarial examples\n",
        "# untargetted_accuracy_ami = model_eval(\n",
        "#         tensorflow_session,\n",
        "#         input_placeholder,\n",
        "#         output_placeholder,\n",
        "#         oracle_ami_pred,\n",
        "#         adversary_test_inputs,\n",
        "#         adversary_test_labels_one_hot,\n",
        "#         args=eval_params\n",
        "# )\n",
        "# print('Test accuracy of oracle on adversarial examples generated '\n",
        "#     'using the substitute: ' + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lg3WsDFbVs0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# An End-to-End Attack\n",
        "\n",
        "In this section, we use the models and attacks developed above to perform a complete attack on the intrusion detection system.\n",
        "\n",
        "First we define two functions:\n",
        "  1. `script_to_command_vector` :: converts a list of bash commands into a command vector (as if it werre generated by `acct`).\n",
        "  2. `pad_script` :: takes an input script and generats an output script with the same behaviour, but with the command counts specified by command_vector."
      ]
    },
    {
      "metadata": {
        "id": "EY8jUsNCKSxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def script_to_command_vector(script):\n",
        "    lines = script.split(\"\\n\")  # ['netscape', 'sh ./my-script.sh', ...]\n",
        "    commands = [\n",
        "        line.split(\" \")[0] for line in lines\n",
        "    ]  # ['netscape', 'sh', ...]\n",
        "    \n",
        "    commands = pandas.Series(commands).astype(command_dtype)\n",
        "    commands_one_hot = pandas.get_dummies(commands)\n",
        "    command_counts = commands_one_hot.sum()\n",
        "    \n",
        "    return command_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clLauLU2Wu7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For our proof-of-concept, just append --help to turn our commands into no-ops. This won't\n",
        "# actually work for all of these commands, but proves the point.\n",
        "COMMAND_TO_NOOP = {command: command + \" --help\" for command in commands}\n",
        "\n",
        "def pad_script(original_script, target_command_counts):\n",
        "    # First, calculate the command counts of the input script:\n",
        "    original_command_counts = script_to_command_vector(original_script)\n",
        "    \n",
        "    # Find the number of each command we need to pad by:\n",
        "    additional_command_counts = target_command_counts - original_command_counts\n",
        "    \n",
        "    # Loop over additional_command_counts and append no-op commands for each additional\n",
        "    # command needed:\n",
        "    \n",
        "    # TODO: finish this\n",
        "    padded_script = original_script\n",
        "    \n",
        "    return padded_script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqrpRkx4Wq1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def masq(script, target_user):\n",
        "    command_vector = script_to_command_vector(script)\n",
        "    original_command_vectors = numpy.array([command_vector])\n",
        "\n",
        "    target_labels = keras.utils.to_categorical(numpy.array([target_user]), num_classes=50)\n",
        "\n",
        "    attack = FastGradientMethod(substitute, sess=tensorflow_session)\n",
        "    adversarial_examples = attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        y_target=target_labels,\n",
        "        eps=1.0,\n",
        "        ord=numpy.inf,\n",
        "        clip_min=0.0,\n",
        "        clip_max=100.0,\n",
        "    )\n",
        "\n",
        "    predicted_labels = oracle.predict(adversarial_examples)\n",
        "    \n",
        "    adversarial_example = adversarial_examples[0]\n",
        "    predicted_label = numpy.argmax(predicted_labels[0])\n",
        "    \n",
        "    if predicted_label == target_user:\n",
        "        fool = 'We have fooled the model!'\n",
        "    else:\n",
        "        fool = 'We have failed to fool the model... oh dear.'\n",
        "    \n",
        "    print('These are the adversarial example that was generated: \\n')\n",
        "    print(str(adversarial_example))\n",
        "    print('\\n and this is the predicted user: \\n')\n",
        "    print(str(predicted_label))\n",
        "    print('\\n '+fool)\n",
        "\n",
        "\n",
        "masq(\n",
        "    script=\"\"\"\n",
        "        cat\n",
        "        hostname\n",
        "        awk\n",
        "        stty\n",
        "        tset\n",
        "        sh\n",
        "        chmod\n",
        "        chmod\n",
        "        chmod\n",
        "        chmod\n",
        "        news\n",
        "        sh ./my-script.sh\n",
        "        netstat\n",
        "        netscape\n",
        "        netscape\n",
        "        netscape\n",
        "        netscape\n",
        "        netscape\n",
        "        netscape\n",
        "    \"\"\",\n",
        "    target_user=16,    \n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}